@ARTICLE{Betz2021,
  author={Betz, Johannes and Zheng, Hongrui and Liniger, Alexander and Rosolia, Ugo and Karle, Phillip and Behl, Madhur and Krovi, Venkat and Mangharam, Rahul},
  journal={IEEE Open Journal of Intelligent Transportation Systems}, 
  title={Autonomous Vehicles on the Edge: A Survey on Autonomous Vehicle Racing}, 
  year={2022},
  volume={3},
  number={},
  pages={458-488},
  url={https://doi.org/10.1109/OJITS.2022.3181510}
  }


@article{Pendleton2017,
abstract = {Autonomous vehicles are expected to play a key role in the future of urban transportation systems, as they offer potential for additional safety, increased productivity, greater accessibility, better road efficiency, and positive impact on the environment. Research in autonomous systems has seen dramatic advances in recent years, due to the increases in available computing power and reduced cost in sensing and computing technologies, resulting in maturing technological readiness level of fully autonomous vehicles. The objective of this paper is to provide a general overview of the recent developments in the realm of autonomous vehicle software systems. Fundamental components of autonomous vehicle software are reviewed, and recent developments in each area are discussed.},
author = {Pendleton, Scott Drew and Andersen, Hans and Du, Xinxin and Shen, Xiaotong and Meghjani, Malika and Eng, You Hong and Rus, Daniela and Ang, Marcelo H.},
doi = {10.3390/machines5010006},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pendleton et al. - 2017 - Perception, planning, control, and coordination for autonomous vehicles.pdf:pdf},
issn = {20751702},
journal = {Machines},
keywords = {Automotive control,Autonomous vehicles,Localization,Multi-vehicle cooperation,Perception,Planning},
number = {1},
pages = {1--54},
title = {{Perception, planning, control, and coordination for autonomous vehicles}},
volume = {5},
year = {2017}
}
@article{Gao2017,
abstract = {How can a delivery robot navigate reliably to a destination in a new office building, with minimal prior information? To tackle this challenge, this paper introduces a two-level hierarchical approach, which integrates model-free deep learning and model-based path planning. At the low level, a neural-network motion controller, called the intention-net, is trained end-to-end to provide robust local navigation. The intention-net maps images from a single monocular camera and "intentions" directly to robot controls. At the high level, a path planner uses a crude map, e.g., a 2-D floor plan, to compute a path from the robot's current location to the goal. The planned path provides intentions to the intention-net. Preliminary experiments suggest that the learned motion controller is robust against perceptual uncertainty and by integrating with a path planner, it generalizes effectively to new environments and goals.},
archivePrefix = {arXiv},
arxivId = {1710.05627},
author = {Gao, Wei and Hsu, David and Lee, Wee Sun and Shen, Shengmei and Subramanian, Karthikk},
eprint = {1710.05627},
file = {:C$\backslash$:/Users/Andrew/Downloads/1710.05627.pdf:pdf},
keywords = {deep learning,imitation learning,path planning,visual navigation},
number = {Figure 1},
pages = {1--10},
title = {{Intention-Net: Integrating Planning and Deep Learning for Goal-Directed Autonomous Navigation}},
url = {http://arxiv.org/abs/1710.05627},
year = {2017}
}
@article{Baheri2020,
abstract = {In this paper, we present a safe deep reinforcement learning system for automated driving. The proposed framework leverages merits of both rule-based and learning-based approaches for safety assurance. Our safety system consists of two modules namely handcrafted safety and dynamically-learned safety. The handcrafted safety module is a heuristic safety rule based on common driving practice that ensure a minimum relative gap to a traffic vehicle. On the other hand, the dynamically-learned safety module is a data-driven safety rule that learns safety patterns from driving data. Specifically, the dynamically-leaned safety module incorporates a model lookahead beyond the immediate reward of reinforcement learning to predict safety longer into the future. If one of the future states leads to a near-miss or collision, then a negative reward will be assigned to the reward function to avoid collision and accelerate the learning process. We demonstrate the capability of the proposed framework in a simulation environment with varying traffic density. Our results show the superior capabilities of the policy enhanced with dynamically-learned safety module.},
archivePrefix = {arXiv},
arxivId = {1910.12905},
author = {Baheri, Ali and Nageshrao, Subramanya and Tseng, H. Eric and Kolmanovsky, Ilya and Girard, Anouck and Filev, Dimitar},
doi = {10.1109/IV47402.2020.9304744},
eprint = {1910.12905},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Baheri et al. - 2020 - Deep Reinforcement Learning with Enhanced Safety for Autonomous Highway Driving.pdf:pdf},
journal = {IEEE Intelligent Vehicles Symposium, Proceedings},
pages = {1550--1555},
title = {{Deep Reinforcement Learning with Enhanced Safety for Autonomous Highway Driving}},
year = {2020}
}

@inproceedings{Pan2017,
    title={Virtual to Real Reinforcement Learning for Autonomous Driving},
    author={Xinlei Pan, Yurong You, Ziyan Wang and Cewu Lu},
    year={2017},
    month={September},
    pages={11.1-11.13},
    articleno={11},
    numpages={13},
    booktitle={Proceedings of the British Machine Vision Conference (BMVC)},
    publisher={BMVA Press},
    editor={Tae-Kyun Kim, Stefanos Zafeiriou, Gabriel Brostow and Krystian Mikolajczyk},
    doi={10.5244/C.31.11},
    isbn={1-901725-60-X},
    url={https://dx.doi.org/10.5244/C.31.11}
}

@article{Yu2020,
abstract = {Existing mobile robots cannot complete some functions. To solve these problems, which include autonomous learning in path planning, the slow convergence of path planning, and planned paths that are not smooth, it is possible to utilize neural networks to enable to the robot to perceive the environment and perform feature extraction, which enables them to have a fitness of environment to state action function. By mapping the current state of these actions through Hierarchical Reinforcement Learning (HRL), the needs of mobile robots are met. It is possible to construct a path planning model for mobile robots based on neural networks and HRL. In this article, the proposed algorithm is compared with different algorithms in path planning. It underwent a performance evaluation to obtain an optimal learning algorithm system. The optimal algorithm system was tested in different environments and scenarios to obtain optimal learning conditions, thereby verifying the effectiveness of the proposed algorithm. Deep Deterministic Policy Gradient (DDPG), a path planning algorithm for mobile robots based on neural networks and hierarchical reinforcement learning, performed better in all aspects than other algorithms. Specifically, when compared with Double Deep Q-Learning (DDQN), DDPG has a shorter path planning time and a reduced number of path steps. When introducing an influence value, this algorithm shortens the convergence time by 91{\%} compared with the Q-learning algorithm and improves the smoothness of the planned path by 79{\%}. The algorithm has a good generalization effect in different scenarios. These results have significance for research on guiding, the precise positioning, and path planning of mobile robots.},
annote = {Technique: 
Deterministic policy gradient and hierarchical reinforcement learning - AC3 added to DQN
End to end agent

Better than: 
Double deep Q-learning - reduced time and shorter path

Problem solved: 
Slow convergence in path planning
Algorithm not dependent on environment

Requirements: 
Smoothness of path meet dynamics of robot
Shortest collision free path

Notes on implementation:
Rewards are scales between -1 and 1
Softmax for policy selection
Feature point positioning through gaussian window

Conclusion:
DQN is effective in end to end learning
DDQN can have continuous action space

Future work:
Cannot 'trial and error' on physical system
Path planning only for static scenarios},
author = {Yu, Jinglun and Su, Yuancheng and Liao, Yifan},
doi = {10.3389/fnbot.2020.00063},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yu, Su, Liao - 2020 - The Path Planning of Mobile Robot by Neural Networks and Hierarchical Reinforcement Learning.pdf:pdf},
issn = {16625218},
journal = {Frontiers in Neurorobotics},
keywords = {fusion algorithm,hierarchical reinforcement learning,mobile robot,neural network,path planning},
number = {October},
pages = {1--12},
title = {{The Path Planning of Mobile Robot by Neural Networks and Hierarchical Reinforcement Learning}},
volume = {14},
year = {2020}
}
@article{Bansal2019,
abstract = {Our goal is to train a policy for autonomous driving via imitation learning that is robust enough to drive a real vehicle. We find that standard behavior cloning is insufficient for handling complex driving scenarios, even when we leverage a perception system for preprocessing the input and a controller for executing the output on the car: 30 million examples are still not enough. We propose exposing the learner to synthesized data in the form of perturbations to the expert's driving, which creates interesting situations such as collisions and/or going off the road. Rather than purely imitating all data, we augment the imitation loss with additional losses that penalize undesirable events and encourage progress – the perturbations then provide an important signal for these losses and lead to robustness of the learned model. We show that the ChauffeurNet model can handle complex situations in simulation, and present ablation experiments that emphasize the importance of each of our proposed changes and show that the model is responding to the appropriate causal factors. Finally, we demonstrate the model driving a real car at our test facility.},
archivePrefix = {arXiv},
arxivId = {1812.03079},
author = {Bansal, Mayank and Krizhevsky, Alex and Ogale, Abhijit},
doi = {10.15607/rss.2019.xv.031},
eprint = {1812.03079},
file = {:C$\backslash$:/Users/Andrew/Downloads/1812.03079.pdf:pdf},
isbn = {9780992374754},
issn = {2330765X},
keywords = {deep learning,learning to drive,mid-to-mid driving,trajectory predic-},
pages = {1--20},
title = {{ChauffeurNet: Learning to Drive by Imitating the Best and Synthesizing the Worst}},
year = {2019}
}
@article{Xiao2019,
abstract = {Moving in complex environments is an essential capability of intelligent mobile robots. Decades of research and engineering have been dedicated to developing sophisticated navigation systems to move mobile robots from one point to another. Despite their overall success, a recently emerging research thrust is devoted to developing machine learning techniques to address the same problem, based in large part on the success of deep learning. However, to date, there has not been much direct comparison between the classical and emerging paradigms to this problem. In this article, we survey recent works that apply machine learning for motion control in mobile robot navigation, within the context of classical navigation systems. The surveyed works are classified into different categories, which delineate the relationship of the learning approaches to classical methods. Based on this classification, we identify common challenges and promising future directions. Keywords},
archivePrefix = {arXiv},
arxivId = {arXiv:2011.13112v1},
author = {Xiao, Xuesu and Liu, Bo and Warnell, Garrett and Stone, Peter},
doi = {10.1177/ToBeAssigned},
eprint = {arXiv:2011.13112v1},
file = {:C$\backslash$:/Users/Andrew/Downloads/2011.13112.pdf:pdf},
journal = {Journal of Vibration and Control},
keywords = {[PHYS.MECA.VIBR]Physics [physics]/Mechanics [physi,bit-rock stochastic interaction model,drillstring dynamics,experimental identification,hysteretic friction},
pages = {107754631982824},
title = {{Motion Control for Mobile Robot Navigation Using Machine Learning: a Survey}},
year = {2019}
}
@article{Yurtsever2020,
abstract = {Automated driving in urban settings is challenging. Human participant behavior is difficult to model, and conventional, rule-based Automated Driving Systems (ADSs) tend to fail when they face unmodeled dynamics. On the other hand, the more recent, end-to-end Deep Reinforcement Learning (DRL) based model-free ADSs have shown promising results. However, pure learning-based approaches lack the hard-coded safety measures of model-based controllers. Here we propose a hybrid approach for integrating a path planning pipe into a vision based DRL framework to alleviate the shortcomings of both worlds. In summary, the DRL agent is trained to follow the path planner's waypoints as close as possible. The agent learns this policy by interacting with the environment. The reward function contains two major terms: the penalty of straying away from the path planner and the penalty of having a collision. The latter has precedence in the form of having a significantly greater numerical value. Experimental results show that the proposed method can plan its path and navigate between randomly chosen origin-destination points in CARLA, a dynamic urban simulation environment. Our code is open-source and available online 11https://github.com/Ekim-Yurtsever/Hybrid-DeepRL-Automated-Driving.},
archivePrefix = {arXiv},
arxivId = {2002.00434},
author = {Yurtsever, Ekim and Capito, Linda and Redmill, Keith and Ozgune, Umit},
doi = {10.1109/IV47402.2020.9304735},
eprint = {2002.00434},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yurtsever et al. - 2020 - Integrating Deep Reinforcement Learning with Model-based Path Planners for Automated Driving.pdf:pdf},
isbn = {9781728166735},
journal = {IEEE Intelligent Vehicles Symposium, Proceedings},
number = {Iv},
pages = {1311--1316},
title = {{Integrating Deep Reinforcement Learning with Model-based Path Planners for Automated Driving}},
year = {2020}
}
@article{Mohammed2019,
abstract = {The autonomous driving field is progressing at a fast pace due to the revolution introduced by Deep Learning technologies in Computer Vision algorithms. To have a functioning autonomous driving vehicle, the system needs to have different functional blocks, e.g, automatic emergency braking (AEB), lane-keeping assist (LKA), active cruise control (ACC), traffic jam assist (TJA), and crash avoidance (CA). Path planning is one of the main blocks of an autonomous driving pipeline that is integrated into the system to ensure that these functionalities are working properly. Traditional path planning algorithms require difficult prerequisites: a) a well-defined map, b) sensor fusion, c) localization, and d) a controller. Unlike the traditional techniques, End-to-End deep path planning and automatic emergency braking solutions are proposed to solve the problem using a single block, instead of the complexities of the four previous blocks. In this paper, we rely only on cameras cocoon that covers 360 o around the vehicle. These cameras are installed on the four sides of the vehicle. They capture a continuous flow of images that are processed by a deep convolutional neural network. The network's output controls three vehicle's components: gas throttle, the steering wheel, and the brakes. We build our own benchmark based on the CARLA simulator due to the in-existence of benchmarks serving our proposed idea. The proposed model shows a generalization capability to differentiate between overtaking and braking, and it is considered as the first deep learning solution for both path planning and automatic emergency braking functionalities. In order to form a good basis that could help to accelerate the contribution in the field of autonomous driving, code and videos are available at https://github.com/eslambakr/Path{\_}Planning{\_}using{\_}Deeplearning},
author = {Mohammed, Eslam and Abdou, Mohammed and Nasr, Omar Ahmed},
file = {:C$\backslash$:/Users/Andrew/Downloads/End-to-End Deep Path Planning and AutomaticEmergency Braking Camera Cocoon-based Solution.pdf:pdf},
number = {NeurIPS},
title = {{End-to-End Deep Path Planning and Automatic Emergency Braking Camera Cocoon-based Solution}},
url = {https://github.com/eslambakr/Path{\_}Planning{\_}using{\_}Deeplearning},
year = {2019}
}
@article{Blum2019,
abstract = {Space exploration missions have seen use of increasingly sophisticated robotic systems with ever more autonomy. Deep learning promises to take this even a step further, and has applications for high-level tasks, like path planning, as well as low-level tasks, like motion control, which are critical components for mission efficiency and success. Using deep reinforcement end-to-end learning with randomized reward function parameters during training, we teach a simulated 8 degree-of-freedom quadruped ant-like robot to travel anywhere within a perimeter, conducting path plan and motion control on a single neural network, without any system model or prior knowledge of the terrain or environment. Our approach also allows for user specified waypoints, which could translate well to either fully autonomous or semi-autonomous/teleoperated space applications that encounter delay times. We trained the agent using randomly generated waypoints linked to the reward function and passed waypoint coordinates as inputs to the neural network. Such applications show promise on a variety of space exploration robots, including high speed rovers for fast locomotion and legged cave robots for rough terrain.},
annote = {Attempt to develop fast, precise obstacle avoidance
RL for reactive control can control robot in realtime

Goals:
Find goal, go through waypoints
Learn to walk
Human operator can specify waypoint

Implementation:
End to end model
No map

Issues with current path plannign techniques:
A* - Requires knowledge about the map
Engineer must carefully design cost function

Approaches:
Hierarchical - a neural network for each task

Agent:
Recieves waypoints
Develops local plan and tracks path

Notes:
Separate learning tasks for vehicle control and path planning - if a different path is specified, vehicle will not need to relearn controls.},
archivePrefix = {arXiv},
arxivId = {1909.06034},
author = {Blum, Tamir and Jones, William and Yoshida, Kazuya},
eprint = {1909.06034},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blum, Jones, Yoshida - 2019 - Deep Learned Path Planning via Randomized Reward-Linked-Goals and Potential Space Applications.pdf:pdf},
keywords = {Reinforcement Learning, Locomotion, Path planning,},
title = {{Deep Learned Path Planning via Randomized Reward-Linked-Goals and Potential Space Applications}},
url = {http://arxiv.org/abs/1909.06034},
year = {2019}
}
@article{Nageshrao2019,
abstract = {The operational space of an autonomous vehicle (AV) can be diverse and vary significantly. Due to this, formulating a rule based decision maker for selecting driving maneuvers may not be ideal. Similarly, it may not be efficient to solve optimal control problem in real-time for a predefined cost function. In order to address these issues and to avoid peculiar behaviors when encountering unforeseen scenario, we propose a reinforcement learning (RL) based method, where the ego car, i.e., an autonomous vehicle, learns to make decisions by directly interacting with the simulated traffic. Here the decision maker is a deep neural network that provides an action choice for a given system state. We demonstrate the performance of the developed algorithm in highway driving scenario where the trained AV encounters varying traffic density.},
archivePrefix = {arXiv},
arxivId = {1904.00035},
author = {Nageshrao, Subramanya and Tseng, H. Eric and Filev, DImitar},
doi = {10.1109/SMC.2019.8914621},
eprint = {1904.00035},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nageshrao, Tseng, Filev - 2019 - Autonomous highway driving using deep reinforcement learning.pdf:pdf},
isbn = {9781728145693},
issn = {1062922X},
journal = {Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics},
pages = {2326--2331},
title = {{Autonomous highway driving using deep reinforcement learning}},
volume = {2019-Octob},
year = {2019}
}
@article{Bellegarda2020,
abstract = {Recent breakthroughs both in reinforcement learning and trajectory optimization have made significant advances towards real world robotic system deployment. Reinforcement learning (RL) can be applied to many problems without needing any modeling or intuition about the system, at the cost of high sample complexity and the inability to prove any metrics about the learned policies. Trajectory optimization (TO) on the other hand allows for stability and robustness analyses on generated motions and trajectories, but is only as good as the often over-simplified derived model, and may have prohibitively expensive computation times for real-time control, for example in contact rich environments. This paper seeks to combine the benefits from these two areas while mitigating their drawbacks by (1) decreasing RL sample complexity by using existing knowledge of the problem with real-time optimal control, and (2) allowing online policy deployment at any point in the training process by using the TO (MPC) as a baseline or worst-case scenario action, while continuously improving the combined learned-optimized policy with deep RL. This method is evaluated on tasks of successively navigating a car model to a series of goal destinations over slippery terrains as fast as possible, in which drifting will allow the system to more quickly change directions while maintaining high speeds.},
author = {Bellegarda, Guillaume and Byl, Katie},
doi = {10.1109/IROS45743.2020.9341021},
file = {:C$\backslash$:/Users/Andrew/Downloads/2178.pdf:pdf},
isbn = {9781728162126},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {5453--5459},
title = {{An online training method for augmenting MPC with deep reinforcement learning}},
year = {2020}
}
@article{Lyu2022,
abstract = {With the increasing emphasis on the safe autonomy for robots, model-based safe control approaches such as Control Barrier Functions have been extensively studied to ensure guaranteed safety during inter-robot interactions. In this paper, we introduce the Parametric Control Barrier Function (Parametric-CBF), a novel variant of the traditional Control Barrier Function to extend its expressivity in describing different safe behaviors among heterogeneous robots. Instead of assuming cooperative and homogeneous robots using the same safe controllers, the ego robot is able to model the neighboring robots' underlying safe controllers through different Parametric-CBFs with observed data. Given learned parametric-CBF and proved forward invariance, it provides greater flexibility for the ego robot to better coordinate with other heterogeneous robots with improved efficiency while enjoying formally provable safety guarantees. We demonstrate the usage of Parametric-CBF in behavior prediction and adaptive safe control in the ramp merging scenario from the applications of autonomous driving. Compared to traditional CBF, Parametric-CBF has the advantage of capturing varying drivers' characteristics given richer description of robot behavior in the context of safe control. Numerical simulations are given to validate the effectiveness of the proposed method.},
archivePrefix = {arXiv},
arxivId = {2202.09936},
author = {Lyu, Yiwei and Luo, Wenhao and Dolan, John M.},
eprint = {2202.09936},
file = {:C$\backslash$:/Users/Andrew/Downloads/2202.09936.pdf:pdf},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
title = {{Adaptive Safe Merging Control for Heterogeneous Autonomous Vehicles using Parametric Control Barrier Functions}},
url = {http://arxiv.org/abs/2202.09936},
year = {2022}
}
@article{Bojarski2020,
abstract = {Four years ago, an experimental system known as PilotNet became the first NVIDIA system to steer an autonomous car along a roadway. This system represents a departure from the classical approach for self-driving in which the process is manually decomposed into a series of modules, each performing a different task. In PilotNet, on the other hand, a single deep neural network (DNN) takes pixels as input and produces a desired vehicle trajectory as output; there are no distinct internal modules connected by human-designed interfaces. We believe that handcrafted interfaces ultimately limit performance by restricting information flow through the system and that a learned approach, in combination with other artificial intelligence systems that add redundancy, will lead to better overall performing systems. We continue to conduct research toward that goal. This document describes the PilotNet lane-keeping effort, carried out over the past five years by our NVIDIA PilotNet group in Holmdel, New Jersey. Here we present a snapshot of system status in mid-2020 and highlight some of the work done by the PilotNet group.},
archivePrefix = {arXiv},
arxivId = {2010.08776},
author = {Bojarski, Mariusz and Chen, Chenyi and Daw, Joyjit and Değirmenci, Alperen and Deri, Joya and Firner, Bernhard and Flepp, Beat and Gogri, Sachin and Hong, Jesse and Jackel, Lawrence and Jia, Zhenhua and Lee, BJ and Liu, Bo and Liu, Fei and Muller, Urs and Payne, Samuel and Prasad, Nischal Kota Nagendra and Provodin, Artem and Roach, John and Rvachov, Timur and Tadimeti, Neha and van Engelen, Jesper and Wen, Haiguang and Yang, Eric and Yang, Zongyi},
eprint = {2010.08776},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bojarski et al. - 2020 - The NVIDIA PilotNet Experiments.pdf:pdf},
pages = {1--28},
title = {{The NVIDIA PilotNet Experiments}},
url = {http://arxiv.org/abs/2010.08776},
year = {2020}
}
@article{Chen2015,
abstract = {Today, there are two major paradigms for vision-based autonomous driving systems: mediated perception approaches that parse an entire scene to make a driving decision, and behavior reflex approaches that directly map an input image to a driving action by a regressor. In this paper, we propose a third paradigm: a direct perception approach to estimate the affordance for driving. We propose to map an input image to a small number of key perception indicators that directly relate to the affordance of a road/traffic state for driving. Our representation provides a set of compact yet complete descriptions of the scene to enable a simple controller to drive autonomously. Falling in between the two extremes of mediated perception and behavior reflex, we argue that our direct perception representation provides the right level of abstraction. To demonstrate this, we train a deep Convolutional Neural Network using recording from 12 hours of human driving in a video game and show that our model can work well to drive a car in a very diverse set of virtual environments. We also train a model for car distance estimation on the KITTI dataset. Results show that our direct perception approach can generalize well to real driving images. Source code and data are available on our project website.},
annote = {Partial end end - a CNN replaces the perception subsystem

maps an image to 'affordance indicators': Distance from the vehicle to left and right lanes, distance to other vehicles.

Rules based decision maker decides what policy to follow (overtake, keep in lane, etc.). Simplified simulation - other vehicles do not move between lanes.

End to end baseline is erratic},
archivePrefix = {arXiv},
arxivId = {1505.00256},
author = {Chen, Chenyi and Seff, Ari and Kornhauser, Alain and Xiao, Jianxiong},
doi = {10.1109/ICCV.2015.312},
eprint = {1505.00256},
file = {:C$\backslash$:/Users/Andrew/Downloads/paper.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
number = {Figure 1},
pages = {2722--2730},
title = {{DeepDriving: Learning affordance for direct perception in autonomous driving}},
volume = {2015 Inter},
year = {2015}
}
@article{Osinski2020,
abstract = {We use reinforcement learning in simulation to obtain a driving system controlling a full-size real-world vehicle. The driving policy takes RGB images from a single camera and their semantic segmentation as input. We use mostly synthetic data, with labelled real-world data appearing only in the training of the segmentation network.Using reinforcement learning in simulation and synthetic data is motivated by lowering costs and engineering effort.In real-world experiments we confirm that we achieved successful sim-to-real policy transfer. Based on the extensive evaluation, we analyze how design decisions about perception, control, and training impact the real-world performance.},
archivePrefix = {arXiv},
arxivId = {1911.12905},
author = {Osinski, Blazej and Jakubowski, Adam and Ziecina, Pawel and Milos, Piotr and Galias, Christopher and Homoceanu, Silviu and Michalewski, Henryk},
doi = {10.1109/ICRA40945.2020.9196730},
eprint = {1911.12905},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Osinski et al. - 2020 - Simulation-Based Reinforcement Learning for Real-World Autonomous Driving.pdf:pdf},
isbn = {9781728173955},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {6411--6418},
title = {{Simulation-Based Reinforcement Learning for Real-World Autonomous Driving}},
year = {2020}
}
@article{Panov2018,
abstract = {Single-shot grid-based path finding is an important problem with the applications in robotics, video games etc. Typically in AI community heuristic search methods (based on A And its variations) are used to solve it. In this work we present the results of preliminary studies on how neural networks can be utilized to path planning on square grids, e.g. how well they can cope with path finding tasks by themselves within the well-known reinforcement problem statement. Conducted experiments show that the agent using neural Q-learning algorithm robustly learns to achieve the goal on small maps and demonstrate promising results on the maps have ben never seen by him before.},
annote = {Technique: 
Grid based path planning
Deep Q-learning

Algorithms for grid based path planning:
A*, IDA*, ARA*, JPS, Theta* - all heuristic search algorithms

Implementation:
Agent is holonomic - actions are move up, down, left and right
Input state space - grid with robot at center, 1 is occupied, 0 is unoccupied
Model free learning

Neural network notes:
Loss function - mean squared error
Convolutional layers
Dropout for regularisation and overfitting avoidance
Dense output layer with linear activation function

2 phases:
Acting - Add to memory
Learning - Shuffle memory},
author = {Panov, Aleksandr I. and Yakovlev, Konstantin S. and Suvorov, Roman},
doi = {10.1016/j.procs.2018.01.054},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Panov, Yakovlev, Suvorov - 2018 - Grid path planning with deep reinforcement learning Preliminary results.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {Convolution networks,Neural networks,Path planning,Q-learning,Q-network,Reinforcement learning},
pages = {347--353},
publisher = {Elsevier B.V.},
title = {{Grid path planning with deep reinforcement learning: Preliminary results}},
url = {https://doi.org/10.1016/j.procs.2018.01.054},
volume = {123},
year = {2018}
}
@article{Altche2018,
abstract = {In order to drive safely and efficiently on public roads, autonomous vehicles will have to understand the intentions of surrounding vehicles, and adapt their own behavior accordingly. If experienced human drivers are generally good at inferring other vehicles' motion up to a few seconds in the future, most current Advanced Driving Assistance Systems (ADAS) are unable to perform such medium-term forecasts, and are usually limited to high-likelihood situations such as emergency braking. In this article, we present a first step towards consistent trajectory prediction by introducing a long short-term memory (LSTM) neural network, which is capable of accurately predicting future longitudinal and lateral trajectories for vehicles on highway. Unlike previous work focusing on a low number of trajectories collected from a few drivers, our network was trained and validated on the NGSIM US-101 dataset, which contains a total of 800 hours of recorded trajectories in various traffic densities, representing more than 6000 individual drivers.},
annote = {LSTM predicts trajectories of other vehicles on a highway using previously observed data (supervised technique)},
archivePrefix = {arXiv},
arxivId = {1801.07962},
author = {Altche, Florent and {De La Fortelle}, Arnaud},
doi = {10.1109/ITSC.2017.8317913},
eprint = {1801.07962},
file = {:C$\backslash$:/Users/Andrew/Downloads/1801.07962.pdf:pdf},
isbn = {9781538615256},
journal = {IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC},
pages = {353--359},
title = {{An LSTM network for highway trajectory prediction}},
volume = {2018-March},
year = {2018}
}
@article{Fragkiadaki2020,
author = {Fragkiadaki, Katerina},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fragkiadaki - 2020 - Sim2Real So far.pdf:pdf},
title = {{Sim2Real So far}},
year = {2020}
}
@article{Nieto-Cabrera2021,
abstract = {Consider learning a policy from example expert behavior, without interaction with the expert or access to a reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data as if it were obtained by reinforcement learning following inverse reinforcement learning. We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments.},
author = {Nieto-Cabrera, M. Elena and Cramer, In{\'{e}}s Mar{\'{i}}a Mart{\'{i}}nez and Nieto-Morales, Concepcion},
doi = {10.2307/j.ctv1dp0vwx.25},
file = {:C$\backslash$:/Users/Andrew/Downloads/NIPS-2016-generative-adversarial-imitation-learning-Paper.pdf:pdf},
journal = {Discurso y experiencias de personas privadas de libertad: afectos y emociones en riesgo. Aqu{\'{i}} y ahora al l{\'{i}}mite: La mujer II},
number = {Nips},
pages = {65--66},
title = {{Gail}},
year = {2021}
}
@article{OpenAI2019,
abstract = {We demonstrate that models trained only in simulation can be used to solve a manipulation problem of unprecedented complexity on a real robot. This is made possible by two key components: a novel algorithm, which we call automatic domain randomization (ADR) and a robot platform built for machine learning. ADR automatically generates a distribution over randomized environments of ever-increasing difficulty. Control policies and vision state estimators trained with ADR exhibit vastly improved sim2real transfer. For control policies, memory-augmented models trained on an ADR-generated distribution of environments show clear signs of emergent meta-learning at test time. The combination of ADR with our custom robot platform allows us to solve a Rubik's cube with a humanoid robot hand, which involves both control and state estimation problems. Videos summarizing our results are available: https://openai.com/blog/solving-rubiks-cube/},
archivePrefix = {arXiv},
arxivId = {1910.07113},
author = {OpenAI and Akkaya, Ilge and Andrychowicz, Marcin and Chociej, Maciek and Litwin, Mateusz and McGrew, Bob and Petron, Arthur and Paino, Alex and Plappert, Matthias and Powell, Glenn and Ribas, Raphael and Schneider, Jonas and Tezak, Nikolas and Tworek, Jerry and Welinder, Peter and Weng, Lilian and Yuan, Qiming and Zaremba, Wojciech and Zhang, Lei},
eprint = {1910.07113},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/OpenAI et al. - 2019 - Solving Rubik's Cube with a Robot Hand.pdf:pdf},
pages = {1--51},
title = {{Solving Rubik's Cube with a Robot Hand}},
url = {http://arxiv.org/abs/1910.07113},
year = {2019}
}
@article{Tobin2017,
abstract = {Bridging the 'reality gap' that separates simulated robotics from experiments on hardware could accelerate robotic research through improved data availability. This paper explores domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization, which is a stepping stone to general robotic manipulation skills. We find that it is possible to train a real-world object detector that is accurate to 1.5 cm and robust to distractors and partial occlusions using only data from a simulator with non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping in a cluttered environment. To our knowledge, this is the first successful transfer of a deep neural network trained only on simulated RGB images (without pre-training on real images) to the real world for the purpose of robotic control.},
archivePrefix = {arXiv},
arxivId = {1703.06907},
author = {Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
doi = {10.1109/IROS.2017.8202133},
eprint = {1703.06907},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tobin et al. - 2017 - Domain randomization for transferring deep neural networks from simulation to the real world.pdf:pdf},
isbn = {9781538626825},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {23--30},
title = {{Domain randomization for transferring deep neural networks from simulation to the real world}},
volume = {2017-Septe},
year = {2017}
}
@article{Loquercio2020,
abstract = {Dynamically changing environments, unreliable state estimation, and operation under severe resource constraints are fundamental challenges that limit the deployment of small autonomous drones. We address these challenges in the context of autonomous, vision-based drone racing in dynamic environments. A racing drone must traverse a track with possibly moving gates at high speed. We enable this functionality by combining the performance of a state-of-the-art planning and control system with the perceptual awareness of a convolutional neural network. The resulting modular system is both platform independent and domain independent: it is trained in simulation and deployed on a physical quadrotor without any fine-tuning. The abundance of simulated data, generated via domain randomization, makes our system robust to changes of illumination and gate appearance. To the best of our knowledge, our approach is the first to demonstrate zero-shot sim-to-real transfer on the task of agile drone flight. We extensively test the precision and robustness of our system, both in simulation and on a physical platform, and show significant improvements over the state of the art.},
archivePrefix = {arXiv},
arxivId = {1905.09727},
author = {Loquercio, Antonio and Kaufmann, Elia and Ranftl, Rene and Dosovitskiy, Alexey and Koltun, Vladlen and Scaramuzza, Davide},
doi = {10.1109/TRO.2019.2942989},
eprint = {1905.09727},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Loquercio et al. - 2020 - Deep Drone Racing From Simulation to Reality with Domain Randomization.pdf:pdf},
issn = {19410468},
journal = {IEEE Transactions on Robotics},
keywords = {Drone racing,learning agile flight,learning for control},
number = {1},
pages = {1--14},
title = {{Deep Drone Racing: From Simulation to Reality with Domain Randomization}},
volume = {36},
year = {2020}
}
@misc{IEEEexample:whitepaper,
howpublished = {White Paper},
institution = {Cisco},
month = {may},
title = {{Advanced {\{}QoS{\}} Services for the Intelligent Internet}},
year = {1997}
}
@article{Evans2021a,
title = {Reward signal design for autonomous racing},
arxivId = {2103.10098},
journal = {International Conference on Advanced Robotics},
author = {Evans, Benjamin and Engelbrecht, Herman A. and Jordaan, Hendrik W.},
url = {http://arxiv.org/abs/2103.10098},
year = {2021}
}

@misc{IEEEexample:jppat,
author = {Hideki, U},
month = {may},
number = {152932/92},
title = {{Quadrature Modulation Circuit}},
year = {1992}
}
@misc{IEEEexample:private,
author = {Konyagin, S},
howpublished = {private communication},
title = {{No Title}},
year = {1998}
}
@article{IEEEexample:article_typical,
author = {Zhang, S and Zhu, C and Sin, J K O and Mok, P K T},
month = {nov},
pages = {569--571},
title = {{A Novel Ultrathin Elevated Channel Low-temperature Poly-{\{}Si{\}} {\{}TFT{\}}}},
volume = {20},
year = {1999}
}
@article{Lutjens2019a,
abstract = {Many current autonomous systems are being designed with a strong reliance on black box predictions from deep neural networks (DNNs). However, DNNs tend to be overconfident in predictions on unseen data and can give unpredictable results for far-from-distribution test data. The importance of predictions that are robust to this distributional shift is evident for safety-critical applications, such as collision avoidance around pedestrians. Measures of model uncertainty can be used to identify unseen data, but the state-of-the-art extraction methods such as Bayesian neural networks are mostly intractable to compute. This paper uses MC-Dropout and Bootstrapping to give computationally tractable and parallelizable uncertainty estimates. The methods are embedded in a Safe Reinforcement Learning framework to form uncertainty-aware navigation around pedestrians. The result is a collision avoidance policy that knows what it does not know and cautiously avoids pedestrians that exhibit unseen behavior. The policy is demonstrated in simulation to be more robust to novel observations and take safer actions than an uncertainty-unaware baseline.},
archivePrefix = {arXiv},
arxivId = {1810.08700},
author = {L{\"{u}}tjens, Bj{\"{o}}rn and Everett, Michael and How, Jonathan P.},
doi = {10.1109/ICRA.2019.8793611},
eprint = {1810.08700},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/L{\"{u}}tjens, Everett, How - 2019 - Safe reinforcement learning with model uncertainty estimates(2).pdf:pdf},
isbn = {9781538660263},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {8662--8668},
title = {{Safe reinforcement learning with model uncertainty estimates}},
volume = {2019-May},
year = {2019}
}

@article{Stevens2016,
abstract = {We used deep reinforcement learning to train an AI to play tetris using an approach similar to [7]. We use a con-volutional neural network to estimate a Q function that describes the best action to take at each game state. This approach failed to converge when directly applied to predicting individual actions with no help from heuristics. However , we implemented many features that improved convergence. By grouping actions together, we allowed the Q network to focus on learning good game configurations instead of how to move pieces. By using transfer learning from a heuristic model we were able to greatly improve performance , having already learned relevant features about Tetris and optimal piece configurations. And by using prioritized sweeping, we were able to reduce the some of the inherent instability in learning to estimate single actions. Although we did not surpass explicitly featurized agents in terms of performance, we demonstrated the basic ability of a neural network to learn to play Tetris, and with greater training time we might have been able to beat even those benchmarks.},
annote = {Method:
Previous methods - explicit feature vectorisation
This attempt - raw pixel data
Use heuristic function to help train agent
Q learning
Agent is sensitive to the way in which state is represented
Neural network - domain specific knowledge of tetris - collapses each column into single pixel
Bellman equation uses cached version of neural network - updates every 1000 iterations to prevent instability

State representation:
Image of current game screen - pixel format
Crop screen to reduce state size in memory

Actions:
Single move - actions are reversible, difficult to determine which move contributed to reward + add noise
OR grouped - render image of final piece placement - feed that into neural network, which determined rewards - siginificantly improve results
Reward must propagate through chain of actions

Scoring function:
Standard scoring rules - MaTris library - number of lines cleared squared
Game-over penalty improved convergence
Stop oscillatory moves - penalty to move piece
OR: Heuristic function
More meaningful rewards, fitness funciton based on height + bumpiness: 
Reward = change in fitness function

Game implementation:
MaTris from python pygame library

Exploration rate/ policy:
Ebsilon greedy
Anneal from 1 to 0.1 over 1 million frames

Training strategy:
Off policy learning - implement already trained agent to increase initial rewards
Switch between neural network policy and trained agent policy using ebsilon

Experience replay:
Network trains on off policy results - different action distributions for previous states since policy changes

Prioritized sweeping:
Train network on experiences for which it had the highest error

Evaluation metrics:
Evaluate gameplay - game lenght + game score

Experiment setup:
RMSProp to optimise neural network
Gamma-discount rate small enough to reward only moves that contributed to clearing a line

Problems:
Time lag between action, reward},
author = {Stevens, Matt and Pradhan, Sabeek},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Stevens, Pradhan - 2016 - Playing Tetris with Deep Reinforcement Learning.pdf:pdf},
title = {{Playing Tetris with Deep Reinforcement Learning}},
year = {2016}
}
@article{Abdolmaleki2018,
abstract = {We introduce a new algorithm for reinforcement learning called Maximum a-posteriori Policy Optimisation (MPO) based on coordinate ascent on a relative-entropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings.},
archivePrefix = {arXiv},
arxivId = {1806.06920},
author = {Abdolmaleki, Abbas and Springenberg, Jost Tobias and Tassa, Yuval and Munos, Remi and Heess, Nicolas and Riedmiller, Martin},
eprint = {1806.06920},
journal = {6th Int. Conf. Learn. Represent. ICLR 2018 - Conf. Track Proc.},
title = {{Maximum a posteriori policy optimisation}},
year = {2018}
}
@incollection{IEEEexample:incollectionmanyauthors,
author = {Dawson, R M A and Shen, Z and Furst, D A and Connor, S and Hsu, J and Kane, M G and Stewart, R G and Ipri, A and King, C N and Green, P J and Flegal, R T and Pearson, S and Barrow, W A and Dickey, E and Ping, K and Tang, C W and Slyke, S Van. and Chen, F and Shi, J and Sturm, J C and Lu, M H},
booktitle = {{\{}SID{\}} Tech. Dig.},
pages = {11--14},
title = {{Design of an Improved Pixel for a Polysilicon Active-Matrix Organic {\{}LED{\}} Display}},
volume = {29},
year = {1998}
}
@article{Goh2016,
abstract = {Professional drivers in 'drifting' competitions are able to precisely negotiate a specified course at high sideslip angles while operating in an unstable region of state-space. Studying this practice could provide insight into autonomous car control during emergency maneuvers that excurse outside stable handling limits. This paper presents a simple and physically insightful controller for autonomous drifting with simultaneous tracking of a reference path. A feasible reference trajectory is treated as a sequence of unstable drifting equilibrium points, and a basic example is generated from vehicle parameters using a four-wheel model with steady-state weight transfer. Lookahead error and sideslip are chosen as reference states, and a controller for tracking both objectives around an equilibrium point is derived using a simpler single-track model. Experiments on the rear-wheel drive MARTY test vehicle demonstrate good tracking performance of both objectives even at values of sideslip as high as 45 degrees.},
author = {Goh, Jonathan Y. and Gerdes, J. Christian},
doi = {10.1109/IVS.2016.7535448},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goh, Gerdes - 2016 - Simultaneous stabilization and tracking of basic automobile drifting trajectories.pdf:pdf},
isbn = {9781509018215},
journal = {IEEE Intelligent Vehicles Symposium, Proceedings},
number = {Iv},
pages = {597--602},
publisher = {IEEE},
title = {{Simultaneous stabilization and tracking of basic automobile drifting trajectories}},
volume = {2016-Augus},
year = {2016}
}
@article{Bohm2005,
author = {B{\"{o}}hm, N and K{\'{o}}okai, G and Mandl, S},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/B{\"{o}}hm, K{\'{o}}okai, Mandl - 2005 - An Evolutionary Approach to Tetris.pdf:pdf},
isbn = {2959977688},
issn = {00222429},
journal = {Proceedings of the 6th Metaheuristics International Conference (MIC2005)},
pages = {137--148},
title = {{An Evolutionary Approach to Tetris}},
url = {https://faui20a.informatik.uni-erlangen.de/publication/download/mic.pdf},
year = {2005}
}
@article{Weiss2020b,
abstract = {Demonstrating high-speed autonomous racing can be considered as a grand challenge for vision based end-to-end deep learning models. DeepRacing AI is a novel end-to-end framework for trajectory synthesis for autonomous racing. We train and demonstrate the effectiveness of our approach using a high fidelity and photo-realistic Formula One gaming environment-used by real racing drivers. This is the first work that has used the highly realistic F1 game as a simulation environment for deep learning models. We present a novel method for single agent autonomous racing by training a deep neural network to predict a parame-terized representation of a trajectory. Our Bezier curve based trajectory synthesis approach outperforms several other end-to-end DNN approaches for autonomous racing. In addition to evaluating our methodology in a closed-loop manner in the game; we also implement the DeepRacing algorithm on a 1/10 scale autonomous racing test-bed and show its ability to handle real-world data at high speeds.},
author = {Weiss, Trent and {Suresh Babu}, Varundev and Behl, Madhur},
journal = {NeurIPS 2020 Mach. Learn. Auton. Driv. Work.},
number = {NeurIPS},
pages = {1-10},
title = {{B` ezier Curve Based End-to-End Trajectory Synthesis for Agile Autonomous Driving}},
year = {2020}
}
@article{Quadflieg2011,
author = {Quadflieg, Jan and Preuss, Mike},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Quadflieg, Preuss - 2011 - Driving Faster Than a Human Player(2).pdf:pdf},
number = {section 2},
pages = {143--144},
title = {{Driving Faster Than a Human Player}},
url = {https://link.springer.com/chapter/10.1007/978-3-642-20525-5{\_}15},
year = {2011}
}
@misc{IEEEexample:bibtexuser,
author = {Patashnik, Oren},
howpublished = {btxdoc.pdf},
month = {feb},
title = {{{\{}$\backslash$BibTeX{\}}ing}},
url = {http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/},
year = {1988}
}
@misc{IEEEexample:frenchpatreq,
author = {Kowalik, F and Isard, M},
number = {9500261},
title = {{Estimateur d'un D{\'{e}}faut de Fonctionnement d'un Modulateur en Quadrature et {\'{E}}tage de Modulation l'Utilisant}},
type = {Patent Request}
}

@article{brunnbauer2021,
abstract = {Despite the rich theoretical foundation of model-based deep reinforcement learning (RL) agents, their effectiveness in real-world robotics-applications is less studied and understood. In this paper, we, therefore, investigate how such agents generalize to real-world autonomous-vehicle control-tasks, where advanced model-free deep RL algorithms fail. In particular, we set up a series of time-lap tasks for an F1TENTH racing robot, equipped with high-dimensional LiDAR sensors, on a set of test tracks with a gradual increase in their complexity. In this continuous-control setting, we show that model-based agents capable of learning in imagination, substantially outperform model-free agents with respect to performance, sample efficiency, successful task completion, and generalization. Moreover, we show that the generalization ability of model-based agents strongly depends on the observation-model choice. Finally, we provide extensive empirical evidence for the effectiveness of model-based agents provided with long enough memory horizons in sim2real tasks.},
annote = {Goal: Learn how to drive autonomously from LiDAR input to finish a lap without collisions
Model free - agent gets stuck in local minima
Model based - learn state transition model, solves complex tasks
Learn directly from lidar input - learns dynamics model},
archivePrefix = {arXiv},
author = {Brunnbauer, Axel and Berducci, Luigi and Brandst{\"{a}}tter, Andreas and Lechner, Mathias and Hasani, Ramin and Rus, Daniela and Grosu, Radu},
title = {Model-based versus Model-free Deep Reinforcement Learning for Autonomous Racing Cars},
url = {https://arxiv.org/pdf/2103.04909v1.pdf},
year = {2021}
}

@article{Osinski2020,
abstract = {We use reinforcement learning in simulation to obtain a driving system controlling a full-size real-world vehicle. The driving policy takes RGB images from a single camera and their semantic segmentation as input. We use mostly synthetic data, with labelled real-world data appearing only in the training of the segmentation network.Using reinforcement learning in simulation and synthetic data is motivated by lowering costs and engineering effort.In real-world experiments we confirm that we achieved successful sim-to-real policy transfer. Based on the extensive evaluation, we analyze how design decisions about perception, control, and training impact the real-world performance.},
archivePrefix = {arXiv},
arxivId = {1911.12905},
author = {Osinski, Blazej and Jakubowski, Adam and Ziecina, Pawel and Milos, Piotr and Galias, Christopher and Homoceanu, Silviu and Michalewski, Henryk},
doi = {10.1109/ICRA40945.2020.9196730},
eprint = {1911.12905},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Osinski et al. - 2020 - Simulation-Based Reinforcement Learning for Real-World Autonomous Driving.pdf:pdf},
isbn = {9781728173955},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {6411--6418},
title = {{Simulation-Based Reinforcement Learning for Real-World Autonomous Driving}},
year = {2020}
}
@article{Jain2020,
abstract = {Learning to race autonomously is a challenging problem. It requires perception, estimation, planning, and control to work together in synchronization while driving at the limit of a vehicle's handling capability. Among others, one of the fundamental challenges lies in predicting the vehicle's future states like position, orientation, and speed with high accuracy because it is inevitably hard to identify vehicle model parameters that capture its real nonlinear dynamics in the presence of lateral tire slip. We present a model-based planning and control framework for autonomous racing that significantly reduces the effort required in system identification. Our approach bridges the gap between the design in a simulation and the real world by learning from on-board sensor measurements. Thus, the teams participating in autonomous racing competitions can start racing on new tracks without having to worry about tuning the vehicle model.},
archivePrefix = {arXiv},
arxivId = {2005.04755},
author = {Jain, Achin and Chaudhari, Pratik and Morari, Manfred},
eprint = {2005.04755},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jain, Chaudhari, Morari - 2020 - BAYESRACE Learning to race autonomously using prior experience.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {Autonomous racing,Gaussian processes,Model predictive control,Sim-to-real,System identification},
number = {CoRL},
title = {{BAYESRACE: Learning to race autonomously using prior experience}},
year = {2020}
}
@article{Singh2019,
abstract = {The combination of deep neural network models and reinforcement learning algorithms can make it possible to learn policies for robotic behaviors that directly read in raw sensory inputs, such as camera images, effectively subsuming both estimation and control into one model. However, real-world applications of reinforcement learning must specify the goal of the task by means of a manually programmed reward function, which in practice requires either designing the very same perception pipeline that end-to-end reinforcement learning promises to avoid, or else instrumenting the environment with additional sensors to determine if the task has been performed successfully. In this paper, we propose an approach for removing the need for manual engineering of reward specifications by enabling a robot to learn from a modest number of examples of successful outcomes, followed by actively solicited queries, where the robot shows the user a state and asks for a label to determine whether that state represents successful completion of the task. While requesting labels for every single state would amount to asking the user to manually provide the reward signal, our method requires labels for only a tiny fraction of the states seen during training, making it an efficient and practical approach for learning skills without manually engineered rewards. We evaluate our method on real-world robotic manipulation tasks where the observations consist of images viewed by the robot's camera. In our experiments, our method effectively learns to arrange objects, place books, and drape cloth, directly from images and without any manually specified reward functions, and with only 1-4 hours of interaction with the real world. Videos of learned behavior are available at sites.google.com/view/reward-learning-rl/.},
archivePrefix = {arXiv},
arxivId = {1904.07854},
author = {Singh, Avi and Yang, Larry and Finn, Chelsea and Levine, Sergey},
doi = {10.15607/rss.2019.xv.073},
eprint = {1904.07854},
isbn = {9780992374754},
issn = {2330765X},
keywords = {Deep Learning in Robotics and Automation},
title = {{End-To-End Robotic Reinforcement Learning without Reward Engineering}},
url = {http://www.roboticsproceedings.org/rss15/p73.pdf},
year = {2019}
}
@article{IEEEexample:articlelargepages,
author = {Castaldini, A and Cavallini, A and Fraboni, B and Fernandez, P and Piqueras, J},
journal = {Phys. Rev. B.},
number = {23},
pages = {14897--14900},
title = {{Midgap Traps Related to Compensation Processes in {\{}CdTe{\}} Alloys}},
volume = {56},
year = {1997}
}
@article{Hou2019,
abstract = {Experience replay plays an important role in reinforcement learning. It reuses previous experiences to prevent the input data from being highly correlated. Recently , a deep reinforcement learning algorithm with experience replay, called deep deterministic policy gradient (DDPG), has achieved good performance in many continuous control tasks. However, it assumes the experiences are of equal importance and samples them uniformly, which obviously neglects the difference in the value of each individual experience. To improve the efficiency of experience replay in DDPG method, we propose to replace the original uniform experience replay with prioritized experience replay. We test the algorithms in five tasks in the OpenAI Gym, a testbed for reinforcement learning algorithms. In the experiment, we find that DDPG with prioritized experience replay mechanism significantly outperforms that with uniform sampling in terms of training time, training stability and final performance. We also find that our algorithm can achieve better performance in three tasks compared with some state-of-the-art algorithms like Q-prop, which directly proves the effectiveness of our proposed scheme. Our weekly report and codes are available at https://github.com/cardwing/Codes-for-RL-Project.},
author = {Hou, Yuenan and Zhang, Yi},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hou, Zhang - 2019 - Improving DDPG via Prioritized Experience Replay(2).pdf:pdf},
title = {{Improving DDPG via Prioritized Experience Replay}},
url = {https://github.com/cardwing/Codes-for-RL-Project.},
year = {2019}
}

@misc{Remonda2021,
  doi = {10.48550/ARXIV.2104.11106},
  url = {https://arxiv.org/abs/2104.11106},
  author = {Remonda, Adrian and Krebs, Sarah and Veas, Eduardo and Luzhnica, Granit and Kern, Roman},
  keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Formula RL: Deep Reinforcement Learning for Autonomous Racing using Telemetry Data},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{Rosolia2017,
abstract = {A Learning Model Predictive Controller (LMPC) for linear system is presented. The proposed controller builds on previous work on nonlinear LMPC and decreases its computational burden for linear system. The control scheme is reference-free and is able to improve its performance by learning from previous iterations. A convex safe set and a terminal cost function are used in order to guarantee recursive feasibility and non-increasing performance at each iteration. The paper presents the control design approach, and shows how to recursively construct the convex terminal set and the terminal cost from state and input trajectories of previous iterations. Simulation results show the effectiveness of the proposed control logic.},
archivePrefix = {arXiv},
arxivId = {1702.07064},
author = {Rosolia, Ugo and Borrelli, Francesco},
doi = {10.1016/j.ifacol.2017.08.324},
eprint = {1702.07064},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rosolia, Borrelli - 2017 - Learning Model Predictive Control for Iterative Tasks A Computationally Efficient Approach for Linear Syst(2).pdf:pdf},
issn = {24058963},
journal = {IFAC-PapersOnLine},
keywords = {Convex Optimization,LMPC,Learning,Model Predictive Control},
number = {1},
pages = {3142--3147},
publisher = {Elsevier B.V.},
title = {{Learning Model Predictive Control for Iterative Tasks: A Computationally Efficient Approach for Linear System}},
url = {https://doi.org/10.1016/j.ifacol.2017.08.324},
volume = {50},
year = {2017}
}
@article{Mohammed2019,
abstract = {The autonomous driving field is progressing at a fast pace due to the revolution introduced by Deep Learning technologies in Computer Vision algorithms. To have a functioning autonomous driving vehicle, the system needs to have different functional blocks, e.g, automatic emergency braking (AEB), lane-keeping assist (LKA), active cruise control (ACC), traffic jam assist (TJA), and crash avoidance (CA). Path planning is one of the main blocks of an autonomous driving pipeline that is integrated into the system to ensure that these functionalities are working properly. Traditional path planning algorithms require difficult prerequisites: a) a well-defined map, b) sensor fusion, c) localization, and d) a controller. Unlike the traditional techniques, End-to-End deep path planning and automatic emergency braking solutions are proposed to solve the problem using a single block, instead of the complexities of the four previous blocks. In this paper, we rely only on cameras cocoon that covers 360 o around the vehicle. These cameras are installed on the four sides of the vehicle. They capture a continuous flow of images that are processed by a deep convolutional neural network. The network's output controls three vehicle's components: gas throttle, the steering wheel, and the brakes. We build our own benchmark based on the CARLA simulator due to the in-existence of benchmarks serving our proposed idea. The proposed model shows a generalization capability to differentiate between overtaking and braking, and it is considered as the first deep learning solution for both path planning and automatic emergency braking functionalities. In order to form a good basis that could help to accelerate the contribution in the field of autonomous driving, code and videos are available at https://github.com/eslambakr/Path{\_}Planning{\_}using{\_}Deeplearning},
author = {Mohammed, Eslam and Abdou, Mohammed and Nasr, Omar Ahmed},
number = {NeurIPS},
title = {{End-to-End Deep Path Planning and Automatic Emergency Braking Camera Cocoon-based Solution}},
url = {https://github.com/eslambakr/Path{\_}Planning{\_}using{\_}Deeplearning},
year = {2019}
}
@misc{IEEEexample:electronhowinfo2,
author = {Valloppillil, V and Ross, K W},
howpublished = {Internet draft},
title = {{Cache Array Routing Protocol v1.1}},
url = {http://ds1.internic.net/internet-drafts/draft-vinod-carp-v1-03.txt},
year = {1998}
}
@book{IEEEexample:bookwitheditor,
address = {New York},
editor = {Candy, J C and Temes, G C},
publisher = {{\{}IEEE{\}} Press.},
title = {{Oversampling Delta-Sigma Data Converters Theory, Design and Simulation}},
year = {1992}
}
@article{Okada2021,
abstract = {In the present paper, we propose a decoder-free extension of Dreamer, a leading model-based reinforcement learning (MBRL) method from pixels. Dreamer is a sample- and cost-efficient solution to robot learning, as it is used to train latent state-space models based on a variational autoencoder and to conduct policy optimization by latent trajectory imagination. However, this autoencoding based approach often causes object vanishing, in which the autoencoder fails to perceives key objects for solving control tasks, and thus significantly limiting Dreamer's potential. This work aims to relieve this Dreamer's bottleneck and enhance its performance by means of removing the decoder. For this purpose, we firstly derive a likelihood-free and InfoMax objective of contrastive learning from the evidence lower bound of Dreamer. Secondly, we incorporate two components, (i) independent linear dynamics and (ii) the random crop data augmentation, to the learning scheme so as to improve the training performance. In comparison to Dreamer and other recent model-free reinforcement learning methods, our newly devised Dreamer with InfoMax and without generative decoder (Dreaming) achieves the best scores on 5 difficult simulated robotics tasks, in which Dreamer suffers from object vanishing.},
archivePrefix = {arXiv},
arxivId = {2007.14535},
author = {Okada, Masashi and Taniguchi, Tadahiro},
doi = {10.1109/ICRA48506.2021.9560734},
eprint = {2007.14535},
isbn = {9781728190778},
issn = {10504729},
journal = {Proc. - IEEE Int. Conf. Robot. Autom.},
pages = {4209--4215},
title = {{Dreaming: Model-based Reinforcement Learning by Latent Imagination without Reconstruction}},
volume = {2021-May},
year = {2021}
}
@misc{Song2021,
  doi = {10.48550/ARXIV.2103.14666},
  url = {https://doi.org/10.48550/arXiv.2103.14666},
  author = {Song, Yunlong and Lin, HaoChih and Kaufmann, Elia and Duerr, Peter and Scaramuzza, Davide},
  keywords = {Robotics (cs.RO), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Autonomous Overtaking in Gran Turismo Sport Using Curriculum Reinforcement Learning},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Liniger2015,
abstract = {Summary This paper describes autonomous racing of RC race cars based on mathematical optimization. Using a dynamical model of the vehicle, control inputs are computed by receding horizon based controllers, where the objective is to maximize progress on the track subject to the requirement of staying on the track and avoiding opponents. Two different control formulations are presented. The first controller employs a two-level structure, consisting of a path planner and a nonlinear model predictive controller (NMPC) for tracking. The second controller combines both tasks in one nonlinear optimization problem (NLP) following the ideas of contouring control. Linear time varying models obtained by linearization are used to build local approximations of the control NLPs in the form of convex quadratic programs (QPs) at each sampling time. The resulting QPs have a typical MPC structure and can be solved in the range of milliseconds by recent structure exploiting solvers, which is key to the real-time feasibility of the overall control scheme. Obstacle avoidance is incorporated by means of a high-level corridor planner based on dynamic programming, which generates convex constraints for the controllers according to the current position of opponents and the track layout. The control performance is investigated experimentally using 1:43 scale RC race cars, driven at speeds of more than 3 m/s and in operating regions with saturated rear tire forces (drifting). The algorithms run at 50 Hz sampling rate on embedded computing platforms, demonstrating the real-time feasibility and high performance of optimization-based approaches for autonomous racing.},
archivePrefix = {arXiv},
arxivId = {1711.07300},
author = {Liniger, Alexander and Domahidi, Alexander and Morari, Manfred},
doi = {10.1002/oca.2123},
eprint = {1711.07300},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liniger, Domahidi, Morari - 2015 - Optimization-based autonomous racing of 143 scale RC cars(2).pdf:pdf},
issn = {10991514},
journal = {Optimal Control Applications and Methods},
keywords = {autonomous racing,drift control,fast model predictive control,nonlinear model predictive control, nonlinear rece,vehicle control},
number = {5},
pages = {628--647},
title = {{Optimization-based autonomous racing of 1:43 scale RC cars}},
volume = {36},
year = {2015},
url={https://doi.org/10.1002/oca.2123}
}

@InProceedings{Schwarting2021,
  title = 	 {Deep Latent Competition: Learning to Race Using Visual Control Policies in Latent Space},
  author =       {Schwarting, Wilko and Seyde, Tim and Gilitschenski, Igor and Liebenwein, Lucas and Sander, Ryan and Karaman, Sertac and Rus, Daniela},
  booktitle = 	 {Proceedings of the 2020 Conference on Robot Learning},
  pages = 	 {1855--1870},
  year = 	 {2021},
  editor = 	 {Kober, Jens and Ramos, Fabio and Tomlin, Claire},
  volume = 	 {155},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16--18 Nov},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v155/schwarting21a/schwarting21a.pdf},
  url = 	 {https://proceedings.mlr.press/v155/schwarting21a.html},
  abstract = 	 {Learning competitive behaviors in multi-agent settings such as racing requires long-term reasoning about potential adversarial interactions. This paper presents Deep Latent Competition (DLC), a novel reinforcement learning algorithm that learns competitive visual control policies through self-play in imagination. The DLC agent imagines multi-agent interaction sequences in the compact latent space of a learned world model that combines a joint transition function with opponent viewpoint prediction. Imagined self-play reduces costly sample generation in the real world, while the latent representation enables planning to scale gracefully with observation dimensionality.  We demonstrate the effectiveness of our algorithm in learning competitive behaviors on a novel multi-agent racing benchmark that requires planning from image observations. Code and videos available at https://sites.google.com/view/deep-latent-competition.}
}

@article{Beal2013,
abstract = {Recent developments in vehicle steering systems offer new opportunities to measure the steering torque and reliably estimate the vehicle sideslip and the tire-road friction coefficient. This paper presents an approach to vehicle stabilization that leverages these estimates to define state boundaries that exclude unstable vehicle dynamics and utilizes a model predictive envelope controller to bound the vehicle motion within this stable region of the state space. This approach provides a large operating region accessible by the driver and smooth interventions at the stability boundaries. Experimental results obtained with a steer-by-wire vehicle and a proof of envelope invariance demonstrate the efficacy of the envelope controller in controlling the vehicle at the limits of handling. {\textcopyright} 1993-2012 IEEE.},
author = {Beal, Craig Earl and Gerdes, J. Christian},
doi = {10.1109/TCST.2012.2200826},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Beal, Gerdes - 2013 - Model predictive control for vehicle stabilization at the limits of handling.pdf:pdf},
issn = {10636536},
journal = {IEEE Transactions on Control Systems Technology},
keywords = {Driver assistance,envelope control,model predictive control (MPC),vehicle dynamics,vehicle safety},
number = {4},
pages = {1258--1269},
publisher = {IEEE},
title = {{Model predictive control for vehicle stabilization at the limits of handling}},
volume = {21},
year = {2013}
}
@article{Groß2008,
abstract = {In this paper the application of reinforcement learning to Tetris is investigated, particulary the idea of temporal difference learning is applied to estimate the state value function V. For two predefined reward functions Tetris agents have been trained by using a {\_}-greedy policy. In the numerical experiments it can be observed that the trained agents can outperform fixed policy agents significantly, e.g. by factor 5 for a complex reward function.},
author = {Gro{\ss}, Alexander and Friedland, Jan and Schwenker, Friedhelm},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gro{\ss}, Friedland, Schwenker - 2008 - Learning to play Tetris applying reinforcement learning methods.pdf:pdf},
isbn = {2930307080},
journal = {ESANN 2008 Proceedings, 16th European Symposium on Artificial Neural Networks - Advances in Computational Intelligence and Learning},
number = {April},
pages = {131--136},
title = {{Learning to play Tetris applying reinforcement learning methods}},
url = {https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.218.4954{\&}rep=rep1{\&}type=pdf},
year = {2008}
}

@article{Gabillon2013,
abstract = {Tetris is a video game that has been widely used as a benchmark for various optimization techniques including approximate dynamic programming (ADP) algorithms. A look at the literature of this game shows that while ADP algorithms that have been (almost) entirely based on approximating the value function (value function based) have performed poorly in Tetris, the methods that search directly in the space of policies by learning the policy parameters using an optimization black box, such as the cross entropy (CE) method, have achieved the best reported results. This makes us conjecture that Tetris is a game in which good policies are easier to represent, and thus, learn than their corresponding value functions. So, in order to obtain a good performance with ADP, we should use ADP algorithms that search in a policy space, instead of the more traditional ones that search in a value function space. In this paper, we put our conjecture to test by applying such an ADP algorithm, called classification-based modified policy iteration (CBMPI), to the game of Tetris. Our experimental results show that for the first time an ADP algorithm, namely CBMPI, obtains the best results reported in the literature for Tetris in both small 10 × 10 and large 10 × 20 boards. Although the CBMPI's results are similar to those of the CE method in the large board, CBMPI uses considerably fewer (almost 1/6) samples (calls to the generative model) than CE.},
author = {Gabillon, Victor and Ghavamzadeh, Mohammad and Scherrer, Bruno},
issn = {10495258},
journal = {Adv. Neural Inf. Process. Syst.},
pages = {1--9},
title = {{Approximate dynamic programming finally performswell in the game of Tetris}},
year = {2013}
}

@article{Yu2020,
abstract = {Existing mobile robots cannot complete some functions. To solve these problems, which include autonomous learning in path planning, the slow convergence of path planning, and planned paths that are not smooth, it is possible to utilize neural networks to enable to the robot to perceive the environment and perform feature extraction, which enables them to have a fitness of environment to state action function. By mapping the current state of these actions through Hierarchical Reinforcement Learning (HRL), the needs of mobile robots are met. It is possible to construct a path planning model for mobile robots based on neural networks and HRL. In this article, the proposed algorithm is compared with different algorithms in path planning. It underwent a performance evaluation to obtain an optimal learning algorithm system. The optimal algorithm system was tested in different environments and scenarios to obtain optimal learning conditions, thereby verifying the effectiveness of the proposed algorithm. Deep Deterministic Policy Gradient (DDPG), a path planning algorithm for mobile robots based on neural networks and hierarchical reinforcement learning, performed better in all aspects than other algorithms. Specifically, when compared with Double Deep Q-Learning (DDQN), DDPG has a shorter path planning time and a reduced number of path steps. When introducing an influence value, this algorithm shortens the convergence time by 91{\%} compared with the Q-learning algorithm and improves the smoothness of the planned path by 79{\%}. The algorithm has a good generalization effect in different scenarios. These results have significance for research on guiding, the precise positioning, and path planning of mobile robots.},
annote = {Technique: 
Deterministic policy gradient and hierarchical reinforcement learning - AC3 added to DQN
End to end agent

Better than: 
Double deep Q-learning - reduced time and shorter path

Problem solved: 
Slow convergence in path planning
Algorithm not dependent on environment

Requirements: 
Smoothness of path meet dynamics of robot
Shortest collision free path

Notes on implementation:
Rewards are scales between -1 and 1
Softmax for policy selection
Feature point positioning through gaussian window

Conclusion:
DQN is effective in end to end learning
DDQN can have continuous action space

Future work:
Cannot 'trial and error' on physical system
Path planning only for static scenarios},
author = {Yu, Jinglun and Su, Yuancheng and Liao, Yifan},
doi = {10.3389/fnbot.2020.00063},
issn = {16625218},
journal = {Front. Neurorobot.},
keywords = {fusion algorithm,hierarchical reinforcement learning,mobile robot,neural network,path planning},
number = {October},
pages = {1--12},
title = {{The Path Planning of Mobile Robot by Neural Networks and Hierarchical Reinforcement Learning}},
volume = {14},
year = {2020}
}
@article{Brunner2018b,
abstract = {We propose an optimization based, data-driven framework to design controllers for repetitive tasks. The proposed framework builds on previous work of Learning Model Predictive Control and focuses on problems where the terminal condition of one iteration is the initial condition of the next iteration. A terminal cost and a sampled safe set are learned from data to guarantee recursive feasibility and non-decreasing performance cost at each iteration. The proposed control logic is tested on an autonomous racing example, where the vehicle dynamics are identified online. Experimental results on a 1:10 scale RC car illustrate the effectiveness of the proposed approach.},
author = {Brunner, Maximilian and Rosolia, Ugo and Gonzales, Jon and Borrelli, Francesco},
doi = {10.1109/CDC.2017.8264027},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Brunner et al. - 2018 - Repetitive learning model predictive control An autonomous racing example(2).pdf:pdf},
isbn = {9781509028733},
journal = {2017 IEEE 56th Annual Conference on Decision and Control, CDC 2017},
number = {Cdc},
pages = {2545--2550},
title = {{Repetitive learning model predictive control: An autonomous racing example}},
volume = {2018-Janua},
year = {2018}
}
@article{Tobin2017,
abstract = {Bridging the 'reality gap' that separates simulated robotics from experiments on hardware could accelerate robotic research through improved data availability. This paper explores domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization, which is a stepping stone to general robotic manipulation skills. We find that it is possible to train a real-world object detector that is accurate to 1.5 cm and robust to distractors and partial occlusions using only data from a simulator with non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping in a cluttered environment. To our knowledge, this is the first successful transfer of a deep neural network trained only on simulated RGB images (without pre-training on real images) to the real world for the purpose of robotic control.},
archivePrefix = {arXiv},
arxivId = {1703.06907},
author = {Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
doi = {10.1109/IROS.2017.8202133},
eprint = {1703.06907},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tobin et al. - 2017 - Domain randomization for transferring deep neural networks from simulation to the real world.pdf:pdf},
isbn = {9781538626825},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {23--30},
title = {{Domain randomization for transferring deep neural networks from simulation to the real world}},
volume = {2017-Septe},
year = {2017}
}
@article{Liniger2019,
abstract = {In this paper, we consider autonomous driving of miniature race cars. The viability kernel is used to efficiently generate finite look-ahead trajectories that maximize progress while remaining recursively feasible with respect to static obstacles (e.g., stay inside the track). Together with a low-level model predictive controller, this method makes real-time autonomous racing possible. The viability kernel computation is based on space discretization. To make the calculation robust against discretization errors, we propose a novel numerical scheme based on game theoretical methods, in particular the discriminating kernel. We show that the resulting algorithm provides an inner approximation of the viability kernel and guarantees that, for all states in the cell surrounding a viable grid point, there exists a control that keeps the system within the kernel. The performance of the proposed control method is studied in simulation where we determine the effects of various design choices and parameters and in experiments on an autonomous racing setup maintained at the Automatic Control Laboratory of ETH Zurich. Both simulation and experimental results suggest that the more conservative approximation using the discriminating kernel results in safer driving style at the cost of a small increase in lap time.},
archivePrefix = {arXiv},
arxivId = {1701.08735},
author = {Liniger, Alexander and Lygeros, John},
doi = {10.1109/TCST.2017.2772903},
eprint = {1701.08735},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liniger, Lygeros - 2019 - Real-Time Control for Autonomous Racing Based on Viability Theory.pdf:pdf},
issn = {1558-0865},
journal = {IEEE Transactions on Control Systems Technology},
keywords = {Autonomous racing,hierarchical control,real-time control,receding horizon control,recursive feasibility,viability theory},
number = {2},
pages = {464--478},
publisher = {IEEE},
title = {{Real-Time Control for Autonomous Racing Based on Viability Theory}},
volume = {27},
year = {2019}
}
@misc{IEEEexample:urlsty,
author = {Arseneau, Donald},
month = {mar},
title = {{The url.sty package}},
url = {http://www.ctan.org/tex-archive/macros/latex/contrib/other/misc/},
year = {1999}
}
@article{Perez2008,
abstract = {The techniques and the technologies supportingAutomatic Vehicle Guidance are important issues. Automobile manufacturers view automatic driving as a very interesting product with motivating key features which allow improvement of the car safety, reduction in emission or fuel consumption or optimization of driver comfort during long journeys. Car racing is an active research field where new advances in aerodynamics, consumption and engine power are critical each season. Our proposal is to research how evolutionary computation techniques can help in this field. For this work we have designed an automatic controller that learns rules with a genetic algorithm. This paper is a report of the results obtained by this controller during the car racing competition held in Hong Kong during the IEEE World Congress on Computational Intelligence (WCCI 2008). {\textcopyright}2008 IEEE.},
author = {Perez, Diego and Saez, Yago and Recio, Gustavo and Isasi, Pedro},
doi = {10.1109/CIG.2008.5035659},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Perez et al. - 2008 - Evolving a rule system controller for automatic driving in a car racing competition(2).pdf:pdf},
isbn = {9781424429745},
journal = {2008 IEEE Symposium on Computational Intelligence and Games, CIG 2008},
pages = {336--342},
title = {{Evolving a rule system controller for automatic driving in a car racing competition}},
url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp={\&}arnumber=5035659},
year = {2008}
}
@misc{IEEEexample:motmanual,
institution = {Motorola},
title = {{{\{}FLEXChip{\}} Signal Processor ({\{}MC68175/D{\}})}},
year = {1996}
}
@article{IEEEexample:bluebookmanual,
address = {Newport Beach, CA},
author = {{Consulative Committee for Space Data Systems (CCSDS)}},
howpublished = {ser. Blue Book, No. 4},
institution = {Consulative Committee for Space Data Systems (CCSDS)},
journal = {Blue Book},
month = {may},
number = {4},
publisher = {CCSDS},
series = {Blue Book},
title = {{Telemetry Channel Coding}},
type = {Recommendation for Space Data System Standard},
url = {http://www.ccsds.org/documents/pdf/CCSDS-101.0-B-4.pdf},
year = {1999}
}
@article{Wang2021,
abstract = {A model-based, data-driven control framework is introduced within the context of autonomous driving in this study. We propose a data-driven control algorithm that combines autonomous system identification using model-free learning and robust control using a model-based controller design. We present a full solution framework that is capable to automatically generate tire-friction limit path while performing system identification of a vehicle with unknown dynamics. We then design model-based control which is actively learned from a data-driven approach. Based on our new system identification algorithm, we can approximate an accurate, explainable, and linearized system representation in a high-dimensional latent space, without any prior knowledge of the system. To validate the algorithm, we conduct the model predictive control of an autonomous vehicle based on the augmented system identification on a scaled racing vehicle. The result indicates that we are able to design control in the lifted space to achieve tasks in path control and obstacle avoidance. The automatic path generation combined with the data driven control requires no a-priori knowledge of the vehicle and also proved to be effective that only requires less than 5 laps to design an low lap-time trajectory while identified a system that is able to achieve minimum lap time without extra learning episodes.},
author = {Wang, Rongyao and Han, Yiqiang and Vaidya, Umesh},
journal = {Early access},
keywords = {Deep Learning,Dynamic Program-ming,Index Terms-Data-driven control,Linear operator approach,Model Predictive Control},
pages = {1--6},
title = {Deep Koopman Data-Driven Optimal Control Framework for Autonomous Racing Deep Koopman Data-Driven Optimal Control Framework for Autonomous Racing},
url = {https://linklab-uva.github.io/icra-autonomous-racing/contributed_papers/paper12.pdf},
year = {2021}

}
@article{Bojarski2020,
abstract = {Four years ago, an experimental system known as PilotNet became the first NVIDIA system to steer an autonomous car along a roadway. This system represents a departure from the classical approach for self-driving in which the process is manually decomposed into a series of modules, each performing a different task. In PilotNet, on the other hand, a single deep neural network (DNN) takes pixels as input and produces a desired vehicle trajectory as output; there are no distinct internal modules connected by human-designed interfaces. We believe that handcrafted interfaces ultimately limit performance by restricting information flow through the system and that a learned approach, in combination with other artificial intelligence systems that add redundancy, will lead to better overall performing systems. We continue to conduct research toward that goal. This document describes the PilotNet lane-keeping effort, carried out over the past five years by our NVIDIA PilotNet group in Holmdel, New Jersey. Here we present a snapshot of system status in mid-2020 and highlight some of the work done by the PilotNet group.},
archivePrefix = {arXiv},
arxivId = {2010.08776},
author = {Bojarski, Mariusz and Chen, Chenyi and Daw, Joyjit and Değirmenci, Alperen and Deri, Joya and Firner, Bernhard and Flepp, Beat and Gogri, Sachin and Hong, Jesse and Jackel, Lawrence and Jia, Zhenhua and Lee, BJ and Liu, Bo and Liu, Fei and Muller, Urs and Payne, Samuel and Prasad, Nischal Kota Nagendra and Provodin, Artem and Roach, John and Rvachov, Timur and Tadimeti, Neha and van Engelen, Jesper and Wen, Haiguang and Yang, Eric and Yang, Zongyi},
eprint = {2010.08776},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bojarski et al. - 2020 - The NVIDIA PilotNet Experiments.pdf:pdf},
pages = {1--28},
title = {{The NVIDIA PilotNet Experiments}},
url = {http://arxiv.org/abs/2010.08776},
year = {2020}
}
@article{Bellemare2017,
abstract = {In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman's equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.},
archivePrefix = {arXiv},
arxivId = {1707.06887},
author = {Bellemare, Marc G and Dabney, Will and Munos, R{\'{e}}mi},
eprint = {1707.06887},
isbn = {9781510855144},
journal = {34th Int. Conf. Mach. Learn. ICML 2017},
pages = {693--711},
title = {{A distributional perspective on reinforcement learning}},
volume = {1},
year = {2017}
}
@article{Xiao2019,
abstract = {Moving in complex environments is an essential capability of intelligent mobile robots. Decades of research and engineering have been dedicated to developing sophisticated navigation systems to move mobile robots from one point to another. Despite their overall success, a recently emerging research thrust is devoted to developing machine learning techniques to address the same problem, based in large part on the success of deep learning. However, to date, there has not been much direct comparison between the classical and emerging paradigms to this problem. In this article, we survey recent works that apply machine learning for motion control in mobile robot navigation, within the context of classical navigation systems. The surveyed works are classified into different categories, which delineate the relationship of the learning approaches to classical methods. Based on this classification, we identify common challenges and promising future directions. Keywords},
archivePrefix = {arXiv},
arxivId = {arXiv:2011.13112v1},
author = {Xiao, Xuesu and Liu, Bo and Warnell, Garrett and Stone, Peter},
doi = {10.1177/ToBeAssigned},
eprint = {arXiv:2011.13112v1},
journal = {J. Vib. Control},
keywords = {[PHYS.MECA.VIBR]Physics [physics]/Mechanics [physi,bit-rock stochastic interaction model,drillstring dynamics,experimental identification,hysteretic friction},
pages = {107754631982824},
title = {{Motion Control for Mobile Robot Navigation Using Machine Learning: a Survey}},
year = {2019}
}
@article{Baheri2020,
abstract = {In this paper, we present a safe deep reinforcement learning system for automated driving. The proposed framework leverages merits of both rule-based and learning-based approaches for safety assurance. Our safety system consists of two modules namely handcrafted safety and dynamically-learned safety. The handcrafted safety module is a heuristic safety rule based on common driving practice that ensure a minimum relative gap to a traffic vehicle. On the other hand, the dynamically-learned safety module is a data-driven safety rule that learns safety patterns from driving data. Specifically, the dynamically-leaned safety module incorporates a model lookahead beyond the immediate reward of reinforcement learning to predict safety longer into the future. If one of the future states leads to a near-miss or collision, then a negative reward will be assigned to the reward function to avoid collision and accelerate the learning process. We demonstrate the capability of the proposed framework in a simulation environment with varying traffic density. Our results show the superior capabilities of the policy enhanced with dynamically-learned safety module.},
archivePrefix = {arXiv},
arxivId = {1910.12905},
author = {Baheri, Ali and Nageshrao, Subramanya and Tseng, H. Eric and Kolmanovsky, Ilya and Girard, Anouck and Filev, Dimitar},
doi = {10.1109/IV47402.2020.9304744},
eprint = {1910.12905},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Baheri et al. - 2020 - Deep Reinforcement Learning with Enhanced Safety for Autonomous Highway Driving.pdf:pdf},
journal = {IEEE Intelligent Vehicles Symposium, Proceedings},
pages = {1550--1555},
title = {{Deep Reinforcement Learning with Enhanced Safety for Autonomous Highway Driving}},
year = {2020}
}
@book{IEEEexample:bookwithseriesvolume,
address = {Berlin, Germany},
editor = {Breckling, J},
publisher = {Springer},
series = {Lecture Notes in Statistics},
title = {{The Analysis of Directional Time Series: Applications to Wind Speed and Direction}},
volume = {61},
year = {1989}
}
@article{Kakade2002,
abstract = {We provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the parameter space. Alt hough gradient methods cannot make large changes in the values of the parameters, we show that the natural gradient is moving toward choosing a greedy optimal act ion rat her than just a better action. These greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate, compatible value functions, as defined by Sutton et al. [9|. We then show drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris.},
author = {Kakade, Sham},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kakade - 2002 - A natural policy gradient.pdf:pdf},
isbn = {0262042088},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
title = {{A natural policy gradient}},
year = {2002}
}
@article{Haskell2012,
abstract = {We are interested in risk constraints for discrete time Markov decision processes (MDPs). Starting with the average reward case, we argue that stochastic dominance constraints are natural risk constraints for MDPs. Specifically, we constrain the empirical distribution of reward to dominate a benchmark distribution in the increasing concave stochastic order. We argue that the optimal policy for the dominance-constrained MDP is a stationary randomized policy. Further, the optimal policy can be computed from a linear program in the space of occupation measures, where the dominance constraint is represented by linear inequalities. The dual of this linear program is computed and is shown to be close to the usual linear programming form of the average dynamic programming optimality equations. In our case, a pricing term appears in the dual corresponding to the dominance constraints. We carry out a parallel development for the discounted reward case with stochastic dominance constraints. We also extend this development to cover multivariate stochastic dominance. {\textcopyright} 2012 IEEE.},
author = {Haskell, William B. and Jain, Rahul},
doi = {10.1109/CDC.2012.6426596},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Haskell, Jain - 2012 - Dominance-constrained Markov decision processes.pdf:pdf},
issn = {01912216},
journal = {Proceedings of the IEEE Conference on Decision and Control},
pages = {5991--5996},
title = {{Dominance-constrained Markov decision processes}},
year = {2012}
}
@misc{IEEEexample:shellCTANpage,
author = {Shell, Michael},
title = {{{\{}IEEE{\}}tran Homepage on {\{}CTAN{\}}}},
url = {http://www.ctan.org/tex-archive/macros/latex/contrib/supported/IEEEtran/},
year = {2002}
}
@article{Ueter2020,
abstract = {Competences required for designing cyber-physical systems cannot be acquired in classical lecture-based education. This article explains how project-based learning enables students to work in teams on problems extending beyond their traditional disciplines. The approach can be used as a blueprint for project-based teaching of CPS design.-Peter Marwedel, TU Dortmund},
author = {Ueter, Niklas and Chen, Kuan Hsun and Chen, Jian Jia},
doi = {10.1109/MDAT.2020.3012085},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ueter, Chen, Chen - 2020 - Project-Based CPS Education A Case Study of an Autonomous Driving Student Project.pdf:pdf},
issn = {21682364},
journal = {IEEE Design and Test},
keywords = {Autonomous Driving,Cyber-Physical Systems,Education,Engineering,Robotics,Student Projects},
number = {6},
pages = {39--46},
title = {{Project-Based CPS Education: A Case Study of an Autonomous Driving Student Project}},
volume = {37},
year = {2020}
}
@misc{IEEEexample:IEEEwebsite,
title = {{The {\{}IEEE{\}} Website}},
url = {http://www.ieee.org/},
year = {2002}
}
@article{Hafner2019,
abstract = {Planning has been very successful for control tasks with known environment dynamics. To leverage planning in unknown environments, the agent needs to learn the dynamics from interactions with the world. However, learning dynamics models that are accurate enough for planning has been a long-standing challenge, especially in image-based domains. We propose the Deep Planning Network (PlaNet), a purely model-based agent that learns the environment dynamics from images and chooses actions through fast online planning in latent space. To achieve high performance, the dynamics model must accurately predict the rewards ahead for multiple time steps. We approach this using a latent dynamics model with both deterministic and stochastic transition components. Moreover, we propose a multi-step variational inference objective that we name latent overshooting. Using only pixel observations, our agent solves continuous control tasks with contact dynamics, partial observability, and sparse rewards, which exceed the difficulty of tasks that were previously solved by planning with learned models. PlaNet uses substantially fewer episodes and reaches final performance close to and sometimes higher than strong model-free algorithms.},
archivePrefix = {arXiv},
arxivId = {1811.04551},
author = {Hafner, Danijar and Lillicrap, Timothy and Fischer, Ian and Villegas, Ruben and Ha, David and Lee, Honglak and Davidson, James},
eprint = {1811.04551},
isbn = {9781510886988},
journal = {36th International Conference for Machince Learning ICML 2019},
pages = {4528--4547},
title = {{Learning latent dynamics for planning from pixels}},
volume = {2019-June},
year = {2019}
}
@misc{IEEEexample:miscforum,
author = {Roberts, L},
howpublished = {{\{}ATM{\}} Forum Contribution 94-0735R1},
month = {aug},
title = {{Enhanced Proportional Rate Control Algorithm {\{}PRCA{\}}}},
year = {1994}
}
@article{Panov2018,
abstract = {Single-shot grid-based path finding is an important problem with the applications in robotics, video games etc. Typically in AI community heuristic search methods (based on A And its variations) are used to solve it. In this work we present the results of preliminary studies on how neural networks can be utilized to path planning on square grids, e.g. how well they can cope with path finding tasks by themselves within the well-known reinforcement problem statement. Conducted experiments show that the agent using neural Q-learning algorithm robustly learns to achieve the goal on small maps and demonstrate promising results on the maps have ben never seen by him before.},
annote = {Technique: 
Grid based path planning
Deep Q-learning

Algorithms for grid based path planning:
A*, IDA*, ARA*, JPS, Theta* - all heuristic search algorithms

Implementation:
Agent is holonomic - actions are move up, down, left and right
Input state space - grid with robot at center, 1 is occupied, 0 is unoccupied
Model free learning

Neural network notes:
Loss function - mean squared error
Convolutional layers
Dropout for regularisation and overfitting avoidance
Dense output layer with linear activation function

2 phases:
Acting - Add to memory
Learning - Shuffle memory},
author = {Panov, Aleksandr I and Yakovlev, Konstantin S and Suvorov, Roman},
doi = {10.1016/j.procs.2018.01.054},
issn = {18770509},
journal = {Procedia Comput. Sci.},
keywords = {Convolution networks,Neural networks,Path planning,Q-learning,Q-network,Reinforcement learning},
pages = {347--353},
publisher = {Elsevier B.V.},
title = {{Grid path planning with deep reinforcement learning: Preliminary results}},
url = {https://doi.org/10.1016/j.procs.2018.01.054},
volume = {123},
year = {2018}
}
@article{Mahmoud2020,
abstract = {In this work we propose scaling down the image resolution of an autonomous vehicle and measuring the performance difference using pre-determined metrics. We formulated a testing strategy and provided suitable testing metrics for RC driven autonomous vehicles. Our goal is to measure and prove that scaling down an image will result in faster response time and higher speeds. Our model shows an increase in response rate of the neural models, improving safety and results in the car having higher speeds.},
author = {Mahmoud, Yaqub and Okuyama, Yuichi and Fukuchi, Tomohide and Kosuke, Tanaka and Ando, Iori},
doi = {10.1051/shsconf/20207704002},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mahmoud et al. - 2020 - Optimizing Deep-Neural-Network-Driven Autonomous Race Car Using Image Scaling(2).pdf:pdf},
journal = {SHS Web of Conferences},
pages = {04002},
title = {{Optimizing Deep-Neural-Network-Driven Autonomous Race Car Using Image Scaling}},
url = {https://doi.org/10.1051/shsconf/20207704002 },
volume = {77},
year = {2020}
}
@article{Ivanov2020,
abstract = {This paper describes a verification case study on an autonomous racing car with a neural network (NN) controller. Although several verification approaches have been recently proposed, they have only been evaluated on low-dimensional systems or systems with constrained environments. To explore the limits of existing approaches, we present a challenging benchmark in which the NN takes raw LiDAR measurements as input and outputs steering for the car. We train a dozen NNs using reinforcement learning (RL) and show that the state of the art in verification can handle systems with around 40 LiDAR rays. Furthermore, we perform real experiments to investigate the benefits and limitations of verification with respect to the sim2real gap, i.e., the difference between a system's modeled and real performance. We identify cases, similar to the modeled environment, in which verification is strongly correlated with safe behavior. Finally, we illustrate LiDAR fault patterns that can be used to develop robust and safe RL algorithms.},
archivePrefix = {arXiv},
author = {Ivanov, Radoslav and Carpenter, Taylor J. and Weimer, James and Alur, Rajeev and Pappas, George J. and Lee, Insup},
doi = {10.1145/3365365.3382216},
isbn = {9781450370189},
journal = {HSCC 2020 - Proceedings of the 23rd International Conference on Hybrid Systems: Computation and Control ,part of CPS-IoT Week},
keywords = {F1/10 racing,learning for control,neural network verification},
title = {{Case study: Verifying the safety of an autonomous racing car with a neural network controller}},
url = {https://doi.org/10.1145/3365365.3382216},
year = {2020}
}
@inproceedings{IEEEexample:presentedatconf,
author = {Finn, S G and M{\'{e}}dard, M and Barry, R A},
booktitle = {Proc. Int. Conf. Commun.},
title = {{A Novel Approach to Automatic Protection Switching Using Trees}},
year = {1997}
}
@unpublished{IEEEexample:unpublished,
annote = {Unpublished},
author = {Ott, T J and Aggarwal, N},
title = {{{\{}TCP{\}} over {\{}ATM{\}}: {\{}ABR{\}} or {\{}UBR{\}}}}
}
@article{Ulyhuohvv2020,
author = {Ulyhuohvv, Hvljq R I and Rq, Kdvvlv Dvhg and Dqer, O L X and Hgx, Vmwx and Vmwx, Ohrqvrqj and Fq, H G X and Vmwx, Vxqzhltl and Fq, H G X and Od, Lpsohphqwdwlrq and Ri, H U and Xqpdqqhg, W K H and Vwhp, Gulylqj V and Yhklfoh, Frpsohwh and Sdudphwhuv, G Qdplf and Wkh, R I and Dqg, Vlpxodwlrq},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ulyhuohvv et al. - 2020 - 'hvljq ri 'ulyhuohvv 5dflqj {\&}kdvvlv {\%}dvhg rq 03{\&}(2).pdf:pdf},
isbn = {9781728176871},
keywords = {drive by wire,lannin j,s at k s,sac,self drivin j},
pages = {6061--6066},
title = {'hvljq ri 'ulyhuohvv 5dflqj {\&}kdvvlv {\%}dvhg rq 03{\&}},
year = {2020}
}
@article{Szita2006,
abstract = {The cross-entropy method is an efficient and general optimization algorithm. However, its applicability in reinforcement learning (RL) seems to be limited because it often converges to suboptimal policies. We apply noise for preventing early convergence of the cross-entropy method, using Tetris, a computer game, for demonstration. The resulting policy outperforms previous RL algorithms by almost two orders of magnitude. {\textcopyright} 2006 Massachusetts Institute of Technology.},
author = {Szita, Istv{\'{a}}n and Lorincz, Andr{\'{a}}s},
doi = {10.1162/neco.2006.18.12.2936},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Szita, Lorincz - 2006 - Learning tetris using the noisy cross-entropy method.pdf:pdf},
issn = {08997667},
journal = {Neural Computation},
number = {12},
pages = {2936--2941},
pmid = {17052153},
title = {{Learning tetris using the noisy cross-entropy method}},
url = {https://watermark.silverchair.com/neco.2006.18.12.2936.pdf?token=AQECAHi208BE49Ooan9kkhW{\_}Ercy7Dm3ZL{\_}9Cf3qfKAc485ysgAAArYwggKyBgkqhkiG9w0BBwagggKjMIICnwIBADCCApgGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQM8RUw6e2j4RwgTgzRAgEQgIICaZl{\_}OKOE1HMoAHZo6WzP6cKZvnvtKFCPi6},
volume = {18},
year = {2006}
}
@article{Braghin2008,
abstract = {The best race driver is the one that, with a given vehicle, is able to drive on a given track in the shortest possible time. Thus, the only target is the lap time. A race driver model has to do the same. The first step towards this target is to decide which trajectory to follow. In fact, the optimal trajectory is the best compromise between the shortest track and the track that allows to achieve the highest speeds (least curvature track). Thus, the problem of trajectory planning is a bounded optimisation problem that has to take into account not only the geometry of the circuit but also the dynamics of the vehicle. A simplified vehicle dynamic model is used for this purpose. Due to the fact that the vehicle will be driven at its limit performances, although simplified, the model has to correctly reproduce the maximum possible acceleration, a function of the vehicle speed, the maximum possible deceleration, again a function of the vehicle speed, and the maximum lateral acceleration, a function of both the vehicle speed and the steering angle. Knowing the trajectory, the vehicle model allows to determine the lap time. Through an optimisation algorithm it is therefore possible to determine the best compromise between shortest track and track with the minimum curvature, i.e. the trajectory (in terms of track and speed profile) that allows to minimize the time lap. Once the best trajectory has been determined (both in terms of best track and best speed profile), it is necessary to identify the driver's inputs to follow the given trajectory. This task is carried out by considering the driver as a controller that acts on a nonlinear plant (the vehicle) in order to achieve the desired results. Thus, the driver converts the best trajectory into vehicle's inputs. The mutual interaction between plant and controller (the driver's inputs are not only a function of the best trajectory but also of the driver's reactions due to the vehicle's dynamics) is not taken into account in this paper. {\textcopyright} 2008 Elsevier Ltd. All rights reserved.},
annote = {Performs multi-objective global optimisation:
objective 1 = shortest path
objective 2 = least curvature
optimal path is a compromise between the 2 objectives

Does not use vehicle dynamics to optimise trajectory

A controller is used to follow the global trajectory},
author = {Braghin, F. and Cheli, F. and Melzi, S. and Sabbioni, E.},
doi = {10.1016/j.compstruc.2007.04.028},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Braghin et al. - 2008 - Race driver model.pdf:pdf},
issn = {00457949},
journal = {Computers and Structures},
keywords = {Driver's inputs,Numerical model,Race driver,Trajectory planning},
number = {13-14},
pages = {1503--1516},
title = {{Race driver model}},
volume = {86},
year = {2008}
}
@article{Dalal2018,
abstract = {We address the problem of deploying a reinforcement learning (RL) agent on a physical system such as a datacenter cooling unit or robot, where critical constraints must never be violated. We show how to exploit the typically smooth dynamics of these systems and enable RL algorithms to never violate constraints during learning. Our technique is to directly add to the policy a safety layer that analytically solves an action correction formulation per each state. The novelty of obtaining an elegant closed-form solution is attained due to a linearized model, learned on past trajectories consisting of arbitrary actions. This is to mimic the real-world circumstances where data logs were generated with a behavior policy that is implausible to describe mathematically; such cases render the known safety-aware off-policy methods inapplicable. We demonstrate the efficacy of our approach on new representative physics-based environments, and prevail where reward shaping fails by maintaining zero constraint violations.},
archivePrefix = {arXiv},
arxivId = {1801.08757},
author = {Dalal, Gal and Dvijotham, Krishnamurthy and Vecerik, Matej and Hester, Todd and Paduraru, Cosmin and Tassa, Yuval},
eprint = {1801.08757},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dalal et al. - 2018 - Safe Exploration in Continuous Action Spaces.pdf:pdf},
title = {{Safe Exploration in Continuous Action Spaces}},
url = {http://arxiv.org/abs/1801.08757},
year = {2018}
}
@incollection{IEEEexample:incollection,
address = {Englewood Cliffs, NJ},
author = {Sorin, W V},
booktitle = {Fiber Optic Test and Measurement},
editor = {Derickson, D},
publisher = {Prentice-Hall},
title = {{Optical Reflectometry for Component Characterization}},
year = {1998}
}

@article{Altche2018,
abstract = {In order to drive safely and efficiently on public roads, autonomous vehicles will have to understand the intentions of surrounding vehicles, and adapt their own behavior accordingly. If experienced human drivers are generally good at inferring other vehicles' motion up to a few seconds in the future, most current Advanced Driving Assistance Systems (ADAS) are unable to perform such medium-term forecasts, and are usually limited to high-likelihood situations such as emergency braking. In this article, we present a first step towards consistent trajectory prediction by introducing a long short-term memory (LSTM) neural network, which is capable of accurately predicting future longitudinal and lateral trajectories for vehicles on highway. Unlike previous work focusing on a low number of trajectories collected from a few drivers, our network was trained and validated on the NGSIM US-101 dataset, which contains a total of 800 hours of recorded trajectories in various traffic densities, representing more than 6000 individual drivers.},
annote = {LSTM predicts trajectories of other vehicles on a highway using previously observed data (supervised technique)},
archivePrefix = {arXiv},
arxivId = {1801.07962},
author = {Altche, Florent and {De La Fortelle}, Arnaud},
doi = {10.1109/ITSC.2017.8317913},
eprint = {1801.07962},
isbn = {9781538615256},
journal = {IEEE Conf. Intell. Transp. Syst. Proceedings, ITSC},
pages = {353--359},
title = {{An LSTM network for highway trajectory prediction}},
volume = {2018-March},
year = {2018}
}
@misc{IEEEexample:hyperrefsty,
author = {Rahtz, Sebastian and Oberdiek, Heiko},
month = {jul},
title = {{The hyperref.sty package}},
url = {http://www.ctan.org/tex-archive/macros/latex/contrib/supported/hyperref/},
year = {2002}
}
@article{Riedmiller2007,
abstract = {The paper describes our first experiments on Reinforcement Learning to steer a real robot car. The applied method, Neural Fitted Q Iteration (NFQ) is purely data-driven based on data directly collected from real-life experiments, i.e. no transition model and no simulation is used. The RL approach is based on learning a neural Q value function, which means that no prior selection of the structure of the control law is required. We demonstrate, that the controller is able to learn a steering task in less than 20 minutes directly on the real car. We consider this as an important step towards the competitive application of neural Q function based RL methods in real-life environments. {\textcopyright} 2007 IEEE.},
author = {Riedmiller, Martin and Montemerlo, Mike and Dahlkamp, Hendrik},
doi = {10.1109/FBIT.2007.37},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Riedmiller, Montemerlo, Dahlkamp - 2007 - Learning to drive a real car in 20 minutes.pdf:pdf},
isbn = {0769529992},
journal = {Proceedings of the Frontiers in the Convergence of Bioscience and Information Technologies, FBIT 2007},
pages = {645--650},
title = {{Learning to drive a real car in 20 minutes}},
year = {2007}
}
@article{Kahn2017,
abstract = {Reinforcement learning can enable complex, adaptive behavior to be learned automatically for autonomous robotic platforms. However, practical deployment of reinforcement learning methods must contend with the fact that the training process itself can be unsafe for the robot. In this paper, we consider the specific case of a mobile robot learning to navigate an a priori unknown environment while avoiding collisions. In order to learn collision avoidance, the robot must experience collisions at training time. However, high-speed collisions, even at training time, could damage the robot. A successful learning method must therefore proceed cautiously, experiencing only low-speed collisions until it gains confidence. To this end, we present an uncertainty-aware model-based learning algorithm that estimates the probability of collision together with a statistical estimate of uncertainty. By formulating an uncertainty-dependent cost function, we show that the algorithm naturally chooses to proceed cautiously in unfamiliar environments, and increases the velocity of the robot in settings where it has high confidence. Our predictive model is based on bootstrapped neural networks using dropout, allowing it to process raw sensory inputs from high-bandwidth sensors such as cameras. Our experimental evaluation demonstrates that our method effectively minimizes dangerous collisions at training time in an obstacle avoidance task for a simulated and real-world quadrotor, and a real-world RC car. Videos of the experiments can be found at https://sites.google.com/site/probcoll.},
archivePrefix = {arXiv},
arxivId = {1702.01182},
author = {Kahn, Gregory and Villaflor, Adam and Pong, Vitchyr and Abbeel, Pieter and Levine, Sergey},
eprint = {1702.01182},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kahn et al. - 2017 - Uncertainty-Aware Reinforcement Learning for Collision Avoidance(2).pdf:pdf},
title = {{Uncertainty-Aware Reinforcement Learning for Collision Avoidance}},
url = {http://arxiv.org/abs/1702.01182},
year = {2017}
}
@INPROCEEDINGS{Gundu2019,
  author={Gundu, Pavan K. and Vamvoudakis, Kyriakos G. and Gerdes, Ryan M.},
  booktitle={2019 American Control Conference (ACC)}, 
  title={An Intermittent Learning Algorithm for High-Speed Autonomous Driving in Unknown Environments}, 
  year={2019},
  volume={},
  number={},
  pages={4286-4292},
  doi={10.23919/ACC.2019.8814462},
  url = {https://doi.org/10.23919/ACC.2019.8814462}
  }

@inproceedings{IEEEexample:confwithadddays,
address = {Rio de Janeiro, Brazil},
author = {Yee, M S and Hanzo, L},
booktitle = {Proc. {\{}IEEE{\}} Globecom '99},
month = {dec},
pages = {2183--2187},
title = {{Radial Basis Function Decision Feedback Equaliser Assisted Burst-by-burst Adaptive Modulation}},
year = {1999}
}
@misc{IEEEexample:motmanualhowpub,
howpublished = {MC68175/D},
institution = {Motorola},
title = {{{\{}FLEXChip{\}} Signal Processor}},
year = {1996}
}
@misc{IEEEexample:draftasmisc,
author = {Widjaja, I and Elwalid, A},
howpublished = {IETF Draft},
title = {{{\{}MATE{\}}: {\{}MPLS{\}} Adaptive Traffic Engineering}},
year = {1999}
}
@article{Galway2008,
abstract = {Artificial intelligence for digital games constitutes the implementation of a set of algorithms and techniques from both traditional and modern artificial intelligence in order to provide solutions to a range of game dependent problems. However, the majority of current approaches lead to predefined, static and predictable game agent responses, with no ability to adjust during game-play to the behaviour or playing style of the player. Machine learning techniques provide a way to improve the behavioural dynamics of computer controlled game agents by facilitating the automated generation and selection of behaviours, thus enhancing the capabilities of digital game artificial intelligence and providing the opportunity to create more engaging and entertaining game-play experiences. This paper provides a survey of the current state of academic machine learning research for digital game environments, with respect to the use of techniques from neural networks, evolutionary computation and reinforcement learning for game agent control. {\textcopyright}2009 Springer Science+Business Media B.V.},
author = {Galway, Leo and Charles, Darryl and Black, Michaela},
doi = {10.1007/s10462-009-9112-y},
issn = {02692821},
journal = {Artif. Intell. Rev.},
keywords = {Computational intelligence,Digital games,Game aI,Machine learning},
number = {2},
pages = {123--161},
title = {{Machine learning in digital games: A survey}},
url = {https://link.springer.com/content/pdf/10.1007/s10462-009-9112-y.pdf},
volume = {29},
year = {2008}
}
@article{Sell2019,
author = {Sell, Raivo and Rass{\~{o}}lkin, Anton and Wang, Ruxin and Otto, Tauno},
doi = {10.3176/proc.2019.4.07},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sell et al. - 2019 - Integration of autonomous vehicles and Industry 4 . 0.pdf:pdf},
number = {December},
title = {{Integration of autonomous vehicles and Industry 4 . 0}},
url = {file:///C:/Users/Andrew/Downloads/proc-2019-4-389-394(1).pdf},
year = {2019}
}
@incollection{IEEEexample:inbookpagesnote,
address = {Moscow},
annote = {(in Russian)},
author = {Bul, B K},
pages = {464},
publisher = {Energia Press},
title = {{Theory Principles and Design of Magnetic Circuits}},
year = {1964}
}
@misc{IEEEexample:miscrfc,
author = {Ramakrishnan, K K and Floyd, S},
howpublished = {RFC 2481},
month = {jan},
title = {{A Proposal to Add Explicit Congestion Notification ({\{}ECN{\}}) to {\{}IP{\}}}},
year = {1999}
}
@article{Thiery2009,
abstract = {For playing the game of Tetris well, training a controller by the cross-entropy method seems to be a viable way (Szita and Lorincz, 2006; Thiery and Scherrer, 2009). We consider this method to tune an evaluation-based one-piece controller as suggested by Szita and Lorincz and we introduce some improvements. In this context, we discuss the influence of the noise, and we perform experiments with several sets of features such as those introduced by Bertsekas and Tsitsiklis (1996), by Dellacherie (Fahey, 2003), and some original features. This approach leads to a controller that outperforms the previous known results. On the original game of Tetris, we show that with probability 0.95 it achieves at least 910,000 ± 5{\%} lines per game on average. On a simplified version of Tetris considered by most research works, it achieves 35,000,000 ± 20{\%} lines per game on average. We used this approach when we took part with the program BCTS in the 2008 Tetris domain Reinforcement Learning Competition and won the competition.},
annote = {1 piece controller
Drops piece from top

Good description of delacherie's controller!},
author = {Thiery, Christophe and Scherrer, Bruno},
doi = {10.3233/ICG-2009-32104},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thiery, Scherrer - 2009 - Improvements on learning tetris with cross entropy.pdf:pdf},
issn = {13896911},
journal = {ICGA Journal},
number = {1},
pages = {23--33},
title = {{Improvements on learning tetris with cross entropy}},
url = {https://hal.inria.fr/inria-00418930/document},
volume = {32},
year = {2009}
}
@article{Fragkiadaki2020,
author = {Fragkiadaki, Katerina},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fragkiadaki - 2020 - Sim2Real So far.pdf:pdf},
title = {{Sim2Real So far}},
year = {2020}
}
@article{Williams2016,
abstract = {In this paper we present a model predictive control algorithm designed for optimizing non-linear systems subject to complex cost criteria. The algorithm is based on a stochastic optimal control framework using a fundamental relationship between the information theoretic notions of free energy and relative entropy. The optimal controls in this setting take the form of a path integral, which we approximate using an efficient importance sampling scheme. We experimentally verify the algorithm by implementing it on a Graphics Processing Unit (GPU) and apply it to the problem of controlling a fifth-scale Auto-Rally vehicle in an aggressive driving task.},
author = {Williams, Grady and Drews, Paul and Goldfain, Brian and Rehg, James M. and Theodorou, Evangelos A.},
doi = {10.1109/ICRA.2016.7487277},
url = {https://doi.org/10.1109/ICRA.2016.7487277},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {1433--1440},
publisher = {IEEE},
title = {{Aggressive driving with model predictive path integral control}},
volume = {2016-June},
year = {2016}
}
@misc{IEEEexample:datasheet,
address = {Mezzovico, Switzerland},
institution = {Opto Speed SA},
title = {{{\{}PDCA12-70{\}} data sheet}}
}
@book{IEEEexample:book_typical,
address = {Reading, MA},
author = {Cullity, B D},
publisher = {Addison-Wesley},
title = {{Introduction to Magnetic Materials}},
year = {1972}
}
@inproceedings{IEEEexample:confwithvolume,
address = {New York, NY},
author = {Yajnik, M and Moon, S B and Kurose, J and Towsley, D},
booktitle = {Proc. {\{}IEEE{\}} {\{}INFOCOM{\}}'99},
month = {mar},
pages = {345--352},
title = {{Measurement and Modeling of the Temporal Dependence in Packet Loss}},
volume = {1},
year = {1999}
}
@misc{Pan2017a,
  doi = {10.48550/ARXIV.1709.07174},
  url = {https://arxiv.org/abs/1709.07174},
  author = {Pan, Yunpeng and Cheng, Ching-An and Saigol, Kamil and Lee, Keuntaek and Yan, Xinyan and Theodorou, Evangelos and Boots, Byron},
  keywords = {Robotics (cs.RO), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Agile Autonomous Driving using End-to-End Deep Imitation Learning},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Wu,
abstract = {Reinforcement learning requires skillful definition and remarkable computational efforts to solve optimization and control problems, which could impair its prospect. Introducing human guidance into reinforcement learning is a promising way to improve learning performance. In this paper, a comprehensive human guidance-based reinforcement learning framework is established. A novel prioritized experience replay mechanism that adapts to human guidance in the reinforcement learning process is proposed to boost the efficiency and performance of the reinforcement learning algorithm. To relieve the heavy workload on human participants, a behavior model is established based on an incremental online learning method to mimic human actions. We design two challenging autonomous driving tasks for evaluating the proposed algorithm. Experiments are conducted to access the training and testing performance and learning mechanism of the proposed algorithm. Comparative results against the state-of-the-arts suggest the advantages of our algorithm in terms of learning efficiency, performance, and robustness.},
author = {Wu, Jingda and Member, Student and Huang, Zhiyu and Huang, Wenhui and Lv, Chen and Member, Senior},
isbn = {1922500046},
keywords = {Index Terms-Reinforcement learning,autonomous driving,human demonstration,priority experience replay},
pages = {1--13},
title = {{Prioritized Experience-based Reinforcement Learning with Human Guidance: Methodology and Application to Autonomous Driving}}
}
@article{Hessel2018,
abstract = {The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.},
archivePrefix = {arXiv},
arxivId = {1710.02298},
author = {Hessel, Matteo and Modayil, Joseph and {Van Hasselt}, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
eprint = {1710.02298},
isbn = {9781577358008},
journal = {32nd AAAI Conf. Artif. Intell. AAAI 2018},
pages = {3215--3222},
title = {{Rainbow: Combining improvements in deep reinforcement learning}},
year = {2018}
}
@article{Ji2018,
abstract = {Parametric modeling uncertainties and unknown external disturbance are major concerns in the development of advanced lateral motion controller for autonomous vehicle at the limits of driving conditions. Considering that tyre operating at or close to its physical limits of friction exhibits highly nonlinear force response and that unknown external disturbance can be caused by changing driving conditions, this paper presents a novel lateral motion control method that can maintain the yaw stability of autonomous vehicle while minimizing lateral path tracking error at the limits of driving conditions The proposed control scheme consists of a robust steering controller and an adaptive neural network (ANN) approximator. First, based on reference path model, dynamics model and kinematics model of vehicle, the robust steering controller is developed via backstepping variable structure control (BVSC) to suppress lateral path tracking deviation, to withstand unknown external disturbance and guarantee the yaw stability of autonomous vehicle. Then, by combining adaptive control mechanism based on Lyapunov stability theory and radial basis function neural network (RBFNN), the ANN approximator is designed to estimate uncertainty of tyre cornering stiffness and reduce its adverse effects by learning to approximate arbitrary nonlinear functions, and it ensures the uniform ultimate boundedness of the closed-loop system. Both simulation and experiment results show that the proposed control strategy can robustly track the reference path and at the same time maintains the yaw stability of vehicle at or near the physical limits of tyre friction.},
author = {Ji, Xuewu and He, Xiangkun and Lv, Chen and Liu, Yahui and Wu, Jian},
doi = {10.1016/j.conengprac.2018.04.007},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ji et al. - 2018 - Adaptive-neural-network-based robust lateral motion control for autonomous vehicle at driving limits.pdf:pdf},
issn = {09670661},
journal = {Control Engineering Practice},
keywords = {Adaptive neural network,Autonomous vehicle,Backstepping variable structure control,Driving limits,Path tracking,Vehicle dynamics and control},
number = {April},
pages = {41--53},
publisher = {Elsevier Ltd},
title = {{Adaptive-neural-network-based robust lateral motion control for autonomous vehicle at driving limits}},
url = {https://doi.org/10.1016/j.conengprac.2018.04.007},
volume = {76},
year = {2018}
}
@article{Mnih2016,
abstract = {Wc propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
archivePrefix = {arXiv},
arxivId = {1602.01783},
author = {Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Lehdi and Graves, Alex and Harley, Tim and Lillicrap, Timothy P. and Silver, David and Kavukcuoglu, Koray},
eprint = {1602.01783},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih et al. - 2016 - Asynchronous methods for deep reinforcement learning(2).pdf:pdf},
isbn = {9781510829008},
journal = {33rd International Conference on Machine Learning, ICML 2016},
pages = {2850--2869},
title = {{Asynchronous methods for deep reinforcement learning}},
volume = {4},
year = {2016}
}
@misc{IEEEexample:babel,
author = {Braams, Johannes},
month = {feb},
title = {{The {\{}Babel{\}} package}},
url = {http://www.ctan.org/tex-archive/macros/latex/required/babel/},
year = {2001}
}
@article{Systems2020,
author = {Systems, Embedded and Agnihotri, Abhijeet and Kelly, Matthew O},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Systems, Agnihotri, Kelly - 2020 - ScholarlyCommons Teaching Autonomous Systems at 1 10th-scale.pdf:pdf},
number = {February},
title = {{ScholarlyCommons Teaching Autonomous Systems at 1 / 10th-scale}},
year = {2020}
}
@article{Geramifard2013,
abstract = {A Markov Decision Process (MDP) is a natural framework for formulating sequential decision-making problems under uncertainty. In recent years, researchers have greatly advanced algorithms for learning and acting in MDPs. This article reviews such algorithms, beginning with well-known dynamic programming methods for solving MDPs such as policy iteration and value iteration, then describes approximate dynamic programming methods such as trajectory based value iteration, and finally moves to reinforcement learning methods such as Q-Learning, SARSA, and least-squares policy iteration. We describe algorithms in a unified framework, giving pseudocode together with memory and iteration complexity analysis for each. Empirical evaluations of these techniques with four representations across four domains, provide insight into how these algorithms perform with various feature sets in terms of running time and performance.{\textcopyright}2013 A. Geramifard, T. J. Walsh, S. Tellex, G. Chowdhary.},
author = {Geramifard, Alborz and Walsh, Thomas J and Tellex, Stefanie and Chowdhary, Girish and Roy, Nicholas and How, Jonathan P},
doi = {10.1561/2200000042},
issn = {19358237},
journal = {Found. Trends Mach. Learn.},
number = {4},
pages = {375--454},
title = {{A tutorial on linear function approximators for dynamic programming and reinforcement learning}},
volume = {6},
year = {2013}
}
@book{sutton2020,
address = {Cambridge, Massachusetts},
author = {Sutton, Richard and Barto, Andrew},
edition = {2nd},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sutton, Barto - 2020 - Reinforcement Learning, An Introduction.pdf:pdf},
isbn = {9780262039246},
publisher = {MIT Press},
title = {{Reinforcement Learning, An Introduction}},
year = {2020}
}
@article{Carr2005,
abstract = {This paper investigates the possible application of reinforcement learn-ing to Tetris. The author investigates the background of Tetris, and qual-ifies it in a mathematical context. The author discusses reinforcement learning, and considers historically successful applications of it. Finally the author discusses considerations surrounding implementation.},
annote = {NP complete - impossible to search through entire policy space
Tetris satisfies markov property

Reason for reinforcement learning: Adjust policy through changing its value function during the game
Higher resolution than genetic algorithms

One piece - Dellacherie
OR 2 piece algorithms - Fahey

Look up conditions for eternal play

Melax - reduced tetris
Driessens - full tetris, relational reinforcement learning

Reward funciton: Based on working height
Issue - steer development of agents policy

Stochastic nature of tetris limits q-learning algorithm ability to determine future rewards

THIS PAPER WAS WRITTEN NEURAL NETWORKS WERE USED FOR FUNCTION APPROXIMAITON!

Punishment/ scoring:
Punish for increase in working height: agent completes rows but does not stack effectively},
author = {Carr, Donald},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Carr - 2005 - Applying reinforcement learning to Tetris.pdf:pdf},
journal = {Business},
pages = {1--15},
title = {{Applying reinforcement learning to Tetris}},
year = {2005}
}
@article{Drews2019,
abstract = {In this letter, we present a framework for combining deep learning-based road detection, particle filters, and model predictive control (MPC) to drive aggressively using only a monocular camera, IMU, and wheel speed sensors. This framework uses deep convolutional neural networks combined with LSTMs to learn a local cost map representation of the track in front of the vehicle. A particle filter uses this dynamic observation model to localize in a schematic map, and MPC is used to drive aggressively using this particle filter based state estimate. We show extensive real world testing results and demonstrate reliable operation of the vehicle at the friction limits on a complex dirt track. We reach speeds above 27 m/h (12 m/s) on a dirt track with a 105 ft (32 m) long straight using our 1:5 scale test vehicle.},
archivePrefix = {arXiv},
arxivId = {1812.02071},
author = {Drews, Paul and Williams, Grady and Goldfain, Brian and Theodorou, Evangelos A. and Rehg, James M.},
doi = {10.1109/LRA.2019.2896449},
journal = {IEEE Robotics and Automation Letters},
keywords = {Deep learning in robotics and automation,autonomous vehicle navigation,computer vision for transportation,localization},
number = {2},
pages = {1564--1571},
title = {{Vision-based high-speed driving with a deep dynamic observer}},
volume = {4},
year = {2019}
}
@misc{IEEEexample:miscgermanreg,
howpublished = {{\{}M{\}}e{\ss}vorschrift {\{}R{\}}eg {\{}TP{\}} {\{}MV{\}} 05},
institution = {Regulierungsbeh{\"{o}}rde f{\"{u}}r {\{}T{\}}elekommunikation und {\{}P{\}}ost ({\{}R{\}}eg {\{}TP{\}})},
title = {{{\{}M{\}}essung von {\{}S{\}}t{\"{o}}rfeldern an {\{}A{\}}nlagen und {\{}L{\}}eitungen der {\{}T{\}}elekommunikation im {\{}F{\}}requenzbereich 9 {\{}kHz{\}} bis 3 {\{}GHz{\}}}}
}
@article{Tsitsiklis1996,
abstract = {We develop a methodological framework and present a few different ways in which dynamic programming and compact representations can be combined to solve large scale stochastic control problems In particular, we develop algorithms that employ two types of feature based compact representations; that is, representations that involve feature extraction and a relatively simple approximation architecture. We prove the convergence of these algorithms and provide hounds on the approximation error. As an example, one of these algorithms is used to generate a strategy for the game of Tetris Furthermore, we provide a counter-example illustrating the difficulties of integrating compact representations with dynamic programming, which exemplifies the shortcomings of certain simple approaches. {\textcopyright} 1996 Kluwer Academic Publishers,.},
author = {Tsitsiklis, John N. and {Van Roy}, Benjamin},
doi = {10.1007/BF00114724},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tsitsiklis, Van Roy - 1996 - Feature-based methods for large scale dynamic programming.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {Compact representation,Curse of dimensionality,Dynamic programming,Features,Function approximation,Neuro-dynamic programming,Reinforcement learning},
number = {1-3},
pages = {59--94},
title = {{Feature-based methods for large scale dynamic programming}},
url = {https://www.mit.edu/{~}jnt/Papers/J060-96-bvr-feature.pdf},
volume = {22},
year = {1996}
}
@incollection{IEEEexample:incollection_chpp,
address = {Amsterdam, The Netherlands},
author = {Hedelin, P and Knagenhjelm, P and Skoglund, M},
booktitle = {Speech Coding and Synthesis},
chapter = {10},
editor = {Kleijn, W B and Paliwal, K K},
pages = {347--396},
publisher = {Elsevier Science},
title = {{Theory for Transmission of Vector Quantization Data}},
year = {1995}
}

@misc{Hafner2019a,
  doi = {10.48550/ARXIV.1912.01603},
  url = {https://arxiv.org/abs/1912.01603},
  author = {Hafner, Danijar and Lillicrap, Timothy and Ba, Jimmy and Norouzi, Mohammad},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Robotics (cs.RO), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Dream to Control: Learning Behaviors by Latent Imagination},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Yurtsever2020,
abstract = {Automated driving systems (ADSs) promise a safe, comfortable and efficient driving experience. However, fatalities involving vehicles equipped with ADSs are on the rise. The full potential of ADSs cannot be realized unless the robustness of state-of-the-art is improved further. This paper discusses unsolved problems and surveys the technical aspect of automated driving. Studies regarding present challenges, high-level system architectures, emerging methodologies and core functions including localization, mapping, perception, planning, and human machine interfaces, were thoroughly reviewed. Furthermore, many state-of-the-art algorithms were implemented and compared on our own platform in a real-world driving setting. The paper concludes with an overview of available datasets and tools for ADS development.},
archivePrefix = {arXiv},
arxivId = {1906.05113},
author = {Yurtsever, Ekim and Lambert, Jacob and Carballo, Alexander and Takeda, Kazuya},
doi = {10.1109/ACCESS.2020.2983149},
eprint = {1906.05113},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yurtsever et al. - 2020 - A Survey of Autonomous Driving Common Practices and Emerging Technologies.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Autonomous vehicles,automation,control,intelligent transportation systems,intelligent vehicles,robotics},
pages = {58443--58469},
title = {{A Survey of Autonomous Driving: Common Practices and Emerging Technologies}},
url = {https://arxiv.org/pdf/1906.05113.pdf},
volume = {8},
year = {2020}
}
@article{Voser2010,
abstract = {This paper presents simple analytical techniques that are used to understand and control high sideslip drift manoeuvres of road vehicles. These are manoeuvres in which a skilled driver stabilises a vehicle beyond its limits of handling, an operating regime responsible for major safety concerns in everyday driving. An analysis of the equilibria of a bicycle model with nonlinear tyres reveals the existence of unstable 'drift equilibria' corresponding to cornering at high sideslip angle in a countersteer configuration. Equipped with this information, linearisation about a desired drift equilibrium is used to design a controller that stabilises the vehicle at the equilibrium. The controller is subsequently implemented on a steer- and drive-by-wire testbed and successfully used to achieve autonomous drifts. {\textcopyright} 2010 Taylor {\&} Francis.},
author = {Voser, Christoph and Hindiyeh, Rami Y. and Gerdes, J. Christian},
doi = {10.1080/00423111003746140},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Voser, Hindiyeh, Gerdes - 2010 - Analysis and control of high sideslip manoeuvres(2).pdf:pdf},
issn = {00423114},
journal = {Vehicle System Dynamics},
keywords = {drifting,high sideslip manoeuvres,nonlinear vehicle dynamics,steering control,vehicle control},
number = {SUPPL. 1},
pages = {317--336},
title = {{Analysis and control of high sideslip manoeuvres}},
volume = {48},
year = {2010}
}
@article{IEEEexample:articleetal,
author = {Delorme, F and Others},
journal = {Electron. Lett.},
number = {15},
pages = {1244--1245},
title = {{Butt-jointed {\{}DBR{\}} Laser With 15 {\{}nm{\}} Tunability Grown in Three {\{}MOVPE{\}} Steps}},
volume = {31},
year = {1995}
}
@article{Sinha2020,
abstract = {Balancing performance and safety is crucial to deploying autonomous vehicles in multi-agent environments. In particular, autonomous racing is a domain that penalizes safe but conservative policies, highlighting the need for robust, adaptive strategies. Current approaches either make simplifying assumptions about other agents or lack robust mechanisms for online adaptation. This work makes algorithmic contributions to both challenges. First, to generate a realistic, diverse set of opponents, we develop a novel method for self-play based on replica-exchange Markov chain Monte Carlo. Second, we propose a distributionally robust bandit optimization procedure that adaptively adjusts risk aversion relative to uncertainty in beliefs about opponents behaviors. We rigorously quantify the tradeoffs in performance and robustness when approximating these computations in real-time motion-planning, and we demonstrate our methods experimentally on autonomous vehicles that achieve scaled speeds comparable to Formula One racecars.},
archivePrefix = {arXiv},
arxivId = {2003.03900},
author = {Sinha, Aman and O'Kelly, Matthew and Zheng, Hongrui and Mangharam, Rahul and Duchi, John and Tedrake, Russ},
eprint = {2003.03900},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sinha et al. - 2020 - Formulazero distributionally robust online adaptation via offline population synthesis.pdf:pdf},
issn = {23318422},
journal = {arXiv},
title = {{Formulazero: distributionally robust online adaptation via offline population synthesis}},
year = {2020}
}
@article{Lundgaard2007,
abstract = {This paper presents the results of experiments carried out with the goal of applying the machine learning techniques of reinforcement learning and neural networks with reinforcement learning to the game of Tetris. Tetris is a well-known computer game that can be played either by a single player or competitively with slight variations, toward the end of accumulating a high score or defeating the opponent. The fundamental hypothesis of this paper is that if the points earned in Tetris are used as the reward function for a machine learning agent, then that agent should be able to learn to play Tetris without other supervision. Toward this end, a state-space that summarizes the essential feature of the Tetris board is designed, high-level actions are developed to interact with the game, and agents are trained using Q-Learning and neural networks. As a result of these efforts, agents learn to play Tetris and to compete with other players. While the learning agents fail to accumulate as many points as the most advanced AI agents, they do learn to play more efficiently.},
annote = {Tetris: NP complete
No formal winning strategy
Look up: Colin Fahey tetris agent and tetris implementation
Look up: Pierre Dellacherie
Look up: Siegel, chaffee
Research question: Maximise points over lines cleared

Relational reinforcement learning might be useful

Goal: Develop agent that clears lines simultaneously - competitive tetris
Normal scoring funciton - agents imploys multi-line clearing strategy

State representation:
Raw: Track configurations of occupied/ unoccupied cells
Problem - size + meaningfulness
Exact configuration of board below top layer may be irrelevent
High level State representations:
Reduce size of state space

Actions: Configuration independent - high level strategy
-minimise holes, etc - represent different evaluation functions

Agents:
Random
Heuristic
Reinforcement learning - Q learning - riskier than sarsa

Neural network using screen pixels - takes too long to converge to be useful
- doesnt perform any better than random agent},
author = {Lundgaard, Nicholas and Mckee, Brian},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lundgaard, Mckee - 2007 - Reinforcement Learning and Neural Networks for Tetris.pdf:pdf},
title = {{Reinforcement Learning and Neural Networks for Tetris}},
url = {http://www.colinfahey.com/tetris},
year = {2007}
}
@inproceedings{IEEEexample:confwithpaper,
author = {Wegmuller, M and von der Weid, J P and Oberson, P and Gisin, N},
booktitle = {Proc. {\{}ECOC{\}}'00},
pages = {109},
title = {{High Resolution Fiber Distributed Measurements With Coherent {\{}OFDR{\}}}},
year = {2000}
}
@inproceedings{IEEEexample:confwithpapertype,
author = {Mikkelsen, B and Raybon, G and Essiambre, R.-J. and Dreyer, K and Su., Y and Nelson, L E and Johnson, J E and Shtengel, G and Bond, A and Moodie, D G and Ellis, A D},
booktitle = {Proc. {\{}ECOC{\}}'99},
pages = {28--29},
title = {{160 {\{}Gbit/s{\}} Single-channel Transmission Over 300 km Nonzero-dispersion Fiber With Semiconductor Based Transmitter and Demultiplexer}},
type = {postdeadline paper},
year = {1999}
}
@article{Gao2017,
abstract = {How can a delivery robot navigate reliably to a destination in a new office building, with minimal prior information? To tackle this challenge, this paper introduces a two-level hierarchical approach, which integrates model-free deep learning and model-based path planning. At the low level, a neural-network motion controller, called the intention-net, is trained end-to-end to provide robust local navigation. The intention-net maps images from a single monocular camera and "intentions" directly to robot controls. At the high level, a path planner uses a crude map, e.g., a 2-D floor plan, to compute a path from the robot's current location to the goal. The planned path provides intentions to the intention-net. Preliminary experiments suggest that the learned motion controller is robust against perceptual uncertainty and by integrating with a path planner, it generalizes effectively to new environments and goals.},
archivePrefix = {arXiv},
arxivId = {1710.05627},
author = {Gao, Wei and Hsu, David and Lee, Wee Sun and Shen, Shengmei and Subramanian, Karthikk},
eprint = {1710.05627},
keywords = {deep learning,imitation learning,path planning,visual navigation},
number = {Figure 1},
pages = {1--10},
title = {{Intention-Net: Integrating Planning and Deep Learning for Goal-Directed Autonomous Navigation}},
url = {http://arxiv.org/abs/1710.05627},
year = {2017}
}
@techreport{IEEEexample:techreptypeii,
author = {Middleton, D and Spaulding, A D},
institution = {National Telecommunications and Information Administration ({\{}NTIA{\}}), U.S. Dept. of Commerce},
month = {may},
number = {86-194},
title = {{A Tutorial Review of Elements of Weak Signal Detection in Non-{\{}G{\}}aussian {\{}EMI{\}} Environments}},
type = {NTIA Report},
year = {1986}
}
@article{Chen2015,
abstract = {Today, there are two major paradigms for vision-based autonomous driving systems: mediated perception approaches that parse an entire scene to make a driving decision, and behavior reflex approaches that directly map an input image to a driving action by a regressor. In this paper, we propose a third paradigm: a direct perception approach to estimate the affordance for driving. We propose to map an input image to a small number of key perception indicators that directly relate to the affordance of a road/traffic state for driving. Our representation provides a set of compact yet complete descriptions of the scene to enable a simple controller to drive autonomously. Falling in between the two extremes of mediated perception and behavior reflex, we argue that our direct perception representation provides the right level of abstraction. To demonstrate this, we train a deep Convolutional Neural Network using recording from 12 hours of human driving in a video game and show that our model can work well to drive a car in a very diverse set of virtual environments. We also train a model for car distance estimation on the KITTI dataset. Results show that our direct perception approach can generalize well to real driving images. Source code and data are available on our project website.},
annote = {Partial end end - a CNN replaces the perception subsystem

maps an image to 'affordance indicators': Distance from the vehicle to left and right lanes, distance to other vehicles.

Rules based decision maker decides what policy to follow (overtake, keep in lane, etc.). Simplified simulation - other vehicles do not move between lanes.

End to end baseline is erratic},
archivePrefix = {arXiv},
arxivId = {1505.00256},
author = {Chen, Chenyi and Seff, Ari and Kornhauser, Alain and Xiao, Jianxiong},
doi = {10.1109/ICCV.2015.312},
eprint = {1505.00256},
isbn = {9781467383912},
issn = {15505499},
journal = {Proc. IEEE Int. Conf. Comput. Vis.},
number = {Figure 1},
pages = {2722--2730},
title = {{DeepDriving: Learning affordance for direct perception in autonomous driving}},
volume = {2015 Inter},
year = {2015}
}
@article{Sutton1991,
abstract = {Dyna is an AI architecture that integrates learning, planning, and reactive execution. Learning methods are used in Dyna both for compiling planning results and for updating a model of the effects of the agent's actions on the world. Planning is incremental and can use the probabilistic and ofttimes incorrect world models generated by learning processes. Execution is fully reactive in the sense that no planning intervenes between perception and action. Dyna relies on machine learning methods for learning from examples---these are among the basic building blocks making up the architecture---yet is not tied to any particular method. This paper briefly introduces Dyna and discusses its strengths and weaknesses with respect to other architectures.},
author = {Sutton, Richard S},
doi = {10.1145/122344.122377},
issn = {0163-5719},
journal = {ACM SIGART Bull.},
number = {4},
pages = {160--163},
title = {{Dyna, an integrated architecture for learning, planning, and reacting}},
volume = {2},
year = {1991}
}
@misc{IEEEexample:TBPmisc,
annote = {to be published},
author = {Coates, M and Hero, A and Nowak, R and Yu, B},
month = {may},
title = {{Internet Tomography}},
year = {2002}
}
@article{Niu2020,
abstract = {Decision making for autonomous driving is a safety-critical control problem. Prior works of safe reinforcement learning either tackle the problem with reward shaping or with modifying the reinforcement learning exploration process. However, the former cannot guarantee the safety during the learning process, while the latter relies heavily on expertise to design exquisite exploration policy. Currently, only short-term decision makings for low-speed driving were achieved in road scenes with basic geometries. In this paper, we propose a two-stage safe reinforcement learning algorithm to automatically learn a long-term policy for high-speed driving that guarantees safety during the entire training. In the first learning stage, model-free reinforcement learning is followed by a rule-based safeguard module to avoid danger at low speed without expert ne-tuning. In the second learning stage, the rule-based module is replaced with a data-driven counterpart to develop a closed-form analytical safety solution for high-speed driving. Moreover, an adaptive reward function is designed to match the different objectives of the two learning stages for faster convergence to an optimal policy. Experiments are conducted on a racing simulator TORCS which has complex racing tracks (e.g. sharp turns, hills). Compared with the state-of-the-art baselines, the results show that our method achieves zero safety violation and quickly converges to a more efficient and stable policy with an average speed of 127 km/h (3.3{\%} higher than the best result of baselines) and an average swing of 3.96 degrees.},
author = {Niu, Jingyu and Hu, Yu and Jin, Beibei and Han, Yinhe and Li, Xiaowei},
doi = {10.1109/SMC42975.2020.9283053},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Niu et al. - 2020 - Two-Stage Safe Reinforcement Learning for High-Speed Autonomous Racing.pdf:pdf},
isbn = {9781728185262},
journal = {Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics},
keywords = {autonomous racing,safe reinforcement learning},
pages = {3934--3941},
title = {{Two-Stage Safe Reinforcement Learning for High-Speed Autonomous Racing}},
url = {https:doi.org/10.1109/SMC42975.2020.9283053},
volume = {2020-Octob},
year = {2020}
}
@article{Pendleton2017,
abstract = {Autonomous vehicles are expected to play a key role in the future of urban transportation systems, as they offer potential for additional safety, increased productivity, greater accessibility, better road efficiency, and positive impact on the environment. Research in autonomous systems has seen dramatic advances in recent years, due to the increases in available computing power and reduced cost in sensing and computing technologies, resulting in maturing technological readiness level of fully autonomous vehicles. The objective of this paper is to provide a general overview of the recent developments in the realm of autonomous vehicle software systems. Fundamental components of autonomous vehicle software are reviewed, and recent developments in each area are discussed.},
author = {Pendleton, Scott Drew and Andersen, Hans and Du, Xinxin and Shen, Xiaotong and Meghjani, Malika and Eng, You Hong and Rus, Daniela and Ang, Marcelo H.},
doi = {10.3390/machines5010006},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pendleton et al. - 2017 - Perception, planning, control, and coordination for autonomous vehicles.pdf:pdf},
issn = {20751702},
journal = {Machines},
keywords = {Automotive control,Autonomous vehicles,Localization,Multi-vehicle cooperation,Perception,Planning},
number = {1},
pages = {1--54},
title = {{Perception, planning, control, and coordination for autonomous vehicles}},
volume = {5},
year = {2017}
}
@phdthesis{IEEEexample:masterstype,
address = {Bangalore, India},
author = {Karnik, A},
month = {jan},
school = {Indian Institute of Science},
title = {{Performance of {\{}TCP{\}} Congestion Control with Rate Feedback: {\{}TCP/ABR{\}} and Rate Adaptive {\{}TCP/IP{\}}}},
type = {M. Eng. thesis},
year = {1999}
}

@article{Weiss2021,
author = {Weiss, Trent and Chrosniak, John and Behl, Madhur},
journal = {2021 International Conference on Robotics and Automation (ICRA 2021) - Workshop Opportunities and Challenges With Autonomous Racing},
title = {{Towards Multi-Agent Autonomous Racing with the DeepRacing framework}},
url = {https://linklab-uva.github.io/icra-autonomous-racing/contributed{\_}papers/paper5.pdf},
year = {2021}
}
@article{Williams2017,
abstract = {We introduce an information theoretic model predictive control (MPC) algorithm capable of handling complex cost criteria and general nonlinear dynamics. The generality of the approach makes it possible to use multi-layer neural networks as dynamics models, which we incorporate into our MPC algorithm in order to solve model-based reinforcement learning tasks. We test the algorithm in simulation on a cart-pole swing up and quadrotor navigation task, as well as on actual hardware in an aggressive driving task. Empirical results demonstrate that the algorithm is capable of achieving a high level of performance and does so only utilizing data collected from the system.},
author = {Williams, Grady and Wagener, Nolan and Goldfain, Brian and Drews, Paul and Rehg, James M and Boots, Byron and Theodorou, Evangelos A},
doi = {10.1109/ICRA.2017.7989202},
url = {https://doi.org/10.1109/ICRA.2017.7989202},
isbn = {9781509046331},
issn = {10504729},
journal = {Proc. - IEEE Int. Conf. Robot. Autom.},
pages = {1714--1721},
title = {{Information theoretic MPC for model-based reinforcement learning}},
year = {2017}
}
@misc{IEEEexample:bibtexdesign,
author = {Patashnik, Oren},
howpublished = {btxhak.pdf},
month = {feb},
title = {{Designing {\{}$\backslash$BibTeX$\backslash$ {\}} Styles}},
url = {http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/},
year = {1988}
}
@article{Yurtsever2020,
abstract = {Automated driving in urban settings is challenging. Human participant behavior is difficult to model, and conventional, rule-based Automated Driving Systems (ADSs) tend to fail when they face unmodeled dynamics. On the other hand, the more recent, end-to-end Deep Reinforcement Learning (DRL) based model-free ADSs have shown promising results. However, pure learning-based approaches lack the hard-coded safety measures of model-based controllers. Here we propose a hybrid approach for integrating a path planning pipe into a vision based DRL framework to alleviate the shortcomings of both worlds. In summary, the DRL agent is trained to follow the path planner's waypoints as close as possible. The agent learns this policy by interacting with the environment. The reward function contains two major terms: the penalty of straying away from the path planner and the penalty of having a collision. The latter has precedence in the form of having a significantly greater numerical value. Experimental results show that the proposed method can plan its path and navigate between randomly chosen origin-destination points in CARLA, a dynamic urban simulation environment. Our code is open-source and available online 11https://github.com/Ekim-Yurtsever/Hybrid-DeepRL-Automated-Driving.},
archivePrefix = {arXiv},
arxivId = {2002.00434},
author = {Yurtsever, Ekim and Capito, Linda and Redmill, Keith and Ozgune, Umit},
doi = {10.1109/IV47402.2020.9304735},
eprint = {2002.00434},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yurtsever et al. - 2020 - Integrating Deep Reinforcement Learning with Model-based Path Planners for Automated Driving.pdf:pdf},
isbn = {9781728166735},
journal = {IEEE Intelligent Vehicles Symposium, Proceedings},
number = {Iv},
pages = {1311--1316},
title = {{Integrating Deep Reinforcement Learning with Model-based Path Planners for Automated Driving}},
year = {2020}
}
@article{OpenAI2019,
abstract = {We demonstrate that models trained only in simulation can be used to solve a manipulation problem of unprecedented complexity on a real robot. This is made possible by two key components: a novel algorithm, which we call automatic domain randomization (ADR) and a robot platform built for machine learning. ADR automatically generates a distribution over randomized environments of ever-increasing difficulty. Control policies and vision state estimators trained with ADR exhibit vastly improved sim2real transfer. For control policies, memory-augmented models trained on an ADR-generated distribution of environments show clear signs of emergent meta-learning at test time. The combination of ADR with our custom robot platform allows us to solve a Rubik's cube with a humanoid robot hand, which involves both control and state estimation problems. Videos summarizing our results are available: https://openai.com/blog/solving-rubiks-cube/},
archivePrefix = {arXiv},
arxivId = {1910.07113},
author = {OpenAI and Akkaya, Ilge and Andrychowicz, Marcin and Chociej, Maciek and Litwin, Mateusz and McGrew, Bob and Petron, Arthur and Paino, Alex and Plappert, Matthias and Powell, Glenn and Ribas, Raphael and Schneider, Jonas and Tezak, Nikolas and Tworek, Jerry and Welinder, Peter and Weng, Lilian and Yuan, Qiming and Zaremba, Wojciech and Zhang, Lei},
eprint = {1910.07113},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/OpenAI et al. - 2019 - Solving Rubik's Cube with a Robot Hand.pdf:pdf},
pages = {1--51},
title = {{Solving Rubik's Cube with a Robot Hand}},
url = {http://arxiv.org/abs/1910.07113},
year = {2019}
}
@article{Blum2019,
abstract = {Space exploration missions have seen use of increasingly sophisticated robotic systems with ever more autonomy. Deep learning promises to take this even a step further, and has applications for high-level tasks, like path planning, as well as low-level tasks, like motion control, which are critical components for mission efficiency and success. Using deep reinforcement end-to-end learning with randomized reward function parameters during training, we teach a simulated 8 degree-of-freedom quadruped ant-like robot to travel anywhere within a perimeter, conducting path plan and motion control on a single neural network, without any system model or prior knowledge of the terrain or environment. Our approach also allows for user specified waypoints, which could translate well to either fully autonomous or semi-autonomous/teleoperated space applications that encounter delay times. We trained the agent using randomly generated waypoints linked to the reward function and passed waypoint coordinates as inputs to the neural network. Such applications show promise on a variety of space exploration robots, including high speed rovers for fast locomotion and legged cave robots for rough terrain.},
annote = {Attempt to develop fast, precise obstacle avoidance
RL for reactive control can control robot in realtime

Goals:
Find goal, go through waypoints
Learn to walk
Human operator can specify waypoint

Implementation:
End to end model
No map

Issues with current path plannign techniques:
A* - Requires knowledge about the map
Engineer must carefully design cost function

Approaches:
Hierarchical - a neural network for each task

Agent:
Recieves waypoints
Develops local plan and tracks path

Notes:
Separate learning tasks for vehicle control and path planning - if a different path is specified, vehicle will not need to relearn controls.},
archivePrefix = {arXiv},
arxivId = {1909.06034},
author = {Blum, Tamir and Jones, William and Yoshida, Kazuya},
eprint = {1909.06034},
keywords = {Locomotion,Path planning,Reinforcement Learning},
title = {{Deep Learned Path Planning via Randomized Reward-Linked-Goals and Potential Space Applications}},
url = {http://arxiv.org/abs/1909.06034},
year = {2019}
}
@article{algorta2009,
annote = {Reduce grid size to 10x10 to make game shorter without compromising the nature of the game

Tetris not part of atari domain or openAI universe
No deep learning algorithm has learned to play well from raw inputs},
archivePrefix = {arXiv},
arxivId = {arXiv:1905.01652v2},
author = {{Ozgur Simsek}, Simon Algorta},
eprint = {arXiv:1905.01652v2},
title = {{The Game of Tetris in Machine Learning ¨}},
year = {2019}
}
@article{OKelly2020,
abstract = {TUNERCAR is a toolchain that jointly optimizes racing strategy, planning methods, control algorithms, and vehicle parameters for an autonomous racecar. In this paper, we detail the target hardware, software, simulators, and systems infrastructure for this toolchain. Our methodology employs a parallel implementation of CMA-ES which enables simulations to proceed 6 times faster than real-world rollouts. We show our approach can reduce the lap times in autonomous racing, given a fixed computational budget. For all tested tracks, our method provides the lowest lap time, and relative improvements in lap time between 7-21{\%}. We demonstrate improvements over a naive random search method with equivalent computational budget of over 15 seconds/lap, and improvements over expert solutions of over 2 seconds/lap. We further compare the performance of our method against hand-tuned solutions submitted by over 30 international teams, comprised of graduate students working in the field of autonomous vehicles. Finally, we discuss the effectiveness of utilizing an online planning mechanism to reduce the reality gap between our simulation and actual tests.},
author = {O'Kelly, Matthew and Zheng, Hongrui and Jain, Achin and Auckley, Joseph and Luong, Kim and Mangharam, Rahul},
doi = {10.1109/ICRA40945.2020.9197080},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/O'Kelly et al. - 2020 - TUNERCAR A Superoptimization Toolchain for Autonomous Racing.pdf:pdf},
isbn = {9781728173955},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
number = {January},
pages = {5356--5362},
title = {{TUNERCAR: A Superoptimization Toolchain for Autonomous Racing}},
year = {2020}
}
@phdthesis{IEEEexample:masters,
address = {Cambridge},
author = {Loh, Nin C},
school = {Massachusetts Institute of Technology},
title = {{High-Resolution Micromachined Interferometric Accelerometer}},
year = {1992}
}
@article{Barth-Maron2018,
abstract = {This work adopts the very successful distributional perspective on reinforcement learning and adapts it to the continuous control setting. We combine this within a distributed framework for off-policy learning in order to develop what we call the Distributed Distributional Deep Deterministic Policy Gradient algorithm, D4PG. We also combine this technique with a number of additional, simple improvements such as the use of N-step returns and prioritized experience replay. Experimentally we examine the contribution of each of these individual components, and show how they interact, as well as their combined contributions. Our results show that across a wide variety of simple control tasks, difficult manipulation tasks, and a set of hard obstacle-based locomotion tasks the D4PG algorithm achieves state of the art performance.},
archivePrefix = {arXiv},
arxivId = {1804.08617},
author = {Barth-Maron, Gabriel and Hoffman, Matthew W. and Budden, David and Dabney, Will and Horgan, Dan and Dhruva, T. B. and Muldal, Alistair and Heess, Nicolas and Lillicrap, Timothy},
eprint = {1804.08617},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Barth-Maron et al. - 2018 - Distributed distributional deterministic policy gradients(2).pdf:pdf},
journal = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
pages = {1--16},
title = {{Distributed distributional deterministic policy gradients}},
year = {2018}
}
@misc{IEEEexample:bibtexFAQ,
author = {Hoadley, David and Shell, Michael},
howpublished = {btxFAQ.txt},
month = {oct},
title = {{{\{}$\backslash$BibTeX{\}}$\backslash$ Tips and {\{}FAQ{\}}}},
url = {http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/},
year = {2002}
}
@article{Mnih2015,
abstract = {The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.},
annote = {Instability with non linear funciton approximators:
Small updates to Q significantly change policy
Correlations present in sequence of observations
Correlation between action and target values

Solution to address instability: Experience replay
Randomise data
Iterative update - adjust action values toward target values
--Targets are only periodically updated - reduce correlation with target},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
issn = {14764687},
journal = {Nature},
number = {7540},
pages = {529--533},
pmid = {25719670},
title = {{Human-level control through deep reinforcement learning}},
volume = {518},
year = {2015}
}
@article{Lillicrap2016,
  doi = {10.48550/ARXIV.1509.02971},
  url = {https://arxiv.org/abs/1509.02971},
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Continuous control with deep reinforcement learning},
  publisher = {arXiv},
  year = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Bertsekas1997a,
author = {Bertsekas, Dimitri P. and Sergey, Ioffe},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bertsekas, Sergey - 1997 - Temporal Differences-Based Policy Iterationand Applications in Neuro-Dynamic Programming.pdf:pdf},
number = {August 1996},
pages = {1--19},
title = {{Temporal Differences-Based Policy Iterationand Applications in Neuro-Dynamic Programming}},
url = {https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.217.3017{\&}rep=rep1{\&}type=pdf},
volume = {1996},
year = {1997}
}
@article{Wen2021,
abstract = {We study a safe reinforcement learning problem, in which the constraints are defined as the expected cost over finite-length trajectories. We propose a constrained cross-entropy-based method to solve this problem. The key idea is to transform the original constrained optimization problem into an unconstrained one with a surrogate objective. The method explicitly tracks its performance with respect to constraint satisfaction and thus is well suited for safety-critical applications. We show that the asymptotic behavior of the proposed algorithm can be almost-surely described by that of an ordinary differential equation. Then, we give sufficient conditions on the properties of this differential equation for the convergence of the proposed algorithm. At last, we show the performance of the proposed algorithm in two simulation examples. In a constrained linear-quadratic regulator example, we observe that the algorithm converges to the global optimum with high probability. In a 2-D navigation example, we find that the algorithm effectively learns feasible policies without assumptions on the feasibility of initial policies, even with non-Markovian objective functions and constraint functions.},
author = {Wen, Min and Topcu, Ufuk},
doi = {10.1109/TAC.2020.3015931},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wen, Topcu - 2021 - Constrained cross-entropy method for safe reinforcement learning.pdf:pdf},
isbn = {0001417126},
issn = {15582523},
journal = {IEEE Transactions on Automatic Control},
keywords = {Machine learning algorithms,Safe reinforcement learning,Statistical learning},
number = {7},
pages = {3123--3137},
publisher = {IEEE},
title = {{Constrained cross-entropy method for safe reinforcement learning}},
volume = {66},
year = {2021}
}
@article{Amundsen2014,
abstract = {In the recent past letter recognition by the human visual system was an intensely debated topic. Common theories included templates (Holbrook, 1975), spatial frequency (Gervais et. al. 1984), and geometric features (Gibson, 1969). While none of these main theories has been discounted, the spatial frequency model of letter recognition has experienced the most recent success achieving a correlation of r = .70 for predicted and actual confusions. The present work revived this topic and explored prediction potential of feature-based recognition models when features identified by different models are combined and reaction time to respond same or different to a letter pair is measured. Aggregated grand means and averaged median subject data yielded correlation values of r = .60 and r = .66 respectively, though performance was only slightly lower (r = .64) for one of the models alone. Future directions this research might take are also considered.},
author = {Amundsen, Jonas Balgaard},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Amundsen - 2014 - A comparison of feature functions for Tetris strategies.pdf:pdf},
number = {June},
pages = {78},
title = {{A comparison of feature functions for Tetris strategies}},
year = {2014}
}
@article{Deisenroth2011,
abstract = {In this paper, we introduce PILCO, a practical, data-efficient model-based policy search method. PILCO reduces model bias, one of the key problems of model-based reinforcement learning, in a principled way. By learning a probabilistic dynamics model and explicitly incorporating model uncertainty into long-term planning, PILCO can cope with very little data and facilitates learning from scratch in only a few trials. Policy evaluation is performed in closed form using state-of-the-art approximate inference. Furthermore, policy gradients are computed analytically for policy improvement. We report unprecedented learning efficiency on challenging and high-dimensional control tasks. Copyright 2011 by the author(s)/owner(s).},
author = {Deisenroth, Marc Peter and Rasmussen, Carl Edward},
isbn = {9781450306195},
journal = {Proc. 28th Int. Conf. Mach. Learn. ICML 2011},
pages = {465--472},
title = {{PILCO: A model-based and data-efficient approach to policy search}},
year = {2011}
}
@misc{IEEEexample:uspat,
address = {Los Angeles, CA},
author = {Sorace, Ronald E and Reinhardt, Victor S and Vaughn, Steven A},
month = {sep},
number = {5668842},
title = {{High-Speed Digital-to-{\{}RF{\}} Converter}},
year = {1997}
}
@article{Liniger2020,
abstract = {We consider autonomous racing of two cars and present an approach to formulate racing decisions as a noncooperative nonzero-sum game. We design three different games where the players aim to fulfill static track constraints as well as avoid collision with each other; the latter constraint depends on the combined actions of the two players. The difference between the games is the collision constraints and the payoff. In the first game, collision avoidance is only considered by the follower, and each player maximizes their own progress toward the finish line. We show that, thanks to the sequential structure of this game, equilibria can be computed through an efficient sequential maximization approach. Furthermore, we show that these actions, if feasible, are also a Stackelberg and Nash equilibrium in pure strategies of our second game where both players consider the collision constraints. The payoff of our third game is designed to promote blocking, by additionally rewarding the cars for staying ahead at the end of the horizon. We show that this changes the Stackelberg equilibrium, but has a minor influence on the Nash equilibria. For online implementation, we propose to play the games in a moving horizon fashion and discuss two methods for guaranteeing feasibility of the resulting coupled repeated games. Finally, we study the performance of the proposed approaches in simulation for a setup that replicates the miniature race car tested at the Automatic Control Laboratory, ETH Z{\"{u}}rich, Switzerland. The simulation study shows that the presented games can successfully model different racing behaviors and generate interesting racing situations.},
archivePrefix = {arXiv},
arxivId = {1712.03913},
author = {Liniger, Alexander and Lygeros, John},
doi = {10.1109/TCST.2019.2895282},
eprint = {1712.03913},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liniger, Lygeros - 2020 - A Noncooperative Game Approach to Autonomous Racing.pdf:pdf},
issn = {15580865},
journal = {IEEE Transactions on Control Systems Technology},
keywords = {Autonomous racing,game theory,hierarchical control,moving horizon games,noncooperative decision making},
number = {3},
pages = {884--897},
publisher = {IEEE},
title = {{A Noncooperative Game Approach to Autonomous Racing}},
volume = {28},
year = {2020}
}
@inproceedings{IEEEexample:conf_typical,
address = {Nagoya, Japan},
author = {Gupta, R K and Senturia, S D},
booktitle = {Proc. {\{}IEEE{\}} International Workshop on Microelectromechanical Systems ({\{}MEMS{\}}'97)},
month = {jan},
pages = {290--294},
title = {{Pull-in Time Dynamics as a Measure of Absolute Pressure}},
year = {1997}
}
@incollection{IEEEexample:inbook,
address = {New York, NY},
author = {Rose, H E},
chapter = {3},
publisher = {Oxford Univ. Press},
title = {{A Course in Number Theory}},
year = {1988}
}
@article{Kelly2020,
annote = {See benchmarks!},
author = {Kelly, Matthew O},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kelly - 2020 - F1TENTH An Open-source Evaluation Environment for Continuous Control and Reinforcement Learning.pdf:pdf},
keywords = {autonomous racing,autonomous vehicles,reinforcement learning},
pages = {77--89},
title = {{F1TENTH : An Open-source Evaluation Environment for Continuous Control and Reinforcement Learning}},
url = {https://link.springer.com/chapter/10.1007/978-3-642-20525-5{\_}15},
year = {2020}
}
@article{Anon2021,
author = {Anon},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Anon - 2021 - Lyapunov barrier policy optimisation.pdf:pdf},
pages = {1--10},
title = {{Lyapunov barrier policy optimisation}},
year = {2021}
}
@misc{IEEEexample:standard,
address = {Piscataway, NJ},
institution = {IEEE},
number = {802.11},
title = {{Wireless {\{}LAN{\}} Medium Access Control {\{}(MAC){\}} and Physical Layer {\{}(PHY){\}} Specification}},
year = {1997}
}
@article{Pan_Xinlei_2017,
abstract = {Reinforcement learning is considered as a promising direction for driving policy learning. However, training autonomous driving vehicle with reinforcement learning in real environment involves non-affordable trial-and-error. It is more desirable to first train in a virtual environment and then transfer to the real environment. In this paper, we propose a novel realistic translation network to make model trained in virtual environment be workable in real world. The proposed network can convert non-realistic virtual image input into a realistic one with similar scene structure. Given realistic frames as input, driving policy trained by reinforcement learning can nicely adapt to real world driving. Experiments show that our proposed virtual to real (VR) reinforcement learning (RL) works pretty well. To our knowledge, this is the first successful case of driving policy trained by reinforcement learning that can adapt to real world driving data.},
archivePrefix = {arXiv},
arxivId = {1704.03952},
author = {Pan, Xinlei and You, Yurong and Wang, Ziyan and Lu, Cewu},
doi = {10.5244/c.31.11},
eprint = {1704.03952},
isbn = {190172560X},
journal = {Br. Mach. Vis. Conf. 2017, BMVC 2017},
title = {{Virtual to real reinforcement learning for autonomous driving}},
year = {2017}
}
@article{Schrum2018,
abstract = {Tetris is a challenging puzzle game that has received much attention from the AI community, but much of this work relies on intelligent high-level features. Recently, agents played the game using low-level features (10 × 20 board) as input to fully connected neural networks evolved with the indirect encoding HyperNEAT. However, research in deep learning indicates that convolutional neural networks (CNNs) are superior to fully connected networks in processing visuospatial inputs. Therefore, this paper uses HyperNEAT to evolve CNNs. The results indicate that CNNs are indeed superior to fully connected neural networks in Tetris, and identify several factors that influence the successful evolution of indirectly encoded CNNs.},
author = {Schrum, Jacob},
doi = {10.1145/3205455.3205459},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schrum - 2018 - Evolving indirectly encoded convolutional neural networks to play tetris with low-level features.pdf:pdf},
isbn = {9781450356183},
journal = {GECCO 2018 - Proceedings of the 2018 Genetic and Evolutionary Computation Conference},
keywords = {Games,Indirect encoding,Neural networks,Tetris},
pages = {205--212},
title = {{Evolving indirectly encoded convolutional neural networks to play tetris with low-level features}},
url = {https://dl.acm.org/doi/pdf/10.1145/3205455.3205459},
year = {2018}
}
@article{Chang2021,
abstract = {The lack of stability guarantee restricts the practical use of learning-based methods in core control problems in robotics. We develop new methods for learning neural control policies and neural Lyapunov critic functions in the model-free reinforcement learning (RL) setting. We use sample-based approaches and the Almost Lyapunov function conditions to estimate the region of attraction and invariance properties through the learned Lyapunov critic functions. The methods enhance stability of neural controllers for various nonlinear systems including automobile and quadrotor control.},
archivePrefix = {arXiv},
arxivId = {2107.04989},
author = {Chang, Ya Chien and Gao, Sicun},
doi = {10.1109/ICRA48506.2021.9560886},
eprint = {2107.04989},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chang, Gao - 2021 - Stabilizing Neural Control Using Self-Learned Almost Lyapunov Critics(2).pdf:pdf},
isbn = {9781728190778},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
number = {Section V},
pages = {1803--1809},
title = {{Stabilizing Neural Control Using Self-Learned Almost Lyapunov Critics}},
url = {https://yachienchang.github.io/ICRA2021/index.html},
volume = {2021-May},
year = {2021}
}
@article{Tatulea-Codrean2020,
abstract = {This paper addresses the challenges of developing an embedded non-linear model predictive control (NMPC) solution for the optimal driving of miniature scale autonomous vehicles (AVs). The NMPC approach lends itself perfectly to driving applications, provided that a system for localization and tracking of the vehicle is available. An important challenge in the implementation results from the need to accurately steer the vehicle at high speeds, which requires fast actuation. In this paper we present a solution to this problem, which employs an artificial neural network (ANN) controller trained with rigorous NMPC input-output data. We discuss the development process, from modelling until the realization of the ANN controller within the operating system of the AV. The procedure is demonstrated within the virtual environment of the popular F1/10 race car, an AV platform widely used in AI and autonomous driving challenges. The results contain both NMPC and ANN-based simulations for different race tracks and for different driving strategies. The main focus of this work lies in the formulation of the optimal driving control problem and the training method of the ANN. Our approach uses a standardization of the driving problem, which enables us to abstractize optimal driving and to simplify it for the learning process. We show how driving patterns can be learned accurately on a reduced set of training data and that they can subsequently be extended to new and more challenging driving situations.},
annote = {Imitation learning use to learn neural network to imitate non-linear MPC.},
author = {Tatulea-Codrean, Alexandru and Mariani, Tommaso and Engell, Sebastian},
doi = {10.1016/j.ifacol.2020.12.1669},
journal = {IFAC-PapersOnLine},
keywords = {AV,Artificial neural networks,F1/10,Learned control,NMPC},
number = {2},
pages = {6031--6036},
publisher = {Elsevier Ltd},
title = {{Design and simulation of a machine-learning and model predictive control approach to autonomous race driving for the F1/10 platform}},
url = {https://doi.org/10.1016/j.ifacol.2020.12.1669},
volume = {53},
year = {2020}
}
@article{Nageshrao2019,
abstract = {The operational space of an autonomous vehicle (AV) can be diverse and vary significantly. Due to this, formulating a rule based decision maker for selecting driving maneuvers may not be ideal. Similarly, it may not be efficient to solve optimal control problem in real-time for a predefined cost function. In order to address these issues and to avoid peculiar behaviors when encountering unforeseen scenario, we propose a reinforcement learning (RL) based method, where the ego car, i.e., an autonomous vehicle, learns to make decisions by directly interacting with the simulated traffic. Here the decision maker is a deep neural network that provides an action choice for a given system state. We demonstrate the performance of the developed algorithm in highway driving scenario where the trained AV encounters varying traffic density.},
archivePrefix = {arXiv},
arxivId = {1904.00035},
author = {Nageshrao, Subramanya and Tseng, H. Eric and Filev, DImitar},
doi = {10.1109/SMC.2019.8914621},
eprint = {1904.00035},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nageshrao, Tseng, Filev - 2019 - Autonomous highway driving using deep reinforcement learning.pdf:pdf},
isbn = {9781728145693},
issn = {1062922X},
journal = {Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics},
pages = {2326--2331},
title = {{Autonomous highway driving using deep reinforcement learning}},
volume = {2019-Octob},
year = {2019}
}
@article{Kritayakirana2010,
abstract = {This paper explains the design of a feedforward longitudinal controller that will drive an autonomous vehicle through a corner at the limits of tire adhesion. The feedforward longitudinal algorithm applies appropriate amounts of braking force and throttle during cornering to mimic a racecar driver trail-braking into a corner and applying the throttle during corner exit. A "g-g" diagram combined with a clothoid map is used to estimate the feedforward longitudinal command. By introducing this feedforward algorithm into the controller, significant improvements in lap time and cornering exit speed were observed. Analyzing the controller performance during cornering also provides an insight to the future development of the feedback longitudinal controller. The knowledge gained from designing an algorithm that controls a vehicle at the limits can be applied to future driver assistance systems to make cars safer, yet still fun to drive. {\textcopyright} 2010 IFAC.},
author = {Kritayakirana, Krisada and Gerdes, J. Christian},
doi = {10.3182/20100712-3-DE-2013.00060},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kritayakirana, Gerdes - 2010 - Autonomous cornering at the limits Maximizing a g-g diagram by using feedforward trail-braking and thr(2).pdf:pdf},
isbn = {9783902661722},
issn = {14746670},
journal = {IFAC Proceedings Volumes (IFAC-PapersOnline)},
keywords = {"g-g" diagram,Autonomous driving,Clothoid,Throttle-on-exit,Trail-braking,Vehicle dynamics,Vehicle dynamics control},
number = {7},
pages = {548--553},
publisher = {IFAC},
title = {{Autonomous cornering at the limits: Maximizing a "g-g" diagram by using feedforward trail-braking and throttle-on-exit}},
volume = {43},
year = {2010}
}
@book{IEEEexample:book,
address = {Berlin, Germany},
author = {Metev, S M and Veiko, V P},
edition = {Second},
editor = {{Osgood Jr.}, R M},
publisher = {Springer-Verlag},
title = {{Laser Assisted Microtechnology}},
year = {1998}
}
@article{Dankwa2019,
abstract = {In this current research, Twin-Delayed DDPG (TD3) algorithm has been used to solve the most challenging virtual Artificial Intelligence application by training a 4-ant-legged robot as an Intelligent Agent to run across a field. Twin-Delayed DDPG (TD3) is an incredibly smart AI model of a Deep Reinforcement Learning which combines the state-of-the-art methods in Artificial Intelligence. These includes Policy gradient, Actor-Critics, and continuous Double Deep Q-Learning. These Deep Reinforcement Learning approaches trained an Intelligent agent to interact with an environment with automatic feature engineering, that is, necessitating minimal domain knowledge. For the implementation of the TD3, we used a two-layer feedforward neural network of 400 and 300 hidden nodes respectively, with Rectified Linear Units (ReLU) as an activation function between each layer for both the Actor and Critics. We, then added a final tanh unit after the output of the Actor. The Critic receives both the state and action as input to the first layer. Both the network parameters were updated using Adam optimizer. The idea behind the Twin-Delayed DDPG (TD3) is to reduce overestimation bias in Deep Q-Learning with discrete actions which are ineffective in an Actor-Critic domain setting. Based on the Maximum Average Reward over the evaluation time-step, our model achieved an approximate maximum of 2364. Therefore, we can truly say that, TD3 has obviously improved on both the learning speed and performance of the Deep Deterministic Policy Gradient (DDPG) in a challenging environment in a continuous control domain.},
author = {Dankwa, Stephen and Zheng, Wenfeng},
doi = {10.1145/3387168.3387199},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dankwa, Zheng - 2019 - Twin-Delayed DDPG A Deep Reinforcement Learning Technique to Model a Continuous Movement of an Intelligent Rob(2).pdf:pdf},
isbn = {9781450376259},
journal = {ACM International Conference Proceeding Series},
keywords = {Actor-Critic,Artificial Intelligence,Deep Reinforcement Learning,Twin-Delayed Deep Deterministic Policy Gradient},
number = {August},
title = {{Twin-Delayed DDPG: A Deep Reinforcement Learning Technique to Model a Continuous Movement of an Intelligent Robot Agent}},
year = {2019}
}
@article{Thiam2014,
abstract = {In this paper we investigate reinforcement learning approaches for the popular computer game Tetris. User-defined reward functions have been applied to T D(0) learning based on $\epsilon$-greedy strategies in the standard Tetris scenario. The numerical experiments show that reinforcement learning can significantly outperform agents utilizing fixed policies.},
annote = {Method: 
TD(0) learning, with state values

Reward function:
Tetris cannot be won - dont give reward at end of game
Reward at any time step
Linear combinations of weighted features
features = highest column, average column height, holes, uneveness of profile

State encoding: Height difference between adjacent columns - limit height difference to +-3, truncate
State space still equals 40 milion states!

Training: 
Time measured in terms of number of pieces dropped
Learning steps vs played tetrimones per game
410 000 games
3.5*10{\^{}}9 moves

Results:
2nd value function performed better than 1st value function},
author = {Thiam, Patrick and Kessler, Viktor and Schwenker, Friedhelm},
doi = {10.1007/978-3-319-11656-3_15},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thiam, Kessler, Schwenker - 2014 - A reinforcement learning algorithm to train a tetris playing agent.pdf:pdf},
isbn = {9783319116556},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {165--170},
title = {{A reinforcement learning algorithm to train a tetris playing agent}},
volume = {8774},
year = {2014}
}
@article{Nieto-Cabrera2021,
abstract = {Consider learning a policy from example expert behavior, without interaction with the expert or access to a reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data as if it were obtained by reinforcement learning following inverse reinforcement learning. We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments.},
author = {Nieto-Cabrera, M Elena and Cramer, In{\'{e}}s Mar{\'{i}}a Mart{\'{i}}nez and Nieto-Morales, Concepcion},
doi = {10.2307/j.ctv1dp0vwx.25},
journal = {Discurso y Exp. Pers. Priv. Lib. afectos y Emoc. en riesgo. Aqu{\'{i}} y ahora al l{\'{i}}mite La mujer II},
number = {Nips},
pages = {65--66},
title = {{Gail}},
year = {2021}
}
@techreport{IEEEexample:techrep,
address = {MA},
author = {Jain, R and Ramakrishnan, K K and Chiu, D M},
institution = {Digital Equipment Corporation},
month = {aug},
number = {DEC-TR-506},
title = {{Congestion Avoidance in Computer Networks with a Connectionless Network Layer}},
year = {1987}
}
@misc{IEEEexample:beebe_archive,
author = {Beebe, Nelson H F},
month = {may},
title = {{{\{}$\backslash$TeX$\backslash$ {\}}User Group Bibliography Archive}},
url = {http://www.math.utah.edu:8080/pub/tex/bib/index-table.html},
year = {2002}
}
@article{Hoffmann2007,
abstract = {This paper presents a nonlinear control law for an automobile to autonomously track a trajectory, provided in real-time, on rapidly varying, off-road terrain. Existing methods can suffer from a lack of global stability, a lack of tracking accuracy, or a dependence on smooth road surfaces, any one of which could lead to the loss of the vehicle in autonomous off-road driving. This work treats automobile trajectory tracking in a new manner, by considering the orientation of the front wheels - not the vehicle's body - with respect to the desired trajectory, enabling collocated control of the system. A steering control law is designed using the kinematic equations of motion, for which global asymptotic stability is proven. This control law is then augmented to handle the dynamics of pneumatic tires and of the servo-actuated steering wheel. To control vehicle speed, the brake and throttle are actuated by a switching proportional integral (PI) controller. The complete control system consumes a negligible fraction of a computer's resources. It was implemented on a Volkswagen Touareg, "Stanley", the Stanford Racing Team's entry in the DARPA Grand Challenge 2005, a 132 mi autonomous off-road race. Experimental results from Stanley demonstrate the ability of the controller to track trajectories between obstacles, over steep and wavy terrain, through deep mud puddles, and along cliff edges, with a typical root mean square (RMS) crosstrack error of under 0.1 m. In the DARPA National Qualification Event 2005, Stanley was the only vehicle out of 40 competitors to not hit an obstacle or miss a gate, and in the DARPA Grand Challenge 2005 Stanley had the fastest course completion time. {\textcopyright}2007 IEEE.},
author = {Hoffmann, Gabriel M and Tomlin, Claire J and Montemerlo, Michael and Thrun, Sebastian},
doi = {10.1109/ACC.2007.4282788},
url = {https://doi.org/10.1109/ACC.2007.4282788},
journal = {Proc. Am. Control Conf.},
pages = {2296--2301},
title = {{Autonomous automobile trajectory tracking for off-road driving: Controller design, experimental validation and racing}},
year = {2007}
}
@article{BluePlanetSoftware2009,
author = {{Blue Planet Software}},
keywords = {Design,Guideline,Tetris},
title = {{2009 Tetris Design Guideline}},
url = {https://tetris.fandom.com/wiki/Tetris{\_}Guideline},
year = {2009}
}
@misc{IEEEexample:BSTcontrol,
title = {{No Title}}
}
@article{Hafner2020,
abstract = {Intelligent agents need to generalize from past experience to achieve goals in complex environments. World models facilitate such generalization and allow learning behaviors from imagined outcomes to increase sample-efficiency. While learning world models from image inputs has recently become feasible for some tasks, modeling Atari games accurately enough to derive successful behaviors has remained an open challenge for many years. We introduce DreamerV2, a reinforcement learning agent that learns behaviors purely from predictions in the compact latent space of a powerful world model. The world model uses discrete representations and is trained separately from the policy. DreamerV2 constitutes the first agent that achieves human-level performance on the Atari benchmark of 55 tasks by learning behaviors inside a separately trained world model. With the same computational budget and wall-clock time, Dreamer V2 reaches 200M frames and surpasses the final performance of the top single-GPU agents IQN and Rainbow. DreamerV2 is also applicable to tasks with continuous actions, where it learns an accurate world model of a complex humanoid robot and solves stand-up and walking from only pixel inputs.},
archivePrefix = {arXiv},
arxivId = {2010.02193},
author = {Hafner, Danijar and Lillicrap, Timothy and Norouzi, Mohammad and Ba, Jimmy},
eprint = {2010.02193},
pages = {1--26},
title = {{Mastering Atari with Discrete World Models}},
url = {http://arxiv.org/abs/2010.02193},
year = {2020}
}
@techreport{IEEEexample:techreptype,
address = {Amherst, MA},
author = {Padhye, J and Firoiu, V and Towsley, D},
institution = {Univ. of Massachusetts},
number = {99-02},
title = {{A Stochastic Model of {\{}TCP{\}} {\{}R{\}}eno Congestion Avoidance and Control}},
type = {CMPSCI Tech. Rep.},
year = {1999}
}
@article{Schulman2015,
abstract = {In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
archivePrefix = {arXiv},
arxivId = {1502.05477},
author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael and Abbeel, Pieter},
eprint = {1502.05477},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schulman et al. - 2015 - Trust region policy optimization.pdf:pdf},
isbn = {9781510810587},
journal = {32nd International Conference on Machine Learning, ICML 2015},
pages = {1889--1897},
title = {{Trust region policy optimization}},
volume = {3},
year = {2015}
}
@article{Marnin,
abstract = {Buildings with intermittent occupancy may not perform thermally the same as typical commercial and residential facilities. Thermal comfort requirements require careful envelope design coupled with the appropriate air-conditioning system operation strategies. One of the most prominent examples of such buildings is mosques. Mosques are usually occupied five intermittent times day and night all year round. Like any other building, they have to be mechanically air-conditioned to achieve the required thermal comfort for worshippers especially in harsh climatic regions. This paper describes the physical and operating characteristics typical for the intermittently occupied mosques as well as the results of the thermal optimization of a medium size mosque in the two hot-dry and hot-humid Saudi Arabian cities of Riyadh and Jeddah. The analysis utilizes a direct search optimization technique that is coupled to an hourly energy simulation program. Based on that, design guidelines are presented for the optimum thermal performance of mosques in these two cities in addition to other design and operating factors that need to be considered for mosques in general. {\textcopyright} 2009 The Author(s).},
author = {Marnin, John},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Marnin - Unknown - REINFORCEMENT LEARNING ALGORITHMS FOR REPRESENTING AND MANAGING UNCERTAINTY IN ROBOTICS(2).pdf:pdf},
title = {{REINFORCEMENT LEARNING ALGORITHMS FOR REPRESENTING AND MANAGING UNCERTAINTY IN ROBOTICS}},
url = {https://robustfieldautonomylab.github.io/John{\_}Martin{\_}PhD{\_}Thesis.pdf}
}
@incollection{IEEEexample:incollectionwithseries,
address = {New York},
author = {Anderson, J B and Tepe, K},
booktitle = {Codes, Systems and Graphical Models},
publisher = {Springer-Verlag},
series = {{\{}IMA{\}} Volumes in Mathematics and Its Applications},
title = {{Properties of the Tailbiting {\{}BCJR{\}} Decoder}},
year = {2000}
}
@article{Chow2019,
abstract = {We study continuous action reinforcement learning problems in which it is crucial that the agent interacts with the environment only through safe policies, i.e.,{\~{}}policies that do not take the agent to undesirable situations. We formulate these problems as constrained Markov decision processes (CMDPs) and present safe policy optimization algorithms that are based on a Lyapunov approach to solve them. Our algorithms can use any standard policy gradient (PG) method, such as deep deterministic policy gradient (DDPG) or proximal policy optimization (PPO), to train a neural network policy, while guaranteeing near-constraint satisfaction for every policy update by projecting either the policy parameter or the action onto the set of feasible solutions induced by the state-dependent linearized Lyapunov constraints. Compared to the existing constrained PG algorithms, ours are more data efficient as they are able to utilize both on-policy and off-policy data. Moreover, our action-projection algorithm often leads to less conservative policy updates and allows for natural integration into an end-to-end PG training pipeline. We evaluate our algorithms and compare them with the state-of-the-art baselines on several simulated (MuJoCo) tasks, as well as a real-world indoor robot navigation problem, demonstrating their effectiveness in terms of balancing performance and constraint satisfaction. Videos of the experiments can be found in the following link: https://drive.google.com/file/d/1pzuzFqWIE710bE2U6DmS59AfRzqK2Kek/view?usp=sharing.},
archivePrefix = {arXiv},
arxivId = {1901.10031},
author = {Chow, Yinlam and Nachum, Ofir and Faust, Aleksandra and Duenez-Guzman, Edgar and Ghavamzadeh, Mohammad},
eprint = {1901.10031},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chow et al. - 2019 - Lyapunov-based Safe Policy Optimization for Continuous Control.pdf:pdf},
title = {{Lyapunov-based Safe Policy Optimization for Continuous Control}},
url = {http://arxiv.org/abs/1901.10031},
year = {2019}
}
@article{Kabzan2019,
abstract = {In this letter, we present a learning-based control approach for autonomous racing with an application to the AMZ Driverless race car gotthard. One major issue in autonomous racing is that accurate vehicle models that cover the entire performance envelope of a race car are highly nonlinear, complex, and complicated to identify, rendering them impractical for control. To address this issue, we employ a relatively simple nominal vehicle model, which is improved based on measurement data and tools from machine learning.The resulting formulation is an online learning data-driven model predictive controller, which uses Gaussian processes regression to take residual model uncertainty into account and achieve safe driving behavior. To improve the vehicle model online, we select from a constant in-flow of data points according to a criterion reflecting the information gain, and maintain a small dictionary of 300 data points. The framework is tested on the full-size AMZ Driverless race car, where it is able to improve the vehicle model and reduce lap times by {\{}$\backslash$mathbf{\{}10{\}}{\{}$\backslash${\%}{\}}{\}} while maintaining safety of the vehicle.},
annote = {MPC used gaussian process regression to take MODEL uncertainty into account},
author = {Kabzan, Juraj and Hewing, Lukas and Liniger, Alexander and Zeilinger, Melanie N.},
doi = {10.1109/LRA.2019.2926677},
url = {https://doi.org/10.1109/LRA.2019.2926677},
journal = {IEEE Robotics and Automation Letters},
keywords = {Model learning for control,autonomous racing,learning and adaptive systems,model predictive control},
number = {4},
pages = {3363--3370},
publisher = {IEEE},
title = {{Learning-Based Model Predictive Control for Autonomous Racing}},
volume = {4},
year = {2019}
}

@article{Gillespie2017,
abstract = {Intelligent agents have a wide range of applications in robotics, video games, and computer simulations. However, fully general agents should function with as little human guidance as possible. Specifically, agents should learn from large collections of raw state variables instead of small collections of hand-designed features. Learning from raw state variables is difficult, but can be easier when agents are aware of the geometry of the input space. Indirect encodings allow agents to take advantage of the geometry of the task, and scale up to large input spaces. This research demonstrates the relative benefits of a direct and indirect encoding using raw or hand-designed features in Tetris, a challenging video game. Specifically, the direct encoding NEAT is compared against the indirect encoding HyperNEAT. Both algorithms create neural networks to play the game, but HyperNEAT makes better use of raw screen inputs, due to its ability to generate large networks that take advantage of the domain's geometry However, hand-designed features lead to higher scores with both algorithms. HyperNEAT makes better use of hand-designed features early in evolution, but NEAT eventually overtakes it. Since each method succeeds in different circumstances, approaches combining the strengths of both should be explored.},
author = {Gillespie, Lauren E. and Gonzalez, Gabriela R. and Schram, Jacob},
doi = {10.1145/3071178.3071195},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gillespie, Gonzalez, Schram - 2017 - Comparing direct and indirect encodings using both raw and hand-designed features in Tetris.pdf:pdf},
isbn = {9781450349208},
journal = {GECCO 2017 - Proceedings of the 2017 Genetic and Evolutionary Computation Conference},
keywords = {Games,Indirect encoding,Neural networks,Tetris},
pages = {179--186},
title = {{Comparing direct and indirect encodings using both raw and hand-designed features in Tetris}},
url = {https://dl.acm.org/doi/pdf/10.1145/3071178.3071195},
year = {2017}
}
@article{Chisari2021,
abstract = {We present a reinforcement learning-based solution to autonomously race on a miniature race car platform. We show that a policy that is trained purely in simulation using a relatively simple vehicle model, including model randomization, can be successfully transferred to the real robotic setup. We achieve this by using a novel policy output regularization approach and a lifted action space which enables smooth actions but still aggressive race car driving. We show that this regularized policy does outperform the Soft Actor Critic (SAC) baseline method, both in simulation and on the real car, but it is still outperformed by a Model Predictive Controller (MPC) state-of-the-art method. The refinement of the policy with three hours of real-world interaction data allows the reinforcement learning policy to achieve lap times similar to the MPC controller while reducing track constraint violations by 50{\%}.},
archivePrefix = {arXiv},
arxivId = {2011.13332},
author = {Chisari, Eugenio and Liniger, Alexander and Rupenyan, Alisa and van Gool, Luc and Lygeros, John},
doi = {10.1109/ICRA48506.2021.9562079},
url = {https://doi.org/10.1109/ICRA48506.2021.9562079},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
number = {December},
pages = {8046--8052},
title = {{Learning from Simulation, Racing in Reality}},
url = {https://www.researchgate.net/publication/346475535{\_}Learning{\_}from{\_}Simulation{\_}Racing{\_}in{\_}Reality},
volume = {2021-May},
year = {2021}
}
@article{Huang2020,
abstract = {This chapter aims to introduce one of the most important deep reinforcement learning algorithms, called deep Q-networks. We will start with the Q-learning algorithm via temporal difference learning, and introduce the deep Q-networks algorithm and its variants. We will end this chapter with code examples and experimental comparison of deep Q-networks and its variants in practice.},
author = {Huang, Yanhua},
doi = {10.1007/978-981-15-4095-0_4},
isbn = {9789811540950},
journal = {Deep Reinf. Learn. Fundam. Res. Appl.},
keywords = {DQN,Distributional reinforcement learning,Double DQN,Dueling DQN,Prioritized experience replay,Temporal difference learning},
pages = {135--160},
title = {{Deep Q-networks}},
year = {2020}
}
@article{Bertsekas1997,
abstract = {We introduce a new policy iteration method for dynamic programming problems with dis-counted and undiscounted cost. The method is based on the notion of temporal differences, and is primarily geared to the case of large and complex problems where the use of approximations is essential. We develop the theory of the method without approximation, we describe how to em-bed it within a neuro-dynamic programming/reinforcement learning context where feature-based approximation architectures are used, we relate it to TD($\lambda$) methods, and we illustrate its use in the training of a tetris playing program.},
author = {Bertsekas, Dimitri P and Ioffe, Sergey},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bertsekas, Ioffe - 1997 - Temporal Differences-Based Policy Iteration and Applications in Neuro-Dynamic Programming.pdf:pdf},
journal = {Electrical Engineering},
number = {August 1996},
pages = {1--19},
title = {{Temporal Differences-Based Policy Iteration and Applications in Neuro-Dynamic Programming}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.7759{\&}rep=rep1{\&}type=pdf},
volume = {1996},
year = {1997}
}
@misc{IEEEexample:BSTcontrol,
title = {{No Title}}
}

@InProceedings{Fujimoto2018,
  title = 	 {Addressing Function Approximation Error in Actor-Critic Methods},
  author =       {Fujimoto, Scott and van Hoof, Herke and Meger, David},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1587--1596},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/fujimoto18a/fujimoto18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/fujimoto18a.html},
  abstract = 	 {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.}
}

@phdthesis{IEEEexample:phdurl,
address = {Newark},
author = {Li, Q},
month = {may},
school = {Univ. of Delaware},
title = {{Delay Characterization and Performance Control of Wide-area Networks}},
url = {http://www.ece.udel.edu/{~}qli},
year = {2000}
}
@article{IEEEexample:TBParticle,
author = {Kahale, N and Urbanke, R},
title = {{On the Minimum Distance of Parallel and Serially Concatenated Codes}}
}
@article{Cai2021,
abstract = {Autonomous car racing is a challenging task in the robotic control area. Traditional modular methods require accurate mapping, localization and planning, which makes them computationally inefficient and sensitive to environmental changes. Recently, deep-learning-based end-to-end systems have shown promising results for autonomous driving/racing. However, they are commonly implemented by supervised imitation learning (IL), which suffers from the distribution mismatch problem, or by reinforcement learning (RL), which requires a huge amount of risky interaction data. In this work, we present a general deep imitative reinforcement learning approach (DIRL), which successfully achieves agile autonomous racing using visual inputs. The driving knowledge is acquired from both IL and model-based RL, where the agent can learn from human teachers as well as perform self-improvement by safely interacting with an offline world model. We validate our algorithm both in a high-fidelity driving simulation and on a real-world 1/20-scale RC-car with limited onboard computation. The evaluation results demonstrate that our method outperforms previous IL and RL methods in terms of sample efficiency and task performance. Demonstration videos are available at https://caipeide.github.io/autorace-dirl/.},
archivePrefix = {arXiv},
arxivId = {2107.08325},
author = {Cai, Peide and Wang, Hengli and Huang, Huaiyang and Liu, Yuxuan and Liu, Ming},
doi = {10.1109/LRA.2021.3097345},
journal = {IEEE Robotics and Automation Letters},
keywords = {Reinforcement learning,autonomous racing,imitation learning,model learning for control,uncertainty awareness},
number = {4},
pages = {7262--7269},
title = {{Vision-Based Autonomous Car Racing Using Deep Imitative Reinforcement Learning}},
url = {https://www.researchgate.net/publication/353566937{\_}Vision-Based{\_}Autonomous{\_}Car{\_}Racing{\_}Using{\_}Deep{\_}Imitative{\_}Reinforcement{\_}Learning},
volume = {6},
year = {2021}
}
@misc{IEEEexample:electronhowinfo,
author = {Jacobson, V},
howpublished = {end2end-interest mailing list},
month = {apr},
title = {{Modified {\{}TCP{\}} Congestion Avoidance Algorithm}},
url = {ftp://ftp.isi.edu/end2end/end2end-interest-1990.mail},
year = {1990}
}
@misc{IEEEexample:electronorgadd,
address = {Haifa, Israel},
author = {Lorenz, D H and Orda, A},
institution = {Dept. Elect. Eng., Technion},
month = {jul},
title = {{Optimal Partition of {\{}QoS{\}} Requirements on Unicast Paths and Multicast Trees}},
url = {ftp://ftp.technion.ac.il/pub/supported/ee/Network/lor.mopq98.ps},
year = {1998}
}
@article{Perot2017,
abstract = {We address the problem of autonomous race car driving. Using a recent rally game (WRC6) with realistic physics and graphics we train an Asynchronous Actor Critic (A3C) in an end-to-end fashion and propose an improved reward function to learn faster. The network is trained simultaneously on three very different tracks (snow, mountain, and coast) with various road structures, graphics and physics. Despite the more complex environments the trained agent learns significant features and exhibits good performance while driving in a more stable way than existing end-to-end approaches.},
archivePrefix = {arXiv},
author = {Perot, Etienne and Jaritz, Maximilian and Toromanoff, Marin and Charette, Raoul De},
doi = {10.1109/CVPRW.2017.64},
url = {https://doi.org/10.1109/CVPRW.2017.64},
journal = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
pages = {474--475},
publisher = {IEEE},
title = {{End-to-End Driving in a Realistic Racing Game with Deep Reinforcement Learning}},
volume = {2017-July},
year = {2017}
}
@misc{IEEEexample:periodical,
month = {aug},
title = {{, Special Issue on Wireless {\{}ATM{\}}}},
volume = {3},
year = {1996}
}
@article{Ha2018,
abstract = {A generative recurrent neural network is quickly trained in an unsupervised manner to model popular reinforcement learning environments through compressed spatio-temporal representations. The world model's extracted features are fed into compact and simple policies trained by evolution, achieving state of the art results in various environments. We also train our agent entirely inside of an environment generated by its own internal world model, and transfer this policy back into the actual environment. Interactive version of paper: https://worldmodels.github.io.},
author = {Ha, David and Schmidhuber, J{\"{u}}rgen},
issn = {10495258},
journal = {Adv. Neural Inf. Process. Syst.},
number = {C},
pages = {2450--2462},
title = {{Recurrent world models facilitate policy evolution}},
volume = {2018-Decem},
year = {2018}
}
@article{Pup2020,
abstract = {The information about a system's dynamics represented by measurement data sets are often confined to regions of restricted operations where the system is not sufficiently excited for model identification purposes. Experiments performed in closed-loop with safety constraints allow only for reduced order modeling. In the paper, a set of low order models are identified from real experimental data of the lateral dynamics of an electric passenger car. Low order models are advantageous for on-line computation in model-based control, though uncertainty due to neglected dynamics may deteriorate control performance and constraint satisfaction. The effect of uncertainty is analyzed by controller cross-validation where a controller designed based on one model is evaluated on other models playing the role of the true system. This method allows us to qualify not only model-controller pairs, but to determine the properties of input data and model uncertainty, which lead to more useful data sets, more robust and better performing controllers than the others.},
author = {Pup, Daniel and Kisari, Adam and Vigh, Zsombor and Rodonyi, Gabor and Soumelidis, Alexandros and Bokor, Jozsef},
doi = {10.1109/ISMCR51255.2020.9263745},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pup et al. - 2020 - Characterization of Model Uncertainty Features Relevant to Model Predictive Control of Lateral Vehicle Dynamics(2).pdf:pdf},
isbn = {9781665404792},
journal = {2020 23rd IEEE International Symposium on Measurement and Control in Robotics, ISMCR 2020},
keywords = {classification,feature selection,model predictive control,uncertainty modeling},
title = {{Characterization of Model Uncertainty Features Relevant to Model Predictive Control of Lateral Vehicle Dynamics}},
url = {https://www.academia.edu/68062803/Characterization{\_}of{\_}Model{\_}Uncertainty{\_}Features{\_}Relevant{\_}to{\_}Model{\_}Predictive{\_}Control{\_}of{\_}Lateral{\_}Vehicle{\_}Dynamics?from{\_}sitemaps=true{\&}version=2},
year = {2020}
}
@misc{IEEEexample:standardproposed,
address = {Washington, DC},
institution = {NCITS},
title = {{Fiber Channel Physical Interface ({\{}FC-PI{\}})}},
type = {Working Draft Proposed Standard},
year = {1999}
}
@article{Bellegarda2020,
abstract = {Recent breakthroughs both in reinforcement learning and trajectory optimization have made significant advances towards real world robotic system deployment. Reinforcement learning (RL) can be applied to many problems without needing any modeling or intuition about the system, at the cost of high sample complexity and the inability to prove any metrics about the learned policies. Trajectory optimization (TO) on the other hand allows for stability and robustness analyses on generated motions and trajectories, but is only as good as the often over-simplified derived model, and may have prohibitively expensive computation times for real-time control, for example in contact rich environments. This paper seeks to combine the benefits from these two areas while mitigating their drawbacks by (1) decreasing RL sample complexity by using existing knowledge of the problem with real-time optimal control, and (2) allowing online policy deployment at any point in the training process by using the TO (MPC) as a baseline or worst-case scenario action, while continuously improving the combined learned-optimized policy with deep RL. This method is evaluated on tasks of successively navigating a car model to a series of goal destinations over slippery terrains as fast as possible, in which drifting will allow the system to more quickly change directions while maintaining high speeds.},
author = {Bellegarda, Guillaume and Byl, Katie},
doi = {10.1109/IROS45743.2020.9341021},
isbn = {9781728162126},
issn = {21530866},
journal = {IEEE Int. Conf. Intell. Robot. Syst.},
pages = {5453--5459},
title = {{An online training method for augmenting MPC with deep reinforcement learning}},
year = {2020}
}
@article{Bansal2019,
abstract = {Our goal is to train a policy for autonomous driving via imitation learning that is robust enough to drive a real vehicle. We find that standard behavior cloning is insufficient for handling complex driving scenarios, even when we leverage a perception system for preprocessing the input and a controller for executing the output on the car: 30 million examples are still not enough. We propose exposing the learner to synthesized data in the form of perturbations to the expert's driving, which creates interesting situations such as collisions and/or going off the road. Rather than purely imitating all data, we augment the imitation loss with additional losses that penalize undesirable events and encourage progress – the perturbations then provide an important signal for these losses and lead to robustness of the learned model. We show that the ChauffeurNet model can handle complex situations in simulation, and present ablation experiments that emphasize the importance of each of our proposed changes and show that the model is responding to the appropriate causal factors. Finally, we demonstrate the model driving a real car at our test facility.},
archivePrefix = {arXiv},
arxivId = {1812.03079},
author = {Bansal, Mayank and Krizhevsky, Alex and Ogale, Abhijit},
doi = {10.15607/rss.2019.xv.031},
eprint = {1812.03079},
isbn = {9780992374754},
issn = {2330765X},
keywords = {deep learning,learning to drive,mid-to-mid driving,trajectory predic-},
pages = {1--20},
title = {{ChauffeurNet: Learning to Drive by Imitating the Best and Synthesizing the Worst}},
year = {2019}
}
@article{Loquercio2020,
abstract = {Dynamically changing environments, unreliable state estimation, and operation under severe resource constraints are fundamental challenges that limit the deployment of small autonomous drones. We address these challenges in the context of autonomous, vision-based drone racing in dynamic environments. A racing drone must traverse a track with possibly moving gates at high speed. We enable this functionality by combining the performance of a state-of-the-art planning and control system with the perceptual awareness of a convolutional neural network. The resulting modular system is both platform independent and domain independent: it is trained in simulation and deployed on a physical quadrotor without any fine-tuning. The abundance of simulated data, generated via domain randomization, makes our system robust to changes of illumination and gate appearance. To the best of our knowledge, our approach is the first to demonstrate zero-shot sim-to-real transfer on the task of agile drone flight. We extensively test the precision and robustness of our system, both in simulation and on a physical platform, and show significant improvements over the state of the art.},
archivePrefix = {arXiv},
arxivId = {1905.09727},
author = {Loquercio, Antonio and Kaufmann, Elia and Ranftl, Rene and Dosovitskiy, Alexey and Koltun, Vladlen and Scaramuzza, Davide},
doi = {10.1109/TRO.2019.2942989},
eprint = {1905.09727},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Loquercio et al. - 2020 - Deep Drone Racing From Simulation to Reality with Domain Randomization.pdf:pdf},
issn = {19410468},
journal = {IEEE Transactions on Robotics},
keywords = {Drone racing,learning agile flight,learning for control},
number = {1},
pages = {1--14},
title = {{Deep Drone Racing: From Simulation to Reality with Domain Randomization}},
volume = {36},
year = {2020}
}
@article{Thiery2009a,
annote = {Note: ALgorithms are difficult to compare - niose + different tetris implementations

Tetris domain reinforcement learning competition!

Goal: Maximise average number of lines scored per game

Tetris as a test bed: lambda policy iteration algorithm by vertsekas and tsitsiklis!

See table of features!

Implementation: Drop piece from teh top

See notes on confidence interval!

Their method:
Cross entropy, based on Szita and Lorinez using Dellacherie features
Performs after-state evaluation 

2 piece vs 1 piece controllers: Reinforcement learning approaches so far use 2 piece},
author = {Thiery, Christophe and Scherrer, Bruno and Thiery, Christophe and Scherrer, Bruno and Controllers, Building and Computer, International},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thiery et al. - 2009 - Building Controllers for Tetris.pdf:pdf},
pages = {3--11},
title = {{Building Controllers for Tetris}},
url = {https://hal.inria.fr/inria-00418954/document},
year = {2009}
}
@article{Zeng,
author = {Zeng, Baihong and Wang, Yuwei and Mangharam, Instructor Rahul},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zeng, Wang, Mangharam - Unknown - ROBO 599 Masters Independent Study Learning Model Predictive Control on F1tenth Race Car.pdf:pdf},
title = {{ROBO 599 Masters Independent Study Learning Model Predictive Control on F1tenth Race Car}}
}
@misc{IEEEexample:bibtexuser,
author = {Patashnik, Oren},
howpublished = {btxdoc.pdf},
month = {feb},
title = {{{\{}{\$}\backslash{\$}BibTeX{\}}ing}},
url = {http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/},
year = {1988}
}
@article{Jeon2013,
abstract = {We discuss an implementation of the RRT* optimal motion planning algorithm for the half-car dynamical model to enable autonomous high-speed driving. To develop fast solutions of the associated local steering problem, we observe that the motion of a special point (namely, the front center of oscillation) can be modeled as a double integrator augmented with fictitious inputs. We first map the constraints on tire friction forces to constraints on these augmented inputs, which provides instantaneous, state-dependent bounds on the curvature of geometric paths feasibly traversable by the front center of oscillation. Next, we map the vehicle's actual inputs to the augmented inputs. The local steering problem for the half-car dynamical model can then be transformed to a simpler steering problem for the front center of oscillation, which we solve efficiently by first constructing a curvature-bounded geometric path and then imposing a suitable speed profile on this geometric path. Finally, we demonstrate the efficacy of the proposed motion planner via numerical simulation results. {\textcopyright} 2013 AACC American Automatic Control Council.},
author = {Jeon, Jeong Hwan and Cowlagi, Raghvendra V. and Peters, Steven C. and Karaman, Sertac and Frazzoli, Emilio and Tsiotras, Panagiotis and Iagnemma, Karl},
doi = {10.1109/acc.2013.6579835},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jeon et al. - 2013 - Optimal motion planning with the half-car dynamical model for autonomous high-speed driving.pdf:pdf},
isbn = {9781479901777},
issn = {07431619},
journal = {Proceedings of the American Control Conference},
pages = {188--193},
publisher = {IEEE},
title = {{Optimal motion planning with the half-car dynamical model for autonomous high-speed driving}},
year = {2013}
}
@article{Schaul2016,
abstract = {Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.},
archivePrefix = {arXiv},
arxivId = {1511.05952},
author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
eprint = {1511.05952},
journal = {4th Int. Conf. Learn. Represent. ICLR 2016 - Conf. Track Proc.},
pages = {1--21},
title = {{Prioritized experience replay}},
year = {2016}
}
@misc{weiss2020a,
  doi = {10.48550/ARXIV.2005.05178},
  url = {https://doi.org/10.48550/arXiv.2005.05178},
  author = {Weiss, Trent and Behl, Madhur},
  keywords = {Robotics (cs.RO), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Systems and Control (eess.SY), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  title = {DeepRacing: Parameterized Trajectories for Autonomous Racing},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{IEEEexample:articledualmonths,
author = {Okada, Y and Dejima, K and Ohishi, T},
pages = {1047--1053},
title = {{Analysis and Comparison of {\{}PM{\}} Synchronous Motor and Induction Motor Type Magnetic Bearings}},
volume = {31},
year = {1995}
}
@article{Geibel2005,
abstract = {In this paper, we consider Markov Decision Processes (MDPs) with error states. Error states are those states entering which is undesirable or dangerous. We define the risk with respect to a policy as the probability of entering such a state when the policy is pursued. We consider the problem of finding good policies whose risk is smaller than some user-specified threshold, and formalize it as a constrained MDP with two criteria. The first criterion corresponds to the value function originally given. We will show that the risk can be formulated as a second criterion function based on a cumulative return, whose definition is independent of the original value function. We present a model free, heuristic reinforcement learning algorithm that aims at finding good deterministic policies. It is based on weighting the original value function and the risk. The weight parameter is adapted in order to find a feasible solution for the constrained problem that has a good performance with respect to the value function. The algorithm was successfully applied to the control of a feed tank with stochastic inflows that lies upstream of a distillation column. This control task was originally formulated as an optimal control problem with chance constraints, and it was solved under certain assumptions on the model to obtain an optimal solution. The power of our learning algorithm is that it can be used even when some of these restrictive assumptions are relaxed. {\textcopyright}2005 AI Access Foundation. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {1109.2147},
author = {Geibel, Peter and Wysotzki, Fritz},
doi = {10.1613/jair.1666},
eprint = {1109.2147},
issn = {10769757},
journal = {J. Artif. Intell. Res.},
pages = {81--108},
title = {{Risk-sensitive reinforcement learning applied to control under constraints}},
volume = {24},
year = {2005}
}
@article{Meng2020,
abstract = {Multi-step (also called n-step) methods in Reinforcement Learning (RL) have been shown to be more efficient than the 1-step method due to faster propagation of the reward signal, both theoretically and empirically, in tasks exploiting tabular representation of the value-function. Recently, research in Deep Reinforcement Learning (DRL) also shows that multi-step methods improve learning speed and final performance in applications where the value-function and policy are represented with deep neural networks. However, there is a lack of understanding about what is actually contributing to the boost of performance. In this work, we analyze the effect of multi-step methods on alleviating the overestimation problem in DRL, where multi-step experiences are sampled from a replay buffer. Specifically building on top of Deep Deterministic Policy Gradient (DDPG), we propose Multi-step DDPG (MDDPG), where different step sizes are manually set, and a variant called Mixed Multi-step DDPG (MMDDPG) where an average over different multi-step backups is used as an update target for the Q-value function. Empirically, we show that both MDDPG and MMDDPG are significantly less affected by the overestimation problem than DDPG with 1-step backup, which consequently results in better final performance and learning speed. We also discuss the advantages and disadvantages of different ways to do multi-step expansion in order to reduce approximation error, and expose the tradeoff between overestimation and underestimation that underlies offline multi-step methods. Finally, we compare the computational resource needs of MDDPG and MMDDPG with those of Twin Delayed Deep Deterministic Policy Gradient (TD3), a state-of-the-art algorithm proposed to address overestimation in actor-critic methods, since they show comparable final performance and learning speed.},
archivePrefix = {arXiv},
arxivId = {2006.12692},
author = {Meng, Lingheng and Gorbet, Rob and Kuli{\'{c}}, Dana},
doi = {10.1109/ICPR48806.2021.9413027},
eprint = {2006.12692},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Meng, Gorbet, Kuli{\'{c}} - 2020 - The effect of multi-step methods on overestimation in deep reinforcement learning.pdf:pdf},
isbn = {9781728188089},
issn = {10514651},
journal = {Proceedings - International Conference on Pattern Recognition},
pages = {9740--9747},
title = {{The effect of multi-step methods on overestimation in deep reinforcement learning}},
year = {2020}
}
@article{Bevilacqua2017,
annote = {Strategies for determining optimal trajectory:
1. Highest velocity (minimum curvature)
2. Lowest distance
Use genetic algorithm to find optimal weighting of combination of objectives 1 and 2.

OR:
Cost function = maximise distance in fixed number of time steps.
No vehicle dynamics - problem reduces to shortest path.

Track encoding:
Centerline in (x,y) coordinates

Results:
Genetic algorithm convergence in 2 hours
Particle swarm convergence in 30 minutes},
author = {Bevilacqua, M and Tsourdos, A and Starr, A},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bevilacqua, Tsourdos, Starr - 2017 - Particle swarm for path planning in a racing circuit simulation(2).pdf:pdf},
isbn = {9781509035960},
keywords = {as a trajectory for,autonomous races,current position,during qualification rounds,ex,genetic algorithm,laps done alone,more importantly,optimization,or,particle swarm,race line,trajectory planning},
publisher = {IEEE},
title = {{Particle swarm for path planning in a racing circuit simulation}},
url = {http://proceedings.mlr.press/v123/o-kelly20a.html},
year = {2017}
}
@article{Guckiran2019,
abstract = {Self-Driving Cars are, currently a hot topic throughout the globe thanks to the advancements in Deep Learning techniques on computer vision problems. Since driving simulations are fairly important before real life autonomous implementations, there are multiple driving-racing simulations for testing purposes. The Open Racing Car Simulation (TORCS) is a highly portable open source car racing-self-driving-simulation. While it can be used as a game in which human players compete with scripted agents, TORCS provides observation and action API to develop an artificial intelligence agent. This study explores near-optimal Deep Reinforcement Learning agents for TORCS environment using Soft Actor-Critic and Rainbow DQN algorithms, exploration and generalization techniques.},
author = {Guckiran, Kivanc and Bolat, Bulent},
doi = {10.1109/ASYU48272.2019.8946332},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Guckiran, Bolat - 2019 - Autonomous Car Racing in Simulation Environment Using Deep Reinforcement Learning.pdf:pdf},
isbn = {9781728128689},
journal = {Proceedings - 2019 Innovations in Intelligent Systems and Applications Conference, ASYU 2019},
keywords = {Deep Reinforcement Learning,Self-Driving Car,TORCS},
title = {{Autonomous Car Racing in Simulation Environment Using Deep Reinforcement Learning}},
year = {2019}
}
@article{Wang2016,
abstract = {In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.},
archivePrefix = {arXiv},
arxivId = {1511.06581},
author = {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and {Van Hasselt}, Hado and Lanctot, Marc and {De Frcitas}, Nando},
eprint = {1511.06581},
isbn = {9781510829008},
journal = {33rd Int. Conf. Mach. Learn. ICML 2016},
number = {9},
pages = {2939--2947},
title = {{Dueling Network Architectures for Deep Reinforcement Learning}},
volume = {4},
year = {2016}
}
@article{Thiery2009a,
annote = {Note: ALgorithms are difficult to compare - niose + different tetris implementations

Tetris domain reinforcement learning competition!

Goal: Maximise average number of lines scored per game

Tetris as a test bed: lambda policy iteration algorithm by vertsekas and tsitsiklis!

See table of features!

Implementation: Drop piece from teh top

See notes on confidence interval!

Their method:
Cross entropy, based on Szita and Lorinez using Dellacherie features
Performs after-state evaluation 

2 piece vs 1 piece controllers: Reinforcement learning approaches so far use 2 piece},
author = {Thiery, Christophe and Scherrer, Bruno and Thiery, Christophe and Scherrer, Bruno and Controllers, Building and Computer, International},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thiery et al. - 2009 - Building Controllers for Tetris.pdf:pdf},
pages = {3--11},
title = {{Building Controllers for Tetris To cite this version :}},
url = {https://hal.inria.fr/inria-00418954/document},
year = {2009}
}
@article{Perez2008,
abstract = {The techniques and the technologies supportingAutomatic Vehicle Guidance are important issues. Automobile manufacturers view automatic driving as a very interesting product with motivating key features which allow improvement of the car safety, reduction in emission or fuel consumption or optimization of driver comfort during long journeys. Car racing is an active research field where new advances in aerodynamics, consumption and engine power are critical each season. Our proposal is to research how evolutionary computation techniques can help in this field. For this work we have designed an automatic controller that learns rules with a genetic algorithm. This paper is a report of the results obtained by this controller during the car racing competition held in Hong Kong during the IEEE World Congress on Computational Intelligence (WCCI 2008). {\textcopyright}2008 IEEE.},
author = {Perez, Diego and Saez, Yago and Recio, Gustavo and Isasi, Pedro},
doi = {10.1109/CIG.2008.5035659},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Perez et al. - 2008 - Evolving a rule system controller for automatic driving in a car racing competition(2).pdf:pdf},
isbn = {9781424429745},
journal = {2008 IEEE Symposium on Computational Intelligence and Games, CIG 2008},
pages = {336--342},
title = {{Evolving a rule system controller for automatic driving in a car racing competition}},
url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp={\&}arnumber=5035659},
year = {2008}
}
@misc{Evans2021b,
  doi = {10.48550/ARXIV.2102.11042},
  url = {https://arxiv.org/abs/2102.11042},
  author = {Evans, Benjamin and Jordaan, Hendrik W. and Engelbrecht, Herman A.},
  keywords = {Robotics (cs.RO), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Learning the Subsystem of Local Planning for Autonomous Racing},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@article{Jaritz2018,
abstract = {We present research using the latest reinforcement learning algorithm for end-to-end driving without any mediated perception (object recognition, scene understanding). The newly proposed reward and learning strategies lead together to faster convergence and more robust driving using only RGB image from a forward facing camera. An Asynchronous Actor Critic (A3C) framework is used to learn the car control in a physically and graphically realistic rally game, with the agents evolving simultaneously on tracks with a variety of road structures (turns, hills), graphics (seasons, location) and physics (road adherence). A thorough evaluation is conducted and generalization is proven on unseen tracks and using legal speed limits. Open loop tests on real sequences of images show some domain adaption capability of our method.},
archivePrefix = {arXiv},
arxivId = {1807.02371},
author = {Jaritz, Maximilian and {De Charette}, Raoul and Toromanoff, Marin and Perot, Etienne and Nashashibi, Fawzi},
doi = {10.1109/ICRA.2018.8460934},
url = {https://doi.org/10.1109/ICRA.2018.8460934},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {2070--2075},
title = {{End-to-End Race Driving with Deep Reinforcement Learning}},
year = {2018}
}
@article{Grigorescu2020,
abstract = {The last decade witnessed increasingly rapid progress in self-driving vehicle technology, mainly backed up by advances in the area of deep learning and artificial intelligence (AI). The objective of this paper is to survey the current state-of-the-art on deep learning technologies used in autonomous driving. We start by presenting AI-based self-driving architectures, convolutional and recurrent neural networks, as well as the deep reinforcement learning paradigm. These methodologies form a base for the surveyed driving scene perception, path planning, behavior arbitration, and motion control algorithms. We investigate both the modular perception-planning-action pipeline, where each module is built using deep learning methods, as well as End2End systems, which directly map sensory information to steering commands. Additionally, we tackle current challenges encountered in designing AI architectures for autonomous driving, such as their safety, training data sources, and computational hardware. The comparison presented in this survey helps gain insight into the strengths and limitations of deep learning and AI approaches for autonomous driving and assist with design choices.},
archivePrefix = {arXiv},
arxivId = {1910.07738},
author = {Grigorescu, Sorin and Trasnea, Bogdan and Cocias, Tiberiu and Macesanu, Gigel},
doi = {10.1002/rob.21918},
eprint = {1910.07738},
file = {::},
issn = {15564967},
journal = {Journal of Field Robotics},
keywords = {AI for self-driving vehicles,artificial intelligence,autonomous driving,deep learning for autonomous driving},
number = {3},
pages = {362--386},
title = {{A survey of deep learning techniques for autonomous driving}},
volume = {37},
year = {2020}
}
@article{Yurtsever2020,
abstract = {Automated driving systems (ADSs) promise a safe, comfortable and efficient driving experience. However, fatalities involving vehicles equipped with ADSs are on the rise. The full potential of ADSs cannot be realized unless the robustness of state-of-the-art is improved further. This paper discusses unsolved problems and surveys the technical aspect of automated driving. Studies regarding present challenges, high-level system architectures, emerging methodologies and core functions including localization, mapping, perception, planning, and human machine interfaces, were thoroughly reviewed. Furthermore, many state-of-the-art algorithms were implemented and compared on our own platform in a real-world driving setting. The paper concludes with an overview of available datasets and tools for ADS development.},
archivePrefix = {arXiv},
arxivId = {1906.05113},
author = {Yurtsever, Ekim and Lambert, Jacob and Carballo, Alexander and Takeda, Kazuya},
doi = {10.1109/ACCESS.2020.2983149},
eprint = {1906.05113},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yurtsever et al. - 2020 - A Survey of Autonomous Driving Common Practices and Emerging Technologies.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Autonomous vehicles,automation,control,intelligent transportation systems,intelligent vehicles,robotics},
pages = {58443--58469},
title = {{A Survey of Autonomous Driving: Common Practices and Emerging Technologies}},
url = {https://arxiv.org/pdf/1906.05113.pdf},
volume = {8},
year = {2020}
}

@article{Rosolia2017,
abstract = {A Learning Model Predictive Controller (LMPC) for linear system is presented. The proposed controller builds on previous work on nonlinear LMPC and decreases its computational burden for linear system. The control scheme is reference-free and is able to improve its performance by learning from previous iterations. A convex safe set and a terminal cost function are used in order to guarantee recursive feasibility and non-increasing performance at each iteration. The paper presents the control design approach, and shows how to recursively construct the convex terminal set and the terminal cost from state and input trajectories of previous iterations. Simulation results show the effectiveness of the proposed control logic.},
archivePrefix = {arXiv},
arxivId = {1702.07064},
author = {Rosolia, Ugo and Borrelli, Francesco},
doi = {10.1016/j.ifacol.2017.08.324},
eprint = {1702.07064},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rosolia, Borrelli - 2017 - Learning Model Predictive Control for Iterative Tasks A Computationally Efficient Approach for Linear Syst(2).pdf:pdf},
issn = {24058963},
journal = {IFAC-PapersOnLine},
keywords = {Convex Optimization,LMPC,Learning,Model Predictive Control},
number = {1},
pages = {3142--3147},
publisher = {Elsevier B.V.},
title = {{Learning Model Predictive Control for Iterative Tasks: A Computationally Efficient Approach for Linear System}},
url = {https://doi.org/10.1016/j.ifacol.2017.08.324},
volume = {50},
year = {2017}
}
@article{Pagot2020,
abstract = {In this paper, we present a real-time non-linear model-predictive control (NMPC) framework to perform minimum-time motion planning for autonomous racing cars. We introduce an innovative kineto-dynamical vehicle model, able to accurately predict non-linear longitudinal and lateral vehicle dynamics. The main parameters of this vehicle model can be tuned with only experimental or simulated maneuvers, aimed to identify the handling diagram and the maximum performance G-G envelope. The kineto-dynamical model is adopted to generate on-line minimum time trajectories with an indirect optimal control method. The motion planning framework is applied to control an autonomous 1:8 RC vehicle near the limits of handling along a test circuit. Finally, the effectiveness of the proposed algorithms is illustrated by comparing the experimental results with the solution of an off-line minimum-time optimal control problem.},
author = {Pagot, Edoardo and Piccinini, Mattia and Biral, Francesco},
doi = {10.1109/IROS45743.2020.9340640},
file = {::},
isbn = {9781728162126},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {2390--2396},
title = {{Real-time optimal control of an autonomous RC car with minimum-time maneuvers and a novel kineto-dynamical model}},
year = {2020}
}
@article{Strobel2020,
abstract = {Autonomous racing provides the opportunity to test safety-critical perception pipelines at their limit. This paper describes the practical challenges and solutions to applying state-of-the-art computer vision algorithms to build a low-latency, high-accuracy perception system for DUT18 Driverless (DUT18D), a 4WD electric race car with podium finishes at all Formula Driverless competitions for which it raced. The key components of DUT18D include YOLOv3-based object detection, pose estimation, and time synchronization on its dual stereovision/monovision camera setup. We highlight modifications required to adapt perception CNNs to racing domains, improvements to loss functions used for pose estimation, and methodologies for sub-microsecond camera synchronization among other improvements. We perform a thorough experimental evaluation of the system, demonstrating its accuracy and low-latency in real-world racing scenarios.},
archivePrefix = {arXiv},
arxivId = {2007.13971},
author = {Strobel, Kieran and Zhu, Sibo and Chang, Raphael and Koppula, Skanda},
doi = {10.1109/IROS45743.2020.9341683},
eprint = {2007.13971},
file = {::},
isbn = {9781728162126},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {1969--1975},
title = {{Accurate, low-latency visual perception for autonomous racing: Challenges, mechanisms, and practical solutions}},
year = {2020}
}

@article{Brunner2018a,
abstract = {We propose an optimization based, data-driven framework to design controllers for repetitive tasks. The proposed framework builds on previous work of Learning Model Predictive Control and focuses on problems where the terminal condition of one iteration is the initial condition of the next iteration. A terminal cost and a sampled safe set are learned from data to guarantee recursive feasibility and non-decreasing performance cost at each iteration. The proposed control logic is tested on an autonomous racing example, where the vehicle dynamics are identified online. Experimental results on a 1:10 scale RC car illustrate the effectiveness of the proposed approach.},
annote = {No machine learning involved - rather, it has an optimisation variable
Last state of previous loop is first state of next loop
'learns' = improves trajectory using new x{\_}0},
author = {Brunner, Maximilian and Rosolia, Ugo and Gonzales, Jon and Borrelli, Francesco},
doi = {10.1109/CDC.2017.8264027},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Brunner et al. - 2018 - Repetitive learning model predictive control An autonomous racing example(2).pdf:pdf},
isbn = {9781509028733},
journal = {2017 IEEE 56th Annual Conference on Decision and Control, CDC 2017},
number = {Cdc},
pages = {2545--2550},
title = {{Repetitive learning model predictive control: An autonomous racing example}},
volume = {2018-Janua},
year = {2018}
}
@article{Guckiran2019,
abstract = {Self-Driving Cars are, currently a hot topic throughout the globe thanks to the advancements in Deep Learning techniques on computer vision problems. Since driving simulations are fairly important before real life autonomous implementations, there are multiple driving-racing simulations for testing purposes. The Open Racing Car Simulation (TORCS) is a highly portable open source car racing-self-driving-simulation. While it can be used as a game in which human players compete with scripted agents, TORCS provides observation and action API to develop an artificial intelligence agent. This study explores near-optimal Deep Reinforcement Learning agents for TORCS environment using Soft Actor-Critic and Rainbow DQN algorithms, exploration and generalization techniques.},
author = {Guckiran, Kivanc and Bolat, Bulent},
doi = {10.1109/ASYU48272.2019.8946332},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Guckiran, Bolat - 2019 - Autonomous Car Racing in Simulation Environment Using Deep Reinforcement Learning.pdf:pdf},
isbn = {9781728128689},
journal = {Proceedings - 2019 Innovations in Intelligent Systems and Applications Conference, ASYU 2019},
keywords = {Deep Reinforcement Learning,Self-Driving Car,TORCS},
title = {{Autonomous Car Racing in Simulation Environment Using Deep Reinforcement Learning}},
year = {2019}
}
@article{Busch2022,
abstract = {In head-to-head racing, performing tightly constrained, but highly rewarding maneuvers, such as overtaking, require an accurate model of interactive behavior of the opposing target vehicle (TV). However, such information is not typically made available in competitive scenarios, we therefore propose to construct a prediction and uncertainty model given data of the TV from previous races. In particular, a one-step Gaussian Process (GP) model is trained on closed-loop interaction data to learn the behavior of a TV driven by an unknown policy. Predictions of the nominal trajectory and associated uncertainty are rolled out via a sampling-based approach and are used in a model predictive control (MPC) policy for the ego vehicle in order to intelligently trade-off between safety and performance when attempting overtaking maneuvers against a TV. We demonstrate the GP-based predictor in closed loop with the MPC policy in simulation races and compare its performance against several predictors from literature. In a Monte Carlo study, we observe that the GP-based predictor achieves similar win rates while maintaining safety in up to 3x more races. We finally demonstrate the prediction and control framework in real-time on hardware experiments.},
archivePrefix = {arXiv},
arxivId = {2204.12533},
author = {Busch, Finn Lukas and Johnson, Jake and Zhu, Edward L. and Borrelli, Francesco},
eprint = {2204.12533},
file = {:C$\backslash$:/Users/Andrew/Downloads/OCAR-ICRA2022{\_}paper{\_}7.pdf:pdf},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
number = {1931853},
title = {{A Gaussian Process Model for Opponent Prediction in Autonomous Racing}},
url = {http://arxiv.org/abs/2204.12533},
year = {2022}
}

@article{Dhall2019,
abstract = {Considerable progress has been made in semantic scene understanding of road scenes with monocular cameras. It is, however, mainly focused on certain specific classes such as cars, bicyclists and pedestrians. This work investigates traffic cones, an object category crucial for traffic control in the context of autonomous vehicles. 3D object detection using images from a monocular camera is intrinsically an ill-posed problem. In this work, we exploit the unique structure of traffic cones and propose a pipelined approach to solve this problem. Specifically, we first detect cones in images by a modified 2D object detector. Following which the keypoints on a traffic cone are recognized with the help of our deep structural regression network, here, the fact that the cross-ratio is projection invariant is leveraged for network regularization. Finally, the 3D position of cones is recovered via the classical Perspective n-Point algorithm using correspondences obtained from the keypoint regression. Extensive experiments show that our approach can accurately detect traffic cones and estimate their position in the 3D world in real time. The proposed method is also deployed on a real-time, autonomous system. It runs efficiently on the low-power Jetson TX2, providing accurate 3D position estimates, allowing a race-car to map and drive autonomously on an unseen track indicated by traffic cones. With the help of robust and accurate perception, our race-car won both Formula Student Competitions held in Italy and Germany in 2018, cruising at a top speed of 54 km/h on our driverless platform 'gotthard driverless' https://youtu. be/HegmIXASKow?t==11694. Visualization of the complete pipeline, mapping and navigation can be found on our project page http://people.ee.ethz.ch/=tracczucrich/Trnfflcf.lonc/.},
archivePrefix = {arXiv},
arxivId = {1902.02394},
author = {Dhall, Ankit and Dai, Dengxin and {Van Gool}, Luc},
doi = {10.1109/IVS.2019.8814089},
eprint = {1902.02394},
file = {::},
isbn = {9781728105604},
journal = {IEEE Intelligent Vehicles Symposium, Proceedings},
number = {Iv},
pages = {494--501},
publisher = {IEEE},
title = {{Real-time 3D traffic cone detection for autonomous driving}},
volume = {2019-June},
year = {2019}
}

@article{Goh2016,
abstract = {Professional drivers in 'drifting' competitions are able to precisely negotiate a specified course at high sideslip angles while operating in an unstable region of state-space. Studying this practice could provide insight into autonomous car control during emergency maneuvers that excurse outside stable handling limits. This paper presents a simple and physically insightful controller for autonomous drifting with simultaneous tracking of a reference path. A feasible reference trajectory is treated as a sequence of unstable drifting equilibrium points, and a basic example is generated from vehicle parameters using a four-wheel model with steady-state weight transfer. Lookahead error and sideslip are chosen as reference states, and a controller for tracking both objectives around an equilibrium point is derived using a simpler single-track model. Experiments on the rear-wheel drive MARTY test vehicle demonstrate good tracking performance of both objectives even at values of sideslip as high as 45 degrees.},
author = {Goh, Jonathan Y. and Gerdes, J. Christian},
doi = {10.1109/IVS.2016.7535448},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goh, Gerdes - 2016 - Simultaneous stabilization and tracking of basic automobile drifting trajectories.pdf:pdf},
isbn = {9781509018215},
journal = {IEEE Intelligent Vehicles Symposium, Proceedings},
number = {Iv},
pages = {597--602},
publisher = {IEEE},
title = {{Simultaneous stabilization and tracking of basic automobile drifting trajectories}},
volume = {2016-Augus},
year = {2016}
}
@article{Lutjens2019,
abstract = {Many current autonomous systems are being designed with a strong reliance on black box predictions from deep neural networks (DNNs). However, DNNs tend to be overconfident in predictions on unseen data and can give unpredictable results for far-from-distribution test data. The importance of predictions that are robust to this distributional shift is evident for safety-critical applications, such as collision avoidance around pedestrians. Measures of model uncertainty can be used to identify unseen data, but the state-of-the-art extraction methods such as Bayesian neural networks are mostly intractable to compute. This paper uses MC-Dropout and Bootstrapping to give computationally tractable and parallelizable uncertainty estimates. The methods are embedded in a Safe Reinforcement Learning framework to form uncertainty-aware navigation around pedestrians. The result is a collision avoidance policy that knows what it does not know and cautiously avoids pedestrians that exhibit unseen behavior. The policy is demonstrated in simulation to be more robust to novel observations and take safer actions than an uncertainty-unaware baseline.},
archivePrefix = {arXiv},

author = {L{\"{u}}tjens, Bj{\"{o}}rn and Everett, Michael and How, Jonathan P.},
doi = {10.1109/ICRA.2019.8793611},

isbn = {9781538660263},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {8662--8668},
title = {{Safe reinforcement learning with model uncertainty estimates}},
volume = {2019-May},
year = {2019}
}
@article{Althoff2017,
abstract = {Numerical experiments for motion planning of road vehicles require numerous components: vehicle dynamics, a road network, static obstacles, dynamic obstacles and their movement over time, goal regions, a cost function, etc. Providing a description of the numerical experiment precise enough to reproduce it might require several pages of information. Thus, only key aspects are typically described in scientific publications, making it impossible to reproduce results-yet, re-producibility is an important asset of good science. Composable benchmarks for motion planning on roads (CommonRoad) are proposed so that numerical experiments are fully defined by a unique ID; all information required to reconstruct the experiment can be found on the CommonRoad website. Each benchmark is composed by a vehicle model, a cost function, and a scenario (including goals and constraints). The scenarios are partly recorded from real traffic and partly hand-crafted to create dangerous situations. We hope that CommonRoad saves researchers time since one does not have to search for realistic parameters of vehicle dynamics or realistic traffic situations, yet provides the freedom to compose a benchmark that fits ones needs.},
author = {Althoff, Matthias and Koschi, Markus and Manzinger, Stefanie},
doi = {10.1109/IVS.2017.7995802},
journal = {IEEE Intelligent Vehicles Symposium, Proceedings},
number = {Iv},
pages = {719--726},
publisher = {IEEE},
title = {{CommonRoad: Composable benchmarks for motion planning on roads}},
year = {2017}
}
@article{Lyu2022a,
abstract = {With the increasing emphasis on the safe autonomy for robots, model-based safe control approaches such as Control Barrier Functions have been extensively studied to ensure guaranteed safety during inter-robot interactions. In this paper, we introduce the Parametric Control Barrier Function (Parametric-CBF), a novel variant of the traditional Control Barrier Function to extend its expressivity in describing different safe behaviors among heterogeneous robots. Instead of assuming cooperative and homogeneous robots using the same safe controllers, the ego robot is able to model the neighboring robots' underlying safe controllers through different Parametric-CBFs with observed data. Given learned parametric-CBF and proved forward invariance, it provides greater flexibility for the ego robot to better coordinate with other heterogeneous robots with improved efficiency while enjoying formally provable safety guarantees. We demonstrate the usage of Parametric-CBF in behavior prediction and adaptive safe control in the ramp merging scenario from the applications of autonomous driving. Compared to traditional CBF, Parametric-CBF has the advantage of capturing varying drivers' characteristics given richer description of robot behavior in the context of safe control. Numerical simulations are given to validate the effectiveness of the proposed method.},
archivePrefix = {arXiv},
arxivId = {2202.09936},
author = {Lyu, Yiwei and Luo, Wenhao and Dolan, John M.},
eprint = {2202.09936},
file = {:C$\backslash$:/Users/Andrew/Downloads/2202.09936.pdf:pdf},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
title = {{Adaptive Safe Merging Control for Heterogeneous Autonomous Vehicles using Parametric Control Barrier Functions}},
url = {http://arxiv.org/abs/2202.09936},
year = {2022}
}
@article{Drews2019,
abstract = {In this letter, we present a framework for combining deep learning-based road detection, particle filters, and model predictive control (MPC) to drive aggressively using only a monocular camera, IMU, and wheel speed sensors. This framework uses deep convolutional neural networks combined with LSTMs to learn a local cost map representation of the track in front of the vehicle. A particle filter uses this dynamic observation model to localize in a schematic map, and MPC is used to drive aggressively using this particle filter based state estimate. We show extensive real world testing results and demonstrate reliable operation of the vehicle at the friction limits on a complex dirt track. We reach speeds above 27 m/h (12 m/s) on a dirt track with a 105 ft (32 m) long straight using our 1:5 scale test vehicle.},
archivePrefix = {arXiv},
arxivId = {1812.02071},
author = {Drews, Paul and Williams, Grady and Goldfain, Brian and Theodorou, Evangelos A. and Rehg, James M.},
doi = {10.1109/LRA.2019.2896449},
journal = {IEEE Robotics and Automation Letters},
keywords = {Deep learning in robotics and automation,autonomous vehicle navigation,computer vision for transportation,localization},
number = {2},
pages = {1564--1571},
title = {{Vision-based high-speed driving with a deep dynamic observer}},
volume = {4},
year = {2019}
}



@article{Herrmann2021,
abstract = {With the evolution of self-driving cars, autonomous racing series like Roborace and the Indy Autonomous Challenge are rapidly attracting growing attention. Researchers participating in these competitions hope to subsequently transfer their developed functionality to passenger vehicles, in order to improve self-driving technology for reasons of safety, and due to environmental and social benefits. The race track has the advantage of being a safe environment where challenging situations for the algorithms are permanently created. To achieve minimum lap times on the race track, it is important to gather and process information about external influences including, e.g., the position of other cars and the friction potential between the road and the tires. Furthermore, the predicted behavior of the ego-car's propulsion system is crucial for leveraging the available energy as efficiently as possible. In this paper, we therefore present an optimization-based velocity planner, mathematically formulated as a multi-parametric Sequential Quadratic Problem (mpSQP). This planner can handle a spatially and temporally varying friction coefficient, and transfer a race Energy Strategy (ES) to the road. It further handles the velocity-profile-generation task for performance and emergency trajectories in real time on the vehicle's Electronic Control Unit (ECU).},
archivePrefix = {arXiv},
arxivId = {2012.13586},
author = {Herrmann, Thomas and Wischnewski, Alexander and Hermansdorfer, Leonhard and Betz, Johannes and Lienkamp, Markus},
doi = {10.1109/TIV.2020.3047858},
eprint = {2012.13586},
file = {::},
issn = {23798858},
journal = {IEEE Transactions on Intelligent Vehicles},
keywords = {Autonomous electric vehicles,energy strategy,optimal control,real-time numerical optimization,trajectory planning,variable friction potential,velocity planning},
number = {4},
pages = {665--677},
publisher = {IEEE},
title = {{Real-Time Adaptive Velocity Optimization for Autonomous Electric Cars at the Limits of Handling}},
volume = {6},
year = {2021}
}

@misc{Wadekar2021,
  doi = {10.48550/ARXIV.2105.01799},
  url = {https://arxiv.org/abs/2105.01799},
  author = {Wadekar, Shakti N. and Schwartz, Benjamin J. and Kannan, Shyam S. and Mar, Manuel and Manna, Rohan Kumar and Chellapandi, Vishnu and Gonzalez, Daniel J. and Gamal, Aly El},
  keywords = {Robotics (cs.RO), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Towards End-to-End Deep Learning for Autonomous Racing: On Data Collection and a Unified Architecture for Steering and Throttle Prediction},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Cardamone2010,
abstract = {Finding the racing line to follow on the track is at the root of the development of any controller in racing games. In commercial games this issue is usually addressed by using human-designed racing lines provided by domain experts and represents a rather time consuming process. In this paper we introduce a novel approach to compute the racing line without any human intervention. In the proposed approach, the track is decomposed into several segments where a genetic algorithm is applied to search for the best trade-off between the minimization of two conflicting objectives: the length and the curvature of the racing line. The fitness of the candidate solutions is computed through a simulation performed with The Open Racing Car Simulator (TORCS), an open source simulator used as testbed in this work. Finally, to test our approach we carried out an experimental analysis that involved 11 tracks provided with the TORCS distribution. In addition, we compared the performance of our approach to the one achieved by a related approach, previously introduced in the literature, and to the performance of the fastest controller available for TORCS. Our results are very promising and show that the presented approach is able to reach the best performance in almost all the tracks considered. {\textcopyright} 2010 IEEE.},
author = {Cardamone, Luigi and Loiacono, Daniele and Lanzi, Pier Luca and Bardelli, Alessandro Pietro},
doi = {10.1109/ITW.2010.5593330},
file = {::},
isbn = {9781424462971},
journal = {Proceedings of the 2010 IEEE Conference on Computational Intelligence and Games, CIG2010},
pages = {388--394},
publisher = {IEEE},
title = {{Searching for the optimal racing line using genetic algorithms}},
year = {2010}
}
@article{Jain2020,
abstract = {Learning to race autonomously is a challenging problem. It requires perception, estimation, planning, and control to work together in synchronization while driving at the limit of a vehicle's handling capability. Among others, one of the fundamental challenges lies in predicting the vehicle's future states like position, orientation, and speed with high accuracy because it is inevitably hard to identify vehicle model parameters that capture its real nonlinear dynamics in the presence of lateral tire slip. We present a model-based planning and control framework for autonomous racing that significantly reduces the effort required in system identification. Our approach bridges the gap between the design in a simulation and the real world by learning from on-board sensor measurements. Thus, the teams participating in autonomous racing competitions can start racing on new tracks without having to worry about tuning the vehicle model.},
archivePrefix = {arXiv},
arxivId = {2005.04755},
author = {Jain, Achin and Chaudhari, Pratik and Morari, Manfred},
eprint = {2005.04755},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jain, Chaudhari, Morari - 2020 - BAYESRACE Learning to race autonomously using prior experience.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {Autonomous racing,Gaussian processes,Model predictive control,Sim-to-real,System identification},
number = {CoRL},
title = {{BAYESRACE: Learning to race autonomously using prior experience}},
year = {2020}
}
@article{Brunnbauer2021a,
abstract = {World models learn behaviors in a latent imagination space to enhance the sample-efficiency of deep reinforcement learning (RL) algorithms. While learning world models for high-dimensional observations (e.g., pixel inputs) has become practicable on standard RL benchmarks and some games, their effectiveness in real-world robotics applications has not been explored. In this paper, we investigate how such agents generalize to real-world autonomous vehicle control tasks, where advanced model-free deep RL algorithms fail. In particular, we set up a series of time-lap tasks for an F1TENTH racing robot, equipped with a high-dimensional LiDAR sensor, on a set of test tracks with a gradual increase in their complexity. In this continuous-control setting, we show that model-based agents capable of learning in imagination substantially outperform model-free agents with respect to performance, sample efficiency, successful task completion, and generalization. Moreover, we show that the generalization ability of model-based agents strongly depends on the choice of their observation model. We provide extensive empirical evidence for the effectiveness of world models provided with long enough memory horizons in sim2real tasks.},
archivePrefix = {arXiv},
arxivId = {2103.04909},
author = {Brunnbauer, Axel and Berducci, Luigi and Brandst{\"{a}}tter, Andreas and Lechner, Mathias and Hasani, Ramin and Rus, Daniela and Grosu, Radu},
eprint = {2103.04909},
title = {{Latent Imagination Facilitates Zero-Shot Transfer in Autonomous Racing}},
url = {http://arxiv.org/abs/2103.04909},
year = {2021}
}
@article{Riedmiller2007,
abstract = {The paper describes our first experiments on Reinforcement Learning to steer a real robot car. The applied method, Neural Fitted Q Iteration (NFQ) is purely data-driven based on data directly collected from real-life experiments, i.e. no transition model and no simulation is used. The RL approach is based on learning a neural Q value function, which means that no prior selection of the structure of the control law is required. We demonstrate, that the controller is able to learn a steering task in less than 20 minutes directly on the real car. We consider this as an important step towards the competitive application of neural Q function based RL methods in real-life environments. {\textcopyright} 2007 IEEE.},
author = {Riedmiller, Martin and Montemerlo, Mike and Dahlkamp, Hendrik},
doi = {10.1109/FBIT.2007.37},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Riedmiller, Montemerlo, Dahlkamp - 2007 - Learning to drive a real car in 20 minutes.pdf:pdf},
isbn = {0769529992},
journal = {Proceedings of the Frontiers in the Convergence of Bioscience and Information Technologies, FBIT 2007},
pages = {645--650},
title = {{Learning to drive a real car in 20 minutes}},
year = {2007}
}

@misc{DalBianco2019,
abstract = {Minimum lap time simulations are especially important in the design, optimisation and setup of race vehicles. Such problems usually come in different flavours, e.g. quasi-steady state models vs. full dynamic models and pre-defined (fixed) trajectory problems vs. free trajectory problems. This work is focused on full dynamic models with free trajectory. Practical solution techniques include direct methods (i.e. solution of an nonlinear programming problem problem, widespread approach) and indirect method (i.e. based on Pontryagin's principle, less common, yet quite efficient in some cases). In this contribution the performance of the direct and indirect methods are compared in a number of vehicle-related problems.},
author = {{Dal Bianco}, Nicola and Bertolazzi, Enrico and Biral, Francesco and Massaro, Matteo},
booktitle = {Vehicle System Dynamics},
doi = {10.1080/00423114.2018.1480048},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dal Bianco et al. - 2019 - Comparison of direct and indirect methods for minimum lap time optimal control problems.pdf:pdf},
issn = {17445159},
keywords = {NLP,OCP,Optimal control,car simulation,direct methods,indirect methods,lap-time},
number = {5},
pages = {665--696},
title = {{Comparison of direct and indirect methods for minimum lap time optimal control problems}},
volume = {57},
year = {2019}
}

@article{Cai2021,
abstract = {Autonomous car racing is a challenging task in the robotic control area. Traditional modular methods require accurate mapping, localization and planning, which makes them computationally inefficient and sensitive to environmental changes. Recently, deep-learning-based end-to-end systems have shown promising results for autonomous driving/racing. However, they are commonly implemented by supervised imitation learning (IL), which suffers from the distribution mismatch problem, or by reinforcement learning (RL), which requires a huge amount of risky interaction data. In this work, we present a general deep imitative reinforcement learning approach (DIRL), which successfully achieves agile autonomous racing using visual inputs. The driving knowledge is acquired from both IL and model-based RL, where the agent can learn from human teachers as well as perform self-improvement by safely interacting with an offline world model. We validate our algorithm both in a high-fidelity driving simulation and on a real-world 1/20-scale RC-car with limited onboard computation. The evaluation results demonstrate that our method outperforms previous IL and RL methods in terms of sample efficiency and task performance. Demonstration videos are available at https://caipeide.github.io/autorace-dirl/.},
archivePrefix = {arXiv},
arxivId = {2107.08325},
author = {Cai, Peide and Wang, Hengli and Huang, Huaiyang and Liu, Yuxuan and Liu, Ming},
doi = {10.1109/LRA.2021.3097345},
journal = {IEEE Robotics and Automation Letters},
keywords = {Reinforcement learning,autonomous racing,imitation learning,model learning for control,uncertainty awareness},
number = {4},
pages = {7262--7269},
title = {{Vision-Based Autonomous Car Racing Using Deep Imitative Reinforcement Learning}},
url = {https://www.researchgate.net/publication/353566937{\_}Vision-Based{\_}Autonomous{\_}Car{\_}Racing{\_}Using{\_}Deep{\_}Imitative{\_}Reinforcement{\_}Learning},
volume = {6},
year = {2021}
}
@article{Valls2018,
archivePrefix = {arXiv},
arxivId = {1804.03252},
author = {Valls, Miguel I. and Hendrikx, Hubertus F.C. and Reijgwart, Victor J.F. and Meier, Fabio V. and Sa, Inkyu and Dube, Renaud and Gawel, Abel and Burki, Mathias and Siegwart, Roland},
doi = {10.1109/ICRA.2018.8462829},
eprint = {1804.03252},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Valls et al. - 2018 - Design of an Autonomous Racecar Perception, State Estimation and System Integration.pdf:pdf},
isbn = {9781538630815},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {2048--2055},
title = {{Design of an Autonomous Racecar: Perception, State Estimation and System Integration}},
year = {2018},
url = {https://doi.org/10.1109/ICRA.2018.8462829},
}

@article{Weiss2020,
abstract = {We consider the challenging problem of high speed autonomous racing in realistic dynamic environments. DeepRacing is a novel end-to-end framework, and a virtual testbed for training and evaluating algorithms for autonomous racing. The virtual testbed is implemented using the realistic Formula One (F1) Codemasters game, which is used by many F1 drivers for training. We present AdmiralNet - a Convolution Neural Network (CNN) integrated with Long Short-Term Memory (LSTM) cells that can be tuned for the autonomous racing task in the highly realistic F1 game. We evaluate AdmiralNet's performance on unseen race tracks, and also evaluate the degree of transference between the simulation and the real world by implementing end-to-end racing on a physical 1/10 scale autonomous racecar.},
author = {Weiss, Trent and Behl, Madhur},
doi = {10.23919/DATE48585.2020.9116486},
url = {https://doi.org/10.23919/DATE48585.2020.9116486},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Weiss, Behl - 2020 - DeepRacing A Framework for Autonomous Racing.pdf:pdf},
journal = {Proceedings of the 2020 Design, Automation and Test in Europe Conference and Exhibition},
pages = {1163--1168},
title = {DeepRacing: A Framework for Autonomous Racing},
year = {2020}
}

@article{Liniger2015a,
abstract = {We consider a viability based approach to guarantee recursive feasibility of a finite horizon path planner. The path planner is formulated as a hybrid system for which a difference inclusion reformulation is derived by exploiting the special structure of the problem. Based on this approximation, the viability kernel, which characterizes all safe states and the corresponding safe controls, can be calculated. Using the set of safe controls the computation time of the online path planning can be reduced, by only generating viable trajectories. Finally, a condition characterizing the unsafe set in case of online obstacle avoidance is derived.},
author = {Liniger, Alexander and Lygeros, John},
doi = {10.1145/2728606.2728620},
url = {https://doi.org/10.1145/2728606.2728620},
isbn = {9781450334334},
journal = {Proceedings of the 18th International Conference on Hybrid Systems: Computation and Control, HSCC 2015},
keywords = {Autonomous Driving,Hybrid Systems,Path Planning,Viability Theory},
pages = {1--10},
title = {{A viability approach for fast recursive feasible finite horizon path planning of autonomous RC cars}},
year = {2015}
}

@inproceedings{Ghignone2022,
abstract = {Autonomous racing is becoming popular for academic and industry researchers as a test for general autonomous driving by pushing perception, planning, and control algorithms to their limits. While traditional control methods such as MPC are capable of generating an optimal control sequence at the edge of the vehicles physical controllability, these methods are sensitive to the accuracy of the modeling parameters. This paper presents TC-Driver, a RL approach for robust control in autonomous racing. In particular, the TC-Driver agent is conditioned by a trajectory generated by any arbitrary traditional high-level planner. The proposed TC-Driver addresses the tire parameter modeling inaccuracies by exploiting the heuristic nature of RL while leveraging the reliability of traditional planning methods in a hierarchical control structure. We train the agent under varying tire conditions, allowing it to generalize to different model parameters, aiming to increase the racing capabilities of the system in practice. The proposed RL method outperforms a non-learning-based MPC with a 2.7 lower crash ratio in a model mismatch setting, underlining robustness to parameter discrepancies. In addition, the average RL inference duration is 0.25 ms compared to the average MPC solving time of 11.5 ms, yielding a nearly 40-fold speedup, allowing for complex control deployment in computationally constrained devices. Lastly, we show that the frequently utilized end-to-end RL architecture, as a control policy directly learned from sensory input, is not well suited to model mismatch robustness nor track generalization. Our realistic simulations show that TC-Driver achieves a 6.7 and 3-fold lower crash ratio under model mismatch and track generalization settings, while simultaneously achieving lower lap times than an end-to-end approach, demonstrating the viability of TC-driver to robust autonomous racing.},
annote = {RL has 2.7 lower crash ratio than MPC in model mismatch setting - robust to model discreptancies.
0.25ms compute time for RL vs 11.5ms compute time for MPC - 40 times faster.

End to end architecture is not suitable for model mismatch.

6.7 times fewer crashes than end to end under track generalisation settings
3 times fewer crashes than end to end under model mismatch settings

Advantages of autonomous vehicles:
Enhance road safety
Reduce carbon emmisions
Reduce driving related stress

Focus on: 
Difficult to model lateral tyre forces

Progress along centerline is Frenet frame!

Vehicle dynamics model of f1tenth: 
https://ieeexplore.ieee.org/document/7995802, reference 20},
archivePrefix = {arXiv},
arxivId = {2205.09370},
author = {Ghignone, Edoardo and Baumann, Nicolas and Boss, Mike and Magno, Michele},
booktitle = {International conference on robotics and automation},
eprint = {2205.09370},
file = {:C$\backslash$:/Users/Andrew/Downloads/2205.09370.pdf:pdf},
title = {{TC-Driver: Trajectory Conditioned Driving for Robust Autonomous Racing - A Reinforcement Learning Approach}},
url = {http://arxiv.org/abs/2205.09370},
year = {2022}
}

@article{Weiss2020b,
abstract = {Demonstrating high-speed autonomous racing can be considered as a grand challenge for vision based end-to-end deep learning models. DeepRacing AI is a novel end-to-end framework for trajectory synthesis for autonomous racing. We train and demonstrate the effectiveness of our approach using a high fidelity and photo-realistic Formula One gaming environment-used by real racing drivers. This is the first work that has used the highly realistic F1 game as a simulation environment for deep learning models. We present a novel method for single agent autonomous racing by training a deep neural network to predict a parame-terized representation of a trajectory. Our Bezier curve based trajectory synthesis approach outperforms several other end-to-end DNN approaches for autonomous racing. In addition to evaluating our methodology in a closed-loop manner in the game; we also implement the DeepRacing algorithm on a 1/10 scale autonomous racing test-bed and show its ability to handle real-world data at high speeds.},
author = {Weiss, Trent and {Suresh Babu}, Varundev and Behl, Madhur},
file = {:C$\backslash$:/Users/Andrew/Downloads/B{\'{e}}zier Curve Based End-to-End Trajectory Synthesis for Agile Autonomous Driving.pdf:pdf},
journal = {NeurIPS 2020 Machine Learning for Autonomous Driving Workshop (NIPS)},
number = {NeurIPS},
pages = {1--10},
title = {{B` ezier Curve Based End-to-End Trajectory Synthesis for Agile Autonomous Driving}},
year = {2020}
}
@article{vesel,
author = {Vesel, Ricky},
doi = {10.1145/2815474.2815476},
file = {::},
journal = {ACM SIGEVOlution},
number = {44},
pages = {12--20},
title = {{Racing Line Optimization}},
url = {http://www.raceoptimal.com/},
volume = {7},
year = {2015}
}
@article{Vazquez2020,
abstract = {In this paper we propose a hierarchical controller for autonomous racing where the same vehicle model is used in a two level optimization framework for motion planning. The high-level controller computes a trajectory that minimizes the lap time, and the low-level nonlinear model predictive path following controller tracks the computed trajectory online. Following a computed optimal trajectory avoids online planning and enables fast computational times. The efficiency is further enhanced by the coupling of the two levels through a terminal constraint, computed in the high-level controller. Including this constraint in the real-time optimization level ensures that the prediction horizon can be shortened, while safety is guaranteed. This proves crucial for the experimental validation of the approach on a full size driverless race car. The vehicle in question won two international student racing competitions using the proposed framework; moreover, our hierarchical controller achieved an improvement of 20{\%} in the lap time compared to the state of the art result achieved using a very similar car and track.},
archivePrefix = {arXiv},
arxivId = {2003.04882},
author = {Vazquez, Jose L. and Bruhlmeier, Marius and Liniger, Alexander and Rupenyan, Alisa and Lygeros, John},
doi = {10.1109/IROS45743.2020.9341731},
url = {https://doi.org/10.1109/IROS45743.2020.9341731},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vazquez et al. - 2020 - Optimization-based hierarchical motion planning for autonomous racing.pdf:pdf},
isbn = {9781728162126},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {2397--2403},
title = {Optimization-based hierarchical motion planning for autonomous racing},
year = {2020}
}
@book{Liniger2018,
abstract = {more conservative approximation using the discriminating kernel results in a safer driving style which starts braking earlier into curves, but this changed driving style comes at the cost of a small increase in lap time. The proposed hierarchical racing controller with viability constraints is able to tackle the first two challenges in autonomous racing. For the third task where racing against other cars is of interest, we propose an approach to formulate racing decisions as a non-cooperative non-zero-sum game. We design three different games where the players aim to fulfill static track constraints as well as avoid collision with each other; the latter constraint depends on the combined actions of the two players. The difference between the games are the collision constraints and the payoff. In the first game collision avoidance is only considered by the follower, and each player maximizes their own progress towards the finish line. We show that, thanks to the sequential structure of this game, equilibria can be computed through an efficient sequential maximization approach. Further, we show that these actions, if feasible, are also a Stackelberg and Nash equilibrium in pure strategies of our second game where both players consider the collision constraints. The payoff of our third game is designed to promote blocking, by additionally rewarding the cars for staying ahead at the end of the horizon. We show that this changes the Stackelberg equilibrium, but has a minor influence on the Nash equilibria. For online implementation, we propose to play the games in a moving horizon fashion, and discuss two methods for guaranteeing feasibility of the resulting coupled repeated games. Finally, we study the performance of the proposed approaches in simulation for the miniature race car set-up. The simulation study shows that the presented games can successfully model different racing behaviors and generate interesting racing situations.},
author = {Liniger, Alexander},
file = {::},
isbn = {9783906916354},
keywords = {autonomous driving,autonomous racing,decision making,game theory,predictive control,viability theory},
number = {25139},
pages = {122},
title = {{Path Planning and Control for Autonomous Racing}},
year = {2018}
}
@article{Fork2022,
abstract = {We leverage game theory and a new vehicle modeling approach to compute overtaking maneuvers for racecars on a nonplanar surface. We solve for equilibria between noncooperative racing agents and demonstrate that by leveraging the novel nonplanar vehicle dynamics, overtaking can be achieved in situations where simpler models can do not provide a winning strategy.},
archivePrefix = {arXiv},
arxivId = {2204.10446},
author = {Fork, Thomas and Tseng, H. Eric and Borrelli, Francesco},
eprint = {2204.10446},
file = {:C$\backslash$:/Users/Andrew/Downloads/2204.10446.pdf:pdf},
pages = {2--6},
title = {{Overtaking Maneuvers on a Nonplanar Racetrack}},
url = {http://arxiv.org/abs/2204.10446},
year = {2022}
}

@article{Althoff2020,
abstract = {This document presents models in CommonRoad for vehicle dynamics ranging from sim- ple to complicated: The simplest model is a point-mass model, while the most complicated one is a multi-body model. All models are presented in state-space form to facilitate their implementation in standard solvers for ordinary differential equations. We further provide parameter sets and a precise initialization of the multi-body model. To be able to compare the results with simpler models, it is presented how the initial states and the parameters of the multi-body model can be transfered to simpler models. Implementation examples in MATLAB and Python are provided on the CommonRoad website. Our repository also provide routines to convert initial states and parameters. Simulation examples demonstrate the advantages of more complicated models.},
author = {Althoff, Matthias and W{\"{u}}rsching, Gerald},
journal = {Technische Universit{\"{a}}t M{\"{u}}nchen},
title = {{CommonRoad: Vehicle Models}},
url = {https://gitlab.lrz.de/tum-cps/commonroad-vehicle-models},
year = {2020}
}

@article{Guckiran2019a,
abstract = {Self-Driving Cars are, currently a hot topic throughout the globe thanks to the advancements in Deep Learning techniques on computer vision problems. Since driving simulations are fairly important before real life autonomous implementations, there are multiple driving-racing simulations for testing purposes. The Open Racing Car Simulation (TORCS) is a highly portable open source car racing-self-driving-simulation. While it can be used as a game in which human players compete with scripted agents, TORCS provides observation and action API to develop an artificial intelligence agent. This study explores near-optimal Deep Reinforcement Learning agents for TORCS environment using Soft Actor-Critic and Rainbow DQN algorithms, exploration and generalization techniques.},
author = {Guckiran, Kivanc and Bolat, Bulent},
doi = {10.1109/ASYU48272.2019.8946332},
file = {::},
isbn = {9781728128689},
journal = {Proceedings - 2019 Innovations in Intelligent Systems and Applications Conference, ASYU 2019},
keywords = {Deep Reinforcement Learning,Self-Driving Car,TORCS},
pages = {0--5},
title = {{Autonomous Car Racing in Simulation Environment Using Deep Reinforcement Learning}},
url = {https://ieeexplore.ieee.org/document/8946332},
year = {2019}
}
@article{Kahn2017,
abstract = {Reinforcement learning can enable complex, adaptive behavior to be learned automatically for autonomous robotic platforms. However, practical deployment of reinforcement learning methods must contend with the fact that the training process itself can be unsafe for the robot. In this paper, we consider the specific case of a mobile robot learning to navigate an a priori unknown environment while avoiding collisions. In order to learn collision avoidance, the robot must experience collisions at training time. However, high-speed collisions, even at training time, could damage the robot. A successful learning method must therefore proceed cautiously, experiencing only low-speed collisions until it gains confidence. To this end, we present an uncertainty-aware model-based learning algorithm that estimates the probability of collision together with a statistical estimate of uncertainty. By formulating an uncertainty-dependent cost function, we show that the algorithm naturally chooses to proceed cautiously in unfamiliar environments, and increases the velocity of the robot in settings where it has high confidence. Our predictive model is based on bootstrapped neural networks using dropout, allowing it to process raw sensory inputs from high-bandwidth sensors such as cameras. Our experimental evaluation demonstrates that our method effectively minimizes dangerous collisions at training time in an obstacle avoidance task for a simulated and real-world quadrotor, and a real-world RC car. Videos of the experiments can be found at https://sites.google.com/site/probcoll.},
archivePrefix = {arXiv},
arxivId = {1702.01182},
author = {Kahn, Gregory and Villaflor, Adam and Pong, Vitchyr and Abbeel, Pieter and Levine, Sergey},
eprint = {1702.01182},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kahn et al. - 2017 - Uncertainty-Aware Reinforcement Learning for Collision Avoidance(2).pdf:pdf},
title = {{Uncertainty-Aware Reinforcement Learning for Collision Avoidance}},
url = {http://arxiv.org/abs/1702.01182},
year = {2017}
}
@article{Herrmann2019,
abstract = {The automation of passenger vehicles is becoming more and more widespread, leading to full autonomy of cars within the next years. Furthermore, sustainable electric mobility is gaining in importance. As racecars have been a development platform for technology that has later also been transferred to passenger vehicles, a race format for autonomous electric racecars called Roborace has been created.As electric racecars only store a limited amount of energy, an Energy Management Strategy (EMS) is needed to work out the time as well as the minimum energy trajectories for the track. At the same time, the technical limitations and component behavior in the electric powertrain must be taken into account when calculating the race trajectories. In this paper, we present a concept for a special type of EMS. This is based on the Optimal Control Problem (OCP) of generating a time-minimal global trajectory which is solved by the transcription via direct orthogonal collocation to a Nonlinear Programming Problem (NLPP). We extend this minimum lap time problem by adding our ideas for a holistic EMS. This approach proves the fundamental feasibility of the stated ideas, e.g. varying racepaths and velocities due to energy limitations, covered by the EMS. Also, the presented concept forms the basis for future work on meta-models of the powertrain's components that can be fed into the OCP to increase the validity of the control output of the EMS.},
author = {Herrmann, Thomas and Christ, Fabian and Betz, Johannes and Lienkamp, Markus},
doi = {10.1109/ITSC.2019.8917154},
file = {::},
isbn = {9781538670248},
journal = {2019 IEEE Intelligent Transportation Systems Conference, ITSC 2019},
pages = {720--725},
publisher = {IEEE},
title = {{Energy Management Strategy for an Autonomous Electric Racecar using Optimal Control}},
year = {2019}
}

@article{Herrmann2020,
abstract = {Increasing attention to autonomous passenger vehicles has also attracted interest in an autonomous racing series. Because of this, platforms such as Roborace and the Indy Autonomous Challenge are currently evolving.Electric racecars face the challenge of a limited amount of stored energy within their batteries. Furthermore, the thermodynamical influence of an all-electric powertrain on the race performance is crucial. Severe damage can occur to the powertrain components when thermally overstressed. In this work we present a race-time minimal control strategy deduced from an Optimal Control Problem (OCP) that is transcribed into a Nonlinear Problem (NLP). Its optimization variables stem from the driving dynamics as well as from a thermodynamical description of the electric powertrain. We deduce the necessary first-order Ordinary Differential Equations (ODE)s and form simplified loss models for the implementation within the numerical optimization. The significant influence of the powertrain behavior on the race strategy is shown.},
archivePrefix = {arXiv},
arxivId = {2005.07127},
author = {Herrmann, Thomas and Passigato, Francesco and Betz, Johannes and Lienkamp, Markus},
doi = {10.1109/ITSC45102.2020.9294681},
eprint = {2005.07127},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Herrmann et al. - 2020 - Minimum Race-Time Planning-Strategy for an Autonomous Electric Racecar.pdf:pdf},
isbn = {9781728141497},
journal = {2020 IEEE 23rd International Conference on Intelligent Transportation Systems, ITSC 2020},
title = {{Minimum Race-Time Planning-Strategy for an Autonomous Electric Racecar}},
year = {2020}
}
@article{Quadflieg2011,
author = {Quadflieg, Jan and Preuss, Mike},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Quadflieg, Preuss - 2011 - Driving Faster Than a Human Player(2).pdf:pdf},
number = {section 2},
pages = {143--144},
title = {{Driving Faster Than a Human Player}},
url = {https://link.springer.com/chapter/10.1007/978-3-642-20525-5{\_}15},
year = {2011}
}

@article{Nobis2019,
abstract = {The task of simultaneous localization and mapping (SLAM) is a widely studied field in robotics research in the last decades. The goal of SLAM is to create an accurate map of the environment considering uncertainties in the pose as well as the environmental perception of the robot. Historically SLAM algorithms are applied in the field of indoor robotics. Recent developments in the area of autonomous driving surge a focus for SLAM applications in large scale outdoor environments. Two notable open source SLAM software packages are Gmapping and Google Cartographer. This paper focuses on a qualitative comparison of the aforementioned algorithms for such a scenario. We discuss the underlying algorithmic differences of the two packages. This serves as the foundation to present the SLAM results for different parameter configurations. We evaluate the accuracy of the resulting maps and the respective computational limitations. The maps are further evaluated against manually measured ground truth track boundaries. We show that the existing approaches can be adapted to large-scale outdoor environments.},
annote = {Mapping happens offline
2 solutions to mapping:
1. Hector mapping
2. Gmapping

Produces grid based representation of map

loop closure - matches start to end positions},
author = {Nobis, Felix and Betz, Johannes and Hermansdorfer, Leonhard and Lienkamp, Markus},
doi = {10.1145/3332305.3332319},
file = {::},
isbn = {9781450365925},
journal = {ACM International Conference Proceeding Series},
keywords = {Autonomous driving,Cartographer,Gmapping,LIDAR,Large scale SLAM,Perception,Sparse feature SLAM},
pages = {82--89},
title = {{Autonomous racing: A comparison of SLAM algorithms for large scale outdoor environments}},
year = {2019}
}
@article{Liniger2020,
abstract = {We consider autonomous racing of two cars and present an approach to formulate racing decisions as a noncooperative nonzero-sum game. We design three different games where the players aim to fulfill static track constraints as well as avoid collision with each other; the latter constraint depends on the combined actions of the two players. The difference between the games is the collision constraints and the payoff. In the first game, collision avoidance is only considered by the follower, and each player maximizes their own progress toward the finish line. We show that, thanks to the sequential structure of this game, equilibria can be computed through an efficient sequential maximization approach. Furthermore, we show that these actions, if feasible, are also a Stackelberg and Nash equilibrium in pure strategies of our second game where both players consider the collision constraints. The payoff of our third game is designed to promote blocking, by additionally rewarding the cars for staying ahead at the end of the horizon. We show that this changes the Stackelberg equilibrium, but has a minor influence on the Nash equilibria. For online implementation, we propose to play the games in a moving horizon fashion and discuss two methods for guaranteeing feasibility of the resulting coupled repeated games. Finally, we study the performance of the proposed approaches in simulation for a setup that replicates the miniature race car tested at the Automatic Control Laboratory, ETH Z{\"{u}}rich, Switzerland. The simulation study shows that the presented games can successfully model different racing behaviors and generate interesting racing situations.},
archivePrefix = {arXiv},
arxivId = {1712.03913},
author = {Liniger, Alexander and Lygeros, John},
doi = {10.1109/TCST.2019.2895282},
eprint = {1712.03913},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liniger, Lygeros - 2020 - A Noncooperative Game Approach to Autonomous Racing.pdf:pdf},
issn = {15580865},
journal = {IEEE Transactions on Control Systems Technology},
keywords = {Autonomous racing,game theory,hierarchical control,moving horizon games,noncooperative decision making},
number = {3},
pages = {884--897},
publisher = {IEEE},
title = {{A Noncooperative Game Approach to Autonomous Racing}},
volume = {28},
year = {2020}
}
@article{Bevilacqua2017,
annote = {Strategies for determining optimal trajectory:
1. Highest velocity (minimum curvature)
2. Lowest distance
Use genetic algorithm to find optimal weighting of combination of objectives 1 and 2.

OR:
Cost function = maximise distance in fixed number of time steps.
No vehicle dynamics - problem reduces to shortest path.

Track encoding:
Centerline in (x,y) coordinates

Results:
Genetic algorithm convergence in 2 hours
Particle swarm convergence in 30 minutes},
author = {Bevilacqua, M and Tsourdos, A and Starr, A},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bevilacqua, Tsourdos, Starr - 2017 - Particle swarm for path planning in a racing circuit simulation(2).pdf:pdf},
isbn = {9781509035960},
keywords = {as a trajectory for,autonomous races,current position,during qualification rounds,ex,genetic algorithm,laps done alone,more importantly,optimization,or,particle swarm,race line,trajectory planning},
publisher = {IEEE},
title = {{Particle swarm for path planning in a racing circuit simulation}},
url = {http://proceedings.mlr.press/v123/o-kelly20a.html},
year = {2017}
}


@article{Capo2020,
abstract = {The applications of deep reinforcement learning to racing games so far struggled to reach a performance competitive with the state of the art in this field. Previous work, mainly focused on a low-level input design, show that artificial agents are able to learn to stay on track starting from no driving knowledge; however, the final performance is still far from those of competitive driving. The scope of this work is to investigate in which measure rising the abstraction level can help the learning process. Using The Open Racing Car Simulator (TORCS) environment and the Deep Deterministic Policy Gradients (DDPG) algorithm, we develop artificial agents, considering both numerical and visual inputs, based on deep neural networks. These agents learn to compute either a target point on track or, additionally, a correction to the target maximum speed at the current position, which are then provided as input to a low-level control logic. Our results show that our approach is able to achieve a fair performance, though extremely sensitive to the low-level logic. Further work is necessary in order to understand how to fully exploit a high-level control design.},
annote = {Final performance of end-to-end agents are far from competitive

Learns offset from centre of track:
High level agent performs better than low level agent on seen and unseen tracks. Hybrid approach more effetive on HL agent than low level agent. Significant pefromance boost!

Learns offset from centre of track +max target speed:},
author = {Capo, Emilio and Loiacono, Daniele},
doi = {10.1109/SSCI47803.2020.9308138},
url = {https://doi.org/10.1109/SSCI47803.2020.9308138},
isbn = {9781728125473},
journal = {2020 IEEE Symposium Series on Computational Intelligence, SSCI 2020},
pages = {2327--2334},
title = {{Short-Term Trajectory Planning in TORCS using Deep Reinforcement Learning}},
year = {2020}
}


@article{Ulyhuohvv2020,
author = {Liu, Yanbo and Shi, Liangren and Xu, Wenchao and Xiong, Xin and Sun, Weiqi and Qu, Ling},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ulyhuohvv et al. - 2020 - 'hvljq ri 'ulyhuohvv 5dflqj {\&}kdvvlv {\%}dvhg rq 03{\&}(2).pdf:pdf},
isbn = {9781728176871},
journal = {Chinese automation congress},
keywords = {FSAC,MPC,drive by wire,path planning,self driving},
pages = {6061--6066},
title = {{Design of Driverless Racing Chassis Based on MPC}},
year = {2020}
}
@article{Kelly2020,
annote = {See benchmarks!},
author = {Kelly, Matthew O},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kelly - 2020 - F1TENTH An Open-source Evaluation Environment for Continuous Control and Reinforcement Learning.pdf:pdf},
keywords = {autonomous racing,autonomous vehicles,reinforcement learning},
pages = {77--89},
title = {{F1TENTH : An Open-source Evaluation Environment for Continuous Control and Reinforcement Learning}},
url = {https://link.springer.com/chapter/10.1007/978-3-642-20525-5{\_}15},
year = {2020}
}
@article{Vivek2019,
abstract = {This paper presents comparative study between Stanley, LQR (Linear Quadratic Regulator) and MPC (Model Predictive Controller) controllers for path tracking application, which is a level 4 automation feature under ADAS/AD (Advanced Driver Assistance System/Autonomous Driving). The accuracy associated with all the controllers are compared by making the vehicle model run in a prescribed environment. The initial designs are done in MATLAB environment and later they are interfaced with IPG CarMaker vehicle simulation tool for fine tuning. Stanley controller is more of an intuitive steering control law where as LQR and MPC are more advanced optimal controllers. The control actions are calculated by optimising the states of the model. Kinematic vehicle model is used with states as errors and a comparator design is made to find the deviation of the vehicle from the prescribed path. The paper gives a detailed idea about the controllers regarding its use, advantages and limitations in this application.},
author = {Vivek, K. V. and Sheta, Milankumar Ambalal and Gumtapure, Veershetty},
doi = {10.1109/ICISGT44072.2019.00030},
file = {::},
isbn = {9781728114231},
journal = {Proceedings - 2019 IEEE International Conference on Intelligent Systems and Green Technology, ICISGT 2019},
keywords = {Adas/ad,Ipg carmaker,Lqr,Matlab,Mpc,Path tracking,Stanley controller},
pages = {67--71},
title = {{A comparative study of stanley, lqr and mpc controllers for path tracking application (adas/ad)}},
year = {2019}
}



@article{Wischnewski2019,
author = {Wischnewski, Alexander and Stahl, Tim and Betz, Johannes and Lohmann, Boris},
doi = {10.1016/j.ifacol.2019.08.064},
file = {::},
issn = {24058963},
journal = {IFAC-PapersOnLine},
keywords = {Autonomous Vehicles,Kalman Filters,Robust Performance,Sensor Fusion,State Estimation},
number = {8},
pages = {307--312},
publisher = {Elsevier Ltd},
title = {Vehicle Dynamics State Estimation and Localization for High Performance Race Cars},
url = {https://doi.org/10.1016/j.ifacol.2019.08.064},
volume = {52},
year = {2019}
}
@article{Garlick2021,
abstract = {Widespread development of driverless vehicles has led to the formation of autonomous racing, where technological development is accelerated by the high speeds and competitive environment of motorsport. A particular challenge for an autonomous vehicle is that of identifying a target trajectory–or, in the case of a competition vehicle, the racing line. Many existing approaches to finding the racing line are either not time-optimal solutions, or are computationally expensive – rendering them unsuitable for real-time application using on-board processing hardware. This study describes a machine learning approach to generating an accurate prediction of the racing line in real-time on desktop processing hardware. The proposed algorithm is a feed-forward neural network, trained using a dataset comprising racing lines for a large number of circuits calculated via traditional optimal control lap time simulation. The network predicts the racing line with a mean absolute error of ±0.27 m, and just ±0.11 m at corner apex – comparable to human drivers, and autonomous vehicle control subsystems. The approach generates predictions within 33 ms, making it over 9000 times faster than traditional methods of finding the optimal trajectory. Results suggest that for certain applications data-driven approaches to find near-optimal racing lines may be favourable to traditional computational methods.},
annote = {Existing (classical) approaches are not time optimal or are too computationally expensive to be deployed in real time.

This approach takes 33ms to generate optimal trajectory - 9000 times quicker than traditional methods. Data driven methods are favorable.

Rapid appraches to calculating target path are based on circuit geometry, and thus provide no gaurantee of finding a time optimal solution.

Approach: Split tracks into segments, generate optimal path for each segment using existing OCP. ANN can calculate features for new, unseen circuit.},
archivePrefix = {arXiv},
arxivId = {2102.02315},
author = {Garlick, S. and Bradley, A.},
doi = {10.1080/00423114.2021.2011929},
eprint = {2102.02315},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garlick, Bradley - 2021 - Real-time optimal trajectory planning for autonomous vehicles and lap time simulation using machine learning.pdf:pdf},
issn = {17445159},
journal = {Vehicle System Dynamics},
keywords = {Autonomous racing vehicle,lap time simulation,machine learning,neural network,optimal racing line,trajectory planning},
title = {{Real-time optimal trajectory planning for autonomous vehicles and lap time simulation using machine learning}},
url = {https://doi.org/10.1080/00423114.2021.2011929},
year = {2021}
}

@article{Gotlib2019,
author = {Gotlib, Adam and Lukojc, Kornelia and Szczygielski, Mateusz},
doi = {10.1109/IIPHDW.2019.8755418},
isbn = {9781728104232},
journal = {International Interdisciplinary PhD Workshop},
keywords = {ROS,autonomous car,lidar sensor,localization,mapping,odometry},
pages = {7--11},
title = {{Localization-based software architecture for 1:10 scale autonomous car}},
year = {2019},
url = {http://doi.org/10.1109/IIPHDW.2019.8755418}
}

@article{Jaritz2018a,
abstract = {We present research using the latest reinforcement learning algorithm for end-to-end driving without any mediated perception (object recognition, scene understanding). The newly proposed reward and learning strategies lead together to faster convergence and more robust driving using only RGB image from a forward facing camera. An Asynchronous Actor Critic (A3C) framework is used to learn the car control in a physically and graphically realistic rally game, with the agents evolving simultaneously on tracks with a variety of road structures (turns, hills), graphics (seasons, location) and physics (road adherence). A thorough evaluation is conducted and generalization is proven on unseen tracks and using legal speed limits. Open loop tests on real sequences of images show some domain adaption capability of our method.},
archivePrefix = {arXiv},
arxivId = {1807.02371},
author = {Jaritz, Maximilian and {De Charette}, Raoul and Toromanoff, Marin and Perot, Etienne and Nashashibi, Fawzi},
doi = {10.1109/ICRA.2018.8460934},
eprint = {1807.02371},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jaritz et al. - 2018 - End-to-End Race Driving with Deep Reinforcement Learning(3).pdf:pdf},
isbn = {9781538630815},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {2070--2075},
publisher = {IEEE},
title = {{End-to-End Race Driving with Deep Reinforcement Learning}},
url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp={\&}arnumber=8460934},
year = {2018}
}
@article{Frohlich2021,
abstract = {Model predictive control has been widely used in the field of autonomous racing and many data-driven approaches have been proposed to improve the closed-loop performance and to minimize lap time. However, it is often overlooked that a change in the environmental conditions, e.g., when it starts raining, it is not only required to adapt the predictive model but also the controller parameters need to be adjusted. In this paper, we address this challenge with the goal of requiring only few data. The key novelty of the proposed approach is that we leverage the learned dynamics model to encode the environmental condition as context. This insight allows us to employ contextual Bayesian optimization, thus accelerating the controller tuning problem when the environment changes and to transfer knowledge across different cars. The proposed framework is validated on an experimental platform with 1:28 scale RC race cars. We perform an extensive evaluation with more than 2'000 driven laps demonstrating that our approach successfully optimizes the lap time across different contexts faster compared to standard Bayesian optimization.},
archivePrefix = {arXiv},
arxivId = {2110.02710},
author = {Fr{\"{o}}hlich, Lukas P. and K{\"{u}}ttel, Christian and Arcari, Elena and Hewing, Lukas and Zeilinger, Melanie N. and Carron, Andrea},
eprint = {2110.02710},
file = {:C$\backslash$:/Users/Andrew/Documents/masters/presentation/2110.02710.pdf:pdf},
title = {{Model Learning and Contextual Controller Tuning for Autonomous Racing}},
url = {http://arxiv.org/abs/2110.02710},
year = {2021}
}
@article{Brunner2018,
abstract = {We propose an optimization based, data-driven framework to design controllers for repetitive tasks. The proposed framework builds on previous work of Learning Model Predictive Control and focuses on problems where the terminal condition of one iteration is the initial condition of the next iteration. A terminal cost and a sampled safe set are learned from data to guarantee recursive feasibility and non-decreasing performance cost at each iteration. The proposed control logic is tested on an autonomous racing example, where the vehicle dynamics are identified online. Experimental results on a 1:10 scale RC car illustrate the effectiveness of the proposed approach.},
author = {Brunner, Maximilian and Rosolia, Ugo and Gonzales, Jon and Borrelli, Francesco},
doi = {10.1109/CDC.2017.8264027},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Brunner et al. - 2018 - Repetitive learning model predictive control An autonomous racing example.pdf:pdf},
isbn = {9781509028733},
journal = {2017 IEEE 56th Annual Conference on Decision and Control, CDC 2017},
number = {December},
pages = {2545--2550},
title = {{Repetitive learning model predictive control: An autonomous racing example}},
volume = {2018-Janua},
year = {2018}
}
@article{Voser2010,
abstract = {This paper presents simple analytical techniques that are used to understand and control high sideslip drift manoeuvres of road vehicles. These are manoeuvres in which a skilled driver stabilises a vehicle beyond its limits of handling, an operating regime responsible for major safety concerns in everyday driving. An analysis of the equilibria of a bicycle model with nonlinear tyres reveals the existence of unstable 'drift equilibria' corresponding to cornering at high sideslip angle in a countersteer configuration. Equipped with this information, linearisation about a desired drift equilibrium is used to design a controller that stabilises the vehicle at the equilibrium. The controller is subsequently implemented on a steer- and drive-by-wire testbed and successfully used to achieve autonomous drifts. {\textcopyright} 2010 Taylor {\&} Francis.},
annote = {Drift controller?},
author = {Voser, Christoph and Hindiyeh, Rami Y. and Gerdes, J. Christian},
doi = {10.1080/00423111003746140},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Voser, Hindiyeh, Gerdes - 2010 - Analysis and control of high sideslip manoeuvres(2).pdf:pdf},
issn = {00423114},
journal = {Vehicle System Dynamics},
keywords = {drifting,high sideslip manoeuvres,nonlinear vehicle dynamics,steering control,vehicle control},
number = {SUPPL. 1},
pages = {317--336},
title = {{Analysis and control of high sideslip manoeuvres}},
volume = {48},
year = {2010}
}

@article{Abraham2019,
abstract = {This paper presents an active learning strategy for robotic systems that takes into account task information, enables fast learning, and allows control to be readily synthesized by taking advantage of the Koopman operator representation. We first motivate the use of representing nonlinear systems as linear Koopman operator systems by illustrating the improved model-based control performance with an actuated Van der Pol system. Information-theoretic methods are then applied to the Koopman operator formulation of dynamical systems where we derive a controller for active learning of robot dynamics. The active learning controller is shown to increase the rate of information about the Koopman operator. In addition, our active learning controller can readily incorporate policies built on the Koopman dynamics, enabling the benefits of fast active learning and improved control. Results using a quadcopter illustrate single-execution active learning and stabilization capabilities during free fall. The results for active learning are extended for automating Koopman observables and we implement our method on real robotic systems.},
archivePrefix = {arXiv},
arxivId = {1906.05194},
author = {Abraham, Ian and Murphey, Todd D.},
doi = {10.1109/TRO.2019.2923880},
eprint = {1906.05194},
file = {::},
issn = {19410468},
journal = {IEEE Transactions on Robotics},
keywords = {Active learning,Koopman operators,information theoretic control,single execution learning},
number = {5},
pages = {1071--1083},
title = {{Active Learning of Dynamics for Data-Driven Control Using Koopman Operators}},
volume = {35},
year = {2019}
}


@article{Fuchs2021,
abstract = {Autonomous car racing is a major challenge in robotics. It raises fundamental problems for classical approaches such as planning minimum-time trajectories under uncertain dynamics and controlling the car at the limits of its handling. Besides, the requirement of minimizing the lap time, which is a sparse objective, and the difficulty of collecting training data from human experts have also hindered researchers from directly applying learning-based approaches to solve the problem. In the present work, we propose a learning-based system for autonomous car racing by leveraging a high-fidelity physical car simulation, a course-progress proxy reward, and deep reinforcement learning. We deploy our system in Gran Turismo Sport, a world-leading car simulator known for its realistic physics simulation of different race cars and tracks, which is even used to recruit human race car drivers. Our trained policy achieves autonomous racing performance that goes beyond what had been achieved so far by the built-in AI, and at the same time, outperforms the fastest driver in a dataset of over 50,000 human players.},
annote = {Notes:
Neural networks improve high speed driving when combined with model predictive path integral control (MPPI) [1, 4, 5]
Hierarchical approaches are susceptible to failure of each sub-module},
archivePrefix = {arXiv},
arxivId = {2008.07971},
author = {Fuchs, Florian and Song, Yunlong and Kaufmann, Elia and Scaramuzza, Davide and Durr, Peter},
doi = {10.1109/LRA.2021.3064284},
url = {http://doi.org/10.1109/LRA.2021.3064284},
journal = {IEEE Robotics and Automation Letters},
keywords = {Autonomous agents,reinforcement learning},
number = {3},
pages = {4257--4264},
title = {{Super-Human Performance in Gran Turismo Sport Using Deep Reinforcement Learning}},
volume = {6},
year = {2021}
}
@article{Palafox2019,
abstract = {Typically, lane departure warning systems rely on lane lines being present on the road. However, in many scenarios, e.g., secondary roads or some streets in cities, lane lines are either not present or not sufficiently well signaled. In this work, we present a vision-based method to locate a vehicle within the road when no lane lines are present using only RGB images as input. To this end, we propose to fuse together the outputs of a semantic segmentation and a monocular depth estimation architecture to reconstruct locally a semantic 3D point cloud of the viewed scene. We only retain points belonging to the road and, additionally, to any kind of fences or walls that might be present right at the sides of the road. We then compute the width of the road at a certain point on the planned trajectory and, additionally, what we denote as the fence-to-fence distance. Our system is suited to any kind of motoring scenario and is especially useful when lane lines are not present on the road or do not signal the path correctly. The additional fence-to-fence distance computation is complementary to the road's width estimation. We quantitatively test our method on a set of images featuring streets of the city of Munich that contain a road-fence structure, so as to compare our two proposed variants, namely the road's width and the fence-to-fence distance computation. In addition, we also validate our system qualitatively on the Stuttgart sequence of the publicly available Cityscapes dataset, where no fences or walls are present at the sides of the road, thus demonstrating that our system can be deployed in a standard city-like environment. For the benefit of the community, we make our software open source.},
author = {Palafox, Pablo R. and Betz, Johannes and Nobis, Felix and Riedl, Konstantin and Lienkamp, Markus},
doi = {10.3390/s19143224},
file = {::},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Advanced Driver Assistance Systems (ADAS),Autonomous driving,Computer vision,Deep learning,Fusion architecture,Monocular depth estimation,Scene understanding,Semantic segmentation,Situational awareness},
number = {14},
pages = {1--20},
pmid = {31336666},
title = {{Semanticdepth: Fusing semantic segmentation and monocular depth estimation for enabling autonomous driving in roads without lane lines}},
volume = {19},
year = {2019}
}

@article{Ha2018,
abstract = {A generative recurrent neural network is quickly trained in an unsupervised manner to model popular reinforcement learning environments through compressed spatio-temporal representations. The world model's extracted features are fed into compact and simple policies trained by evolution, achieving state of the art results in various environments. We also train our agent entirely inside of an environment generated by its own internal world model, and transfer this policy back into the actual environment. Interactive version of paper: https://worldmodels.github.io.},
author = {Ha, David and Schmidhuber, J{\"{u}}rgen},
file = {:C$\backslash$:/Users/Andrew/Downloads/NeurIPS-2018-recurrent-world-models-facilitate-policy-evolution-Paper.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {C},
pages = {2450--2462},
title = {{Recurrent world models facilitate policy evolution}},
volume = {2018-Decem},
year = {2018}
}
@article{Sutton1991,
abstract = {Dyna is an AI architecture that integrates learning, planning, and reactive execution. Learning methods are used in Dyna both for compiling planning results and for updating a model of the effects of the agent's actions on the world. Planning is incremental and can use the probabilistic and ofttimes incorrect world models generated by learning processes. Execution is fully reactive in the sense that no planning intervenes between perception and action. Dyna relies on machine learning methods for learning from examples---these are among the basic building blocks making up the architecture---yet is not tied to any particular method. This paper briefly introduces Dyna and discusses its strengths and weaknesses with respect to other architectures.},
author = {Sutton, Richard S.},
doi = {10.1145/122344.122377},
file = {:C$\backslash$:/Users/Andrew/Downloads/10.1.1.48.6005.pdf:pdf},
issn = {0163-5719},
journal = {ACM SIGART Bulletin},
number = {4},
pages = {160--163},
title = {{Dyna, an integrated architecture for learning, planning, and reacting}},
volume = {2},
year = {1991}
}
@article{Schulman2015,
abstract = {In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
archivePrefix = {arXiv},
arxivId = {1502.05477},
author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael and Abbeel, Pieter},
eprint = {1502.05477},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schulman et al. - 2015 - Trust region policy optimization.pdf:pdf},
isbn = {9781510810587},
journal = {32nd International Conference on Machine Learning, ICML 2015},
pages = {1889--1897},
title = {{Trust region policy optimization}},
volume = {3},
year = {2015}
}
@article{Marnin,
abstract = {Buildings with intermittent occupancy may not perform thermally the same as typical commercial and residential facilities. Thermal comfort requirements require careful envelope design coupled with the appropriate air-conditioning system operation strategies. One of the most prominent examples of such buildings is mosques. Mosques are usually occupied five intermittent times day and night all year round. Like any other building, they have to be mechanically air-conditioned to achieve the required thermal comfort for worshippers especially in harsh climatic regions. This paper describes the physical and operating characteristics typical for the intermittently occupied mosques as well as the results of the thermal optimization of a medium size mosque in the two hot-dry and hot-humid Saudi Arabian cities of Riyadh and Jeddah. The analysis utilizes a direct search optimization technique that is coupled to an hourly energy simulation program. Based on that, design guidelines are presented for the optimum thermal performance of mosques in these two cities in addition to other design and operating factors that need to be considered for mosques in general. {\textcopyright} 2009 The Author(s).},
author = {Marnin, John},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Marnin - Unknown - REINFORCEMENT LEARNING ALGORITHMS FOR REPRESENTING AND MANAGING UNCERTAINTY IN ROBOTICS(2).pdf:pdf},
title = {{REINFORCEMENT LEARNING ALGORITHMS FOR REPRESENTING AND MANAGING UNCERTAINTY IN ROBOTICS}},
url = {https://robustfieldautonomylab.github.io/John{\_}Martin{\_}PhD{\_}Thesis.pdf}
}

@article{Schaul2016,
abstract = {Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.},
archivePrefix = {arXiv},
arxivId = {1511.05952},
author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
eprint = {1511.05952},
file = {:C$\backslash$:/Users/Andrew/Downloads/1511.05952.pdf:pdf},
journal = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings},
pages = {1--21},
title = {{Prioritized experience replay}},
year = {2016}
}
@article{Geramifard2013,
abstract = {A Markov Decision Process (MDP) is a natural framework for formulating sequential decision-making problems under uncertainty. In recent years, researchers have greatly advanced algorithms for learning and acting in MDPs. This article reviews such algorithms, beginning with well-known dynamic programming methods for solving MDPs such as policy iteration and value iteration, then describes approximate dynamic programming methods such as trajectory based value iteration, and finally moves to reinforcement learning methods such as Q-Learning, SARSA, and least-squares policy iteration. We describe algorithms in a unified framework, giving pseudocode together with memory and iteration complexity analysis for each. Empirical evaluations of these techniques with four representations across four domains, provide insight into how these algorithms perform with various feature sets in terms of running time and performance.{\textcopyright} 2013 A. Geramifard, T. J. Walsh, S. Tellex, G. Chowdhary.},
author = {Geramifard, Alborz and Walsh, Thomas J. and Tellex, Stefanie and Chowdhary, Girish and Roy, Nicholas and How, Jonathan P.},
doi = {10.1561/2200000042},
file = {::},
issn = {19358237},
journal = {Foundations and Trends in Machine Learning},
number = {4},
pages = {375--454},
title = {{A tutorial on linear function approximators for dynamic programming and reinforcement learning}},
volume = {6},
year = {2013}
}
@article{Wu,
abstract = {Reinforcement learning requires skillful definition and remarkable computational efforts to solve optimization and control problems, which could impair its prospect. Introducing human guidance into reinforcement learning is a promising way to improve learning performance. In this paper, a comprehensive human guidance-based reinforcement learning framework is established. A novel prioritized experience replay mechanism that adapts to human guidance in the reinforcement learning process is proposed to boost the efficiency and performance of the reinforcement learning algorithm. To relieve the heavy workload on human participants, a behavior model is established based on an incremental online learning method to mimic human actions. We design two challenging autonomous driving tasks for evaluating the proposed algorithm. Experiments are conducted to access the training and testing performance and learning mechanism of the proposed algorithm. Comparative results against the state-of-the-arts suggest the advantages of our algorithm in terms of learning efficiency, performance, and robustness.},
author = {Wu, Jingda and Member, Student and Huang, Zhiyu and Huang, Wenhui and Lv, Chen and Member, Senior},
file = {:C$\backslash$:/Users/Andrew/Downloads/2109.12516.pdf:pdf},
isbn = {1922500046},
keywords = {Index Terms-Reinforcement learning,autonomous driving,human demonstration,priority experience replay},
pages = {1--13},
title = {{Prioritized Experience-based Reinforcement Learning with Human Guidance: Methodology and Application to Autonomous Driving}}
}
@article{Barth-Maron2018,
abstract = {This work adopts the very successful distributional perspective on reinforcement learning and adapts it to the continuous control setting. We combine this within a distributed framework for off-policy learning in order to develop what we call the Distributed Distributional Deep Deterministic Policy Gradient algorithm, D4PG. We also combine this technique with a number of additional, simple improvements such as the use of N-step returns and prioritized experience replay. Experimentally we examine the contribution of each of these individual components, and show how they interact, as well as their combined contributions. Our results show that across a wide variety of simple control tasks, difficult manipulation tasks, and a set of hard obstacle-based locomotion tasks the D4PG algorithm achieves state of the art performance.},
archivePrefix = {arXiv},
arxivId = {1804.08617},
author = {Barth-Maron, Gabriel and Hoffman, Matthew W. and Budden, David and Dabney, Will and Horgan, Dan and Dhruva, T. B. and Muldal, Alistair and Heess, Nicolas and Lillicrap, Timothy},
eprint = {1804.08617},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Barth-Maron et al. - 2018 - Distributed distributional deterministic policy gradients(2).pdf:pdf},
journal = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
pages = {1--16},
title = {{Distributed distributional deterministic policy gradients}},
year = {2018}
}

@article{Geibel2005,
abstract = {In this paper, we consider Markov Decision Processes (MDPs) with error states. Error states are those states entering which is undesirable or dangerous. We define the risk with respect to a policy as the probability of entering such a state when the policy is pursued. We consider the problem of finding good policies whose risk is smaller than some user-specified threshold, and formalize it as a constrained MDP with two criteria. The first criterion corresponds to the value function originally given. We will show that the risk can be formulated as a second criterion function based on a cumulative return, whose definition is independent of the original value function. We present a model free, heuristic reinforcement learning algorithm that aims at finding good deterministic policies. It is based on weighting the original value function and the risk. The weight parameter is adapted in order to find a feasible solution for the constrained problem that has a good performance with respect to the value function. The algorithm was successfully applied to the control of a feed tank with stochastic inflows that lies upstream of a distillation column. This control task was originally formulated as an optimal control problem with chance constraints, and it was solved under certain assumptions on the model to obtain an optimal solution. The power of our learning algorithm is that it can be used even when some of these restrictive assumptions are relaxed. {\textcopyright}2005 AI Access Foundation. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {1109.2147},
author = {Geibel, Peter and Wysotzki, Fritz},
doi = {10.1613/jair.1666},
eprint = {1109.2147},
file = {:C$\backslash$:/Users/Andrew/Downloads/1109.2147.pdf:pdf},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {81--108},
title = {{Risk-sensitive reinforcement learning applied to control under constraints}},
volume = {24},
year = {2005}
}
@article{Huang2020,
abstract = {This chapter aims to introduce one of the most important deep reinforcement learning algorithms, called deep Q-networks. We will start with the Q-learning algorithm via temporal difference learning, and introduce the deep Q-networks algorithm and its variants. We will end this chapter with code examples and experimental comparison of deep Q-networks and its variants in practice.},
author = {Huang, Yanhua},
doi = {10.1007/978-981-15-4095-0_4},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang - 2020 - Deep Q-networks.pdf:pdf},
isbn = {9789811540950},
journal = {Deep Reinforcement Learning: Fundamentals, Research and Applications},
keywords = {DQN,Distributional reinforcement learning,Double DQN,Dueling DQN,Prioritized experience replay,Temporal difference learning},
pages = {135--160},
title = {{Deep Q-networks}},
year = {2020}
}
@article{Mnih2015,
abstract = {The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.},
annote = {Instability with non linear funciton approximators:
Small updates to Q significantly change policy
Correlations present in sequence of observations
Correlation between action and target values

Solution to address instability: Experience replay
Randomise data
Iterative update - adjust action values toward target values
--Targets are only periodically updated - reduce correlation with target},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih et al. - 2015 - Human-level control through deep reinforcement learning.pdf:pdf},
issn = {14764687},
journal = {Nature},
number = {7540},
pages = {529--533},
pmid = {25719670},
title = {{Human-level control through deep reinforcement learning}},
volume = {518},
year = {2015}
}
@article{Dalal2018,
abstract = {We address the problem of deploying a reinforcement learning (RL) agent on a physical system such as a datacenter cooling unit or robot, where critical constraints must never be violated. We show how to exploit the typically smooth dynamics of these systems and enable RL algorithms to never violate constraints during learning. Our technique is to directly add to the policy a safety layer that analytically solves an action correction formulation per each state. The novelty of obtaining an elegant closed-form solution is attained due to a linearized model, learned on past trajectories consisting of arbitrary actions. This is to mimic the real-world circumstances where data logs were generated with a behavior policy that is implausible to describe mathematically; such cases render the known safety-aware off-policy methods inapplicable. We demonstrate the efficacy of our approach on new representative physics-based environments, and prevail where reward shaping fails by maintaining zero constraint violations.},
archivePrefix = {arXiv},
arxivId = {1801.08757},
author = {Dalal, Gal and Dvijotham, Krishnamurthy and Vecerik, Matej and Hester, Todd and Paduraru, Cosmin and Tassa, Yuval},
eprint = {1801.08757},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dalal et al. - 2018 - Safe Exploration in Continuous Action Spaces.pdf:pdf},
title = {{Safe Exploration in Continuous Action Spaces}},
url = {http://arxiv.org/abs/1801.08757},
year = {2018}
}

@article{Chow2019,
abstract = {We study continuous action reinforcement learning problems in which it is crucial that the agent interacts with the environment only through safe policies, i.e.,{\~{}}policies that do not take the agent to undesirable situations. We formulate these problems as constrained Markov decision processes (CMDPs) and present safe policy optimization algorithms that are based on a Lyapunov approach to solve them. Our algorithms can use any standard policy gradient (PG) method, such as deep deterministic policy gradient (DDPG) or proximal policy optimization (PPO), to train a neural network policy, while guaranteeing near-constraint satisfaction for every policy update by projecting either the policy parameter or the action onto the set of feasible solutions induced by the state-dependent linearized Lyapunov constraints. Compared to the existing constrained PG algorithms, ours are more data efficient as they are able to utilize both on-policy and off-policy data. Moreover, our action-projection algorithm often leads to less conservative policy updates and allows for natural integration into an end-to-end PG training pipeline. We evaluate our algorithms and compare them with the state-of-the-art baselines on several simulated (MuJoCo) tasks, as well as a real-world indoor robot navigation problem, demonstrating their effectiveness in terms of balancing performance and constraint satisfaction. Videos of the experiments can be found in the following link: https://drive.google.com/file/d/1pzuzFqWIE710bE2U6DmS59AfRzqK2Kek/view?usp=sharing.},
archivePrefix = {arXiv},
arxivId = {1901.10031},
author = {Chow, Yinlam and Nachum, Ofir and Faust, Aleksandra and Duenez-Guzman, Edgar and Ghavamzadeh, Mohammad},
eprint = {1901.10031},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chow et al. - 2019 - Lyapunov-based Safe Policy Optimization for Continuous Control.pdf:pdf},
title = {{Lyapunov-based Safe Policy Optimization for Continuous Control}},
url = {http://arxiv.org/abs/1901.10031},
year = {2019}
}
@article{Okada2021,
abstract = {In the present paper, we propose a decoder-free extension of Dreamer, a leading model-based reinforcement learning (MBRL) method from pixels. Dreamer is a sample- and cost-efficient solution to robot learning, as it is used to train latent state-space models based on a variational autoencoder and to conduct policy optimization by latent trajectory imagination. However, this autoencoding based approach often causes object vanishing, in which the autoencoder fails to perceives key objects for solving control tasks, and thus significantly limiting Dreamer's potential. This work aims to relieve this Dreamer's bottleneck and enhance its performance by means of removing the decoder. For this purpose, we firstly derive a likelihood-free and InfoMax objective of contrastive learning from the evidence lower bound of Dreamer. Secondly, we incorporate two components, (i) independent linear dynamics and (ii) the random crop data augmentation, to the learning scheme so as to improve the training performance. In comparison to Dreamer and other recent model-free reinforcement learning methods, our newly devised Dreamer with InfoMax and without generative decoder (Dreaming) achieves the best scores on 5 difficult simulated robotics tasks, in which Dreamer suffers from object vanishing.},
archivePrefix = {arXiv},
arxivId = {2007.14535},
author = {Okada, Masashi and Taniguchi, Tadahiro},
doi = {10.1109/ICRA48506.2021.9560734},
eprint = {2007.14535},
file = {:C$\backslash$:/Users/Andrew/Downloads/2007.14535.pdf:pdf},
isbn = {9781728190778},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {4209--4215},
title = {{Dreaming: Model-based Reinforcement Learning by Latent Imagination without Reconstruction}},
volume = {2021-May},
year = {2021}
}
@article{Bellemare2017,
abstract = {In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman's equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.},
archivePrefix = {arXiv},
arxivId = {1707.06887},
author = {Bellemare, Marc G. and Dabney, Will and Munos, R{\'{e}}mi},
eprint = {1707.06887},
file = {:C$\backslash$:/Users/Andrew/Downloads/1707.06887.pdf:pdf},
isbn = {9781510855144},
journal = {34th International Conference on Machine Learning, ICML 2017},
pages = {693--711},
title = {{A distributional perspective on reinforcement learning}},
volume = {1},
year = {2017}
}
@article{Anon2021,
author = {Anon},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Anon - 2021 - Lyapunov barrier policy optimisation.pdf:pdf},
pages = {1--10},
title = {{Lyapunov barrier policy optimisation}},
year = {2021}
}
@article{Lutjens2019a,
abstract = {Many current autonomous systems are being designed with a strong reliance on black box predictions from deep neural networks (DNNs). However, DNNs tend to be overconfident in predictions on unseen data and can give unpredictable results for far-from-distribution test data. The importance of predictions that are robust to this distributional shift is evident for safety-critical applications, such as collision avoidance around pedestrians. Measures of model uncertainty can be used to identify unseen data, but the state-of-the-art extraction methods such as Bayesian neural networks are mostly intractable to compute. This paper uses MC-Dropout and Bootstrapping to give computationally tractable and parallelizable uncertainty estimates. The methods are embedded in a Safe Reinforcement Learning framework to form uncertainty-aware navigation around pedestrians. The result is a collision avoidance policy that knows what it does not know and cautiously avoids pedestrians that exhibit unseen behavior. The policy is demonstrated in simulation to be more robust to novel observations and take safer actions than an uncertainty-unaware baseline.},
archivePrefix = {arXiv},
arxivId = {1810.08700},
author = {L{\"{u}}tjens, Bj{\"{o}}rn and Everett, Michael and How, Jonathan P.},
doi = {10.1109/ICRA.2019.8793611},
eprint = {1810.08700},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/L{\"{u}}tjens, Everett, How - 2019 - Safe reinforcement learning with model uncertainty estimates(2).pdf:pdf},
isbn = {9781538660263},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {8662--8668},
title = {{Safe reinforcement learning with model uncertainty estimates}},
volume = {2019-May},
year = {2019}
}

@article{Lillicrap2016a,
abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies “end-to-end”: directly from raw pixel inputs.},
archivePrefix = {arXiv},
arxivId = {1509.02971},
author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
eprint = {1509.02971},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lillicrap et al. - 2016 - Continuous control with deep reinforcement learning.pdf:pdf},
journal = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings},
title = {{Continuous control with deep reinforcement learning}},
year = {2016}
}
@article{Deisenroth2011,
abstract = {In this paper, we introduce PILCO, a practical, data-efficient model-based policy search method. PILCO reduces model bias, one of the key problems of model-based reinforcement learning, in a principled way. By learning a probabilistic dynamics model and explicitly incorporating model uncertainty into long-term planning, PILCO can cope with very little data and facilitates learning from scratch in only a few trials. Policy evaluation is performed in closed form using state-of-the-art approximate inference. Furthermore, policy gradients are computed analytically for policy improvement. We report unprecedented learning efficiency on challenging and high-dimensional control tasks. Copyright 2011 by the author(s)/owner(s).},
author = {Deisenroth, Marc Peter and Rasmussen, Carl Edward},
file = {:C$\backslash$:/Users/Andrew/Downloads/DeiRas11.pdf:pdf},
isbn = {9781450306195},
journal = {Proceedings of the 28th International Conference on Machine Learning, ICML 2011},
pages = {465--472},
title = {{PILCO: A model-based and data-efficient approach to policy search}},
year = {2011}
}
@article{Hessel2018,
abstract = {The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.},
archivePrefix = {arXiv},
arxivId = {1710.02298},
author = {Hessel, Matteo and Modayil, Joseph and {Van Hasselt}, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
eprint = {1710.02298},
file = {:C$\backslash$:/Users/Andrew/Downloads/1710.02298.pdf:pdf},
isbn = {9781577358008},
journal = {32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
pages = {3215--3222},
title = {{Rainbow: Combining improvements in deep reinforcement learning}},
year = {2018}
}
@article{Dankwa2019,
abstract = {In this current research, Twin-Delayed DDPG (TD3) algorithm has been used to solve the most challenging virtual Artificial Intelligence application by training a 4-ant-legged robot as an Intelligent Agent to run across a field. Twin-Delayed DDPG (TD3) is an incredibly smart AI model of a Deep Reinforcement Learning which combines the state-of-the-art methods in Artificial Intelligence. These includes Policy gradient, Actor-Critics, and continuous Double Deep Q-Learning. These Deep Reinforcement Learning approaches trained an Intelligent agent to interact with an environment with automatic feature engineering, that is, necessitating minimal domain knowledge. For the implementation of the TD3, we used a two-layer feedforward neural network of 400 and 300 hidden nodes respectively, with Rectified Linear Units (ReLU) as an activation function between each layer for both the Actor and Critics. We, then added a final tanh unit after the output of the Actor. The Critic receives both the state and action as input to the first layer. Both the network parameters were updated using Adam optimizer. The idea behind the Twin-Delayed DDPG (TD3) is to reduce overestimation bias in Deep Q-Learning with discrete actions which are ineffective in an Actor-Critic domain setting. Based on the Maximum Average Reward over the evaluation time-step, our model achieved an approximate maximum of 2364. Therefore, we can truly say that, TD3 has obviously improved on both the learning speed and performance of the Deep Deterministic Policy Gradient (DDPG) in a challenging environment in a continuous control domain.},
author = {Dankwa, Stephen and Zheng, Wenfeng},
doi = {10.1145/3387168.3387199},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dankwa, Zheng - 2019 - Twin-Delayed DDPG A Deep Reinforcement Learning Technique to Model a Continuous Movement of an Intelligent Rob(2).pdf:pdf},
isbn = {9781450376259},
journal = {ACM International Conference Proceeding Series},
keywords = {Actor-Critic,Artificial Intelligence,Deep Reinforcement Learning,Twin-Delayed Deep Deterministic Policy Gradient},
number = {August},
title = {{Twin-Delayed DDPG: A Deep Reinforcement Learning Technique to Model a Continuous Movement of an Intelligent Robot Agent}},
year = {2019}
}
@article{Abdolmaleki2018,
abstract = {We introduce a new algorithm for reinforcement learning called Maximum a-posteriori Policy Optimisation (MPO) based on coordinate ascent on a relative-entropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings.},
archivePrefix = {arXiv},
arxivId = {1806.06920},
author = {Abdolmaleki, Abbas and Springenberg, Jost Tobias and Tassa, Yuval and Munos, Remi and Heess, Nicolas and Riedmiller, Martin},
eprint = {1806.06920},
file = {:C$\backslash$:/Users/Andrew/Downloads/1806.06920.pdf:pdf},
journal = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
title = {{Maximum a posteriori policy optimisation}},
year = {2018}
}
@article{Haskell2012,
abstract = {We are interested in risk constraints for discrete time Markov decision processes (MDPs). Starting with the average reward case, we argue that stochastic dominance constraints are natural risk constraints for MDPs. Specifically, we constrain the empirical distribution of reward to dominate a benchmark distribution in the increasing concave stochastic order. We argue that the optimal policy for the dominance-constrained MDP is a stationary randomized policy. Further, the optimal policy can be computed from a linear program in the space of occupation measures, where the dominance constraint is represented by linear inequalities. The dual of this linear program is computed and is shown to be close to the usual linear programming form of the average dynamic programming optimality equations. In our case, a pricing term appears in the dual corresponding to the dominance constraints. We carry out a parallel development for the discounted reward case with stochastic dominance constraints. We also extend this development to cover multivariate stochastic dominance. {\textcopyright} 2012 IEEE.},
author = {Haskell, William B. and Jain, Rahul},
doi = {10.1109/CDC.2012.6426596},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Haskell, Jain - 2012 - Dominance-constrained Markov decision processes.pdf:pdf},
issn = {01912216},
journal = {Proceedings of the IEEE Conference on Decision and Control},
pages = {5991--5996},
title = {{Dominance-constrained Markov decision processes}},
year = {2012}
}
@article{Wen2021,
abstract = {We study a safe reinforcement learning problem, in which the constraints are defined as the expected cost over finite-length trajectories. We propose a constrained cross-entropy-based method to solve this problem. The key idea is to transform the original constrained optimization problem into an unconstrained one with a surrogate objective. The method explicitly tracks its performance with respect to constraint satisfaction and thus is well suited for safety-critical applications. We show that the asymptotic behavior of the proposed algorithm can be almost-surely described by that of an ordinary differential equation. Then, we give sufficient conditions on the properties of this differential equation for the convergence of the proposed algorithm. At last, we show the performance of the proposed algorithm in two simulation examples. In a constrained linear-quadratic regulator example, we observe that the algorithm converges to the global optimum with high probability. In a 2-D navigation example, we find that the algorithm effectively learns feasible policies without assumptions on the feasibility of initial policies, even with non-Markovian objective functions and constraint functions.},
author = {Wen, Min and Topcu, Ufuk},
doi = {10.1109/TAC.2020.3015931},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wen, Topcu - 2021 - Constrained cross-entropy method for safe reinforcement learning.pdf:pdf},
isbn = {0001417126},
issn = {15582523},
journal = {IEEE Transactions on Automatic Control},
keywords = {Machine learning algorithms,Safe reinforcement learning,Statistical learning},
number = {7},
pages = {3123--3137},
publisher = {IEEE},
title = {{Constrained cross-entropy method for safe reinforcement learning}},
volume = {66},
year = {2021}
}
@article{Singh2019,
abstract = {The combination of deep neural network models and reinforcement learning algorithms can make it possible to learn policies for robotic behaviors that directly read in raw sensory inputs, such as camera images, effectively subsuming both estimation and control into one model. However, real-world applications of reinforcement learning must specify the goal of the task by means of a manually programmed reward function, which in practice requires either designing the very same perception pipeline that end-to-end reinforcement learning promises to avoid, or else instrumenting the environment with additional sensors to determine if the task has been performed successfully. In this paper, we propose an approach for removing the need for manual engineering of reward specifications by enabling a robot to learn from a modest number of examples of successful outcomes, followed by actively solicited queries, where the robot shows the user a state and asks for a label to determine whether that state represents successful completion of the task. While requesting labels for every single state would amount to asking the user to manually provide the reward signal, our method requires labels for only a tiny fraction of the states seen during training, making it an efficient and practical approach for learning skills without manually engineered rewards. We evaluate our method on real-world robotic manipulation tasks where the observations consist of images viewed by the robot's camera. In our experiments, our method effectively learns to arrange objects, place books, and drape cloth, directly from images and without any manually specified reward functions, and with only 1-4 hours of interaction with the real world. Videos of learned behavior are available at sites.google.com/view/reward-learning-rl/.},
archivePrefix = {arXiv},
arxivId = {1904.07854},
author = {Singh, Avi and Yang, Larry and Finn, Chelsea and Levine, Sergey},
doi = {10.15607/rss.2019.xv.073},
eprint = {1904.07854},
file = {:C$\backslash$:/Users/Andrew/Downloads/p73.pdf:pdf},
isbn = {9780992374754},
issn = {2330765X},
keywords = {Deep Learning in Robotics and Automation},
title = {{End-To-End Robotic Reinforcement Learning without Reward Engineering}},
url = {http://www.roboticsproceedings.org/rss15/p73.pdf},
year = {2019}
}
@article{Wang2016,
abstract = {In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.},
archivePrefix = {arXiv},
arxivId = {1511.06581},
author = {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and {Van Hasselt}, Hado and Lanctot, Marc and {De Frcitas}, Nando},
isbn = {9781510829008},
journal = {33rd International Conference on Machine Learning, ICML 2016},
number = {9},
pages = {2939--2947},
title = {{Dueling Network Architectures for Deep Reinforcement Learning}},
volume = {4},
year = {2016}
}

@article{Mnih2016,
abstract = {Wc propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
archivePrefix = {arXiv},
arxivId = {1602.01783},
author = {Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Lehdi and Graves, Alex and Harley, Tim and Lillicrap, Timothy P. and Silver, David and Kavukcuoglu, Koray},
eprint = {1602.01783},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih et al. - 2016 - Asynchronous methods for deep reinforcement learning(2).pdf:pdf},
isbn = {9781510829008},
journal = {33rd International Conference on Machine Learning, ICML 2016},
pages = {2850--2869},
title = {{Asynchronous methods for deep reinforcement learning}},
volume = {4},
year = {2016}
}
@article{Hou2019,
abstract = {Experience replay plays an important role in reinforcement learning. It reuses previous experiences to prevent the input data from being highly correlated. Recently , a deep reinforcement learning algorithm with experience replay, called deep deterministic policy gradient (DDPG), has achieved good performance in many continuous control tasks. However, it assumes the experiences are of equal importance and samples them uniformly, which obviously neglects the difference in the value of each individual experience. To improve the efficiency of experience replay in DDPG method, we propose to replace the original uniform experience replay with prioritized experience replay. We test the algorithms in five tasks in the OpenAI Gym, a testbed for reinforcement learning algorithms. In the experiment, we find that DDPG with prioritized experience replay mechanism significantly outperforms that with uniform sampling in terms of training time, training stability and final performance. We also find that our algorithm can achieve better performance in three tasks compared with some state-of-the-art algorithms like Q-prop, which directly proves the effectiveness of our proposed scheme. Our weekly report and codes are available at https://github.com/cardwing/Codes-for-RL-Project.},
author = {Hou, Yuenan and Zhang, Yi},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hou, Zhang - 2019 - Improving DDPG via Prioritized Experience Replay(2).pdf:pdf},
title = {{Improving DDPG via Prioritized Experience Replay}},
url = {https://github.com/cardwing/Codes-for-RL-Project.},
year = {2019}
}
@article{Hafner2020,
abstract = {Intelligent agents need to generalize from past experience to achieve goals in complex environments. World models facilitate such generalization and allow learning behaviors from imagined outcomes to increase sample-efficiency. While learning world models from image inputs has recently become feasible for some tasks, modeling Atari games accurately enough to derive successful behaviors has remained an open challenge for many years. We introduce DreamerV2, a reinforcement learning agent that learns behaviors purely from predictions in the compact latent space of a powerful world model. The world model uses discrete representations and is trained separately from the policy. DreamerV2 constitutes the first agent that achieves human-level performance on the Atari benchmark of 55 tasks by learning behaviors inside a separately trained world model. With the same computational budget and wall-clock time, Dreamer V2 reaches 200M frames and surpasses the final performance of the top single-GPU agents IQN and Rainbow. DreamerV2 is also applicable to tasks with continuous actions, where it learns an accurate world model of a complex humanoid robot and solves stand-up and walking from only pixel inputs.},
archivePrefix = {arXiv},
arxivId = {2010.02193},
author = {Hafner, Danijar and Lillicrap, Timothy and Norouzi, Mohammad and Ba, Jimmy},
eprint = {2010.02193},
file = {:C$\backslash$:/Users/Andrew/Downloads/2010.02193.pdf:pdf},
pages = {1--26},
title = {{Mastering Atari with Discrete World Models}},
url = {http://arxiv.org/abs/2010.02193},
year = {2020}
}
@article{Meng2020,
abstract = {Multi-step (also called n-step) methods in Reinforcement Learning (RL) have been shown to be more efficient than the 1-step method due to faster propagation of the reward signal, both theoretically and empirically, in tasks exploiting tabular representation of the value-function. Recently, research in Deep Reinforcement Learning (DRL) also shows that multi-step methods improve learning speed and final performance in applications where the value-function and policy are represented with deep neural networks. However, there is a lack of understanding about what is actually contributing to the boost of performance. In this work, we analyze the effect of multi-step methods on alleviating the overestimation problem in DRL, where multi-step experiences are sampled from a replay buffer. Specifically building on top of Deep Deterministic Policy Gradient (DDPG), we propose Multi-step DDPG (MDDPG), where different step sizes are manually set, and a variant called Mixed Multi-step DDPG (MMDDPG) where an average over different multi-step backups is used as an update target for the Q-value function. Empirically, we show that both MDDPG and MMDDPG are significantly less affected by the overestimation problem than DDPG with 1-step backup, which consequently results in better final performance and learning speed. We also discuss the advantages and disadvantages of different ways to do multi-step expansion in order to reduce approximation error, and expose the tradeoff between overestimation and underestimation that underlies offline multi-step methods. Finally, we compare the computational resource needs of MDDPG and MMDDPG with those of Twin Delayed Deep Deterministic Policy Gradient (TD3), a state-of-the-art algorithm proposed to address overestimation in actor-critic methods, since they show comparable final performance and learning speed.},
archivePrefix = {arXiv},
arxivId = {2006.12692},
author = {Meng, Lingheng and Gorbet, Rob and Kuli{\'{c}}, Dana},
doi = {10.1109/ICPR48806.2021.9413027},
eprint = {2006.12692},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Meng, Gorbet, Kuli{\'{c}} - 2020 - The effect of multi-step methods on overestimation in deep reinforcement learning.pdf:pdf},
isbn = {9781728188089},
issn = {10514651},
journal = {Proceedings - International Conference on Pattern Recognition},
pages = {9740--9747},
title = {{The effect of multi-step methods on overestimation in deep reinforcement learning}},
year = {2020}
}
@article{Chang2021,
abstract = {The lack of stability guarantee restricts the practical use of learning-based methods in core control problems in robotics. We develop new methods for learning neural control policies and neural Lyapunov critic functions in the model-free reinforcement learning (RL) setting. We use sample-based approaches and the Almost Lyapunov function conditions to estimate the region of attraction and invariance properties through the learned Lyapunov critic functions. The methods enhance stability of neural controllers for various nonlinear systems including automobile and quadrotor control.},
archivePrefix = {arXiv},
arxivId = {2107.04989},
author = {Chang, Ya Chien and Gao, Sicun},
doi = {10.1109/ICRA48506.2021.9560886},
eprint = {2107.04989},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chang, Gao - 2021 - Stabilizing Neural Control Using Self-Learned Almost Lyapunov Critics(2).pdf:pdf},
isbn = {9781728190778},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
number = {Section V},
pages = {1803--1809},
title = {{Stabilizing Neural Control Using Self-Learned Almost Lyapunov Critics}},
url = {https://yachienchang.github.io/ICRA2021/index.html},
volume = {2021-May},
year = {2021}
}
@article{Schrum2018,
abstract = {Tetris is a challenging puzzle game that has received much attention from the AI community, but much of this work relies on intelligent high-level features. Recently, agents played the game using low-level features (10 × 20 board) as input to fully connected neural networks evolved with the indirect encoding HyperNEAT. However, research in deep learning indicates that convolutional neural networks (CNNs) are superior to fully connected networks in processing visuospatial inputs. Therefore, this paper uses HyperNEAT to evolve CNNs. The results indicate that CNNs are indeed superior to fully connected neural networks in Tetris, and identify several factors that influence the successful evolution of indirectly encoded CNNs.},
author = {Schrum, Jacob},
doi = {10.1145/3205455.3205459},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schrum - 2018 - Evolving indirectly encoded convolutional neural networks to play tetris with low-level features.pdf:pdf},
isbn = {9781450356183},
journal = {GECCO 2018 - Proceedings of the 2018 Genetic and Evolutionary Computation Conference},
keywords = {Games,Indirect encoding,Neural networks,Tetris},
pages = {205--212},
title = {{Evolving indirectly encoded convolutional neural networks to play tetris with low-level features}},
url = {https://dl.acm.org/doi/pdf/10.1145/3205455.3205459},
year = {2018}
}
@article{Szita2006,
abstract = {The cross-entropy method is an efficient and general optimization algorithm. However, its applicability in reinforcement learning (RL) seems to be limited because it often converges to suboptimal policies. We apply noise for preventing early convergence of the cross-entropy method, using Tetris, a computer game, for demonstration. The resulting policy outperforms previous RL algorithms by almost two orders of magnitude. {\textcopyright} 2006 Massachusetts Institute of Technology.},
author = {Szita, Istv{\'{a}}n and Lorincz, Andr{\'{a}}s},
doi = {10.1162/neco.2006.18.12.2936},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Szita, Lorincz - 2006 - Learning tetris using the noisy cross-entropy method.pdf:pdf},
issn = {08997667},
journal = {Neural Computation},
number = {12},
pages = {2936--2941},
pmid = {17052153},
title = {{Learning tetris using the noisy cross-entropy method}},
url = {https://watermark.silverchair.com/neco.2006.18.12.2936.pdf?token=AQECAHi208BE49Ooan9kkhW{\_}Ercy7Dm3ZL{\_}9Cf3qfKAc485ysgAAArYwggKyBgkqhkiG9w0BBwagggKjMIICnwIBADCCApgGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQM8RUw6e2j4RwgTgzRAgEQgIICaZl{\_}OKOE1HMoAHZo6WzP6cKZvnvtKFCPi6},
volume = {18},
year = {2006}
}
@article{Tsitsiklis1996,
abstract = {We develop a methodological framework and present a few different ways in which dynamic programming and compact representations can be combined to solve large scale stochastic control problems In particular, we develop algorithms that employ two types of feature based compact representations; that is, representations that involve feature extraction and a relatively simple approximation architecture. We prove the convergence of these algorithms and provide hounds on the approximation error. As an example, one of these algorithms is used to generate a strategy for the game of Tetris Furthermore, we provide a counter-example illustrating the difficulties of integrating compact representations with dynamic programming, which exemplifies the shortcomings of certain simple approaches. {\textcopyright} 1996 Kluwer Academic Publishers,.},
author = {Tsitsiklis, John N. and {Van Roy}, Benjamin},
doi = {10.1007/BF00114724},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tsitsiklis, Van Roy - 1996 - Feature-based methods for large scale dynamic programming.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {Compact representation,Curse of dimensionality,Dynamic programming,Features,Function approximation,Neuro-dynamic programming,Reinforcement learning},
number = {1-3},
pages = {59--94},
title = {{Feature-based methods for large scale dynamic programming}},
url = {https://www.mit.edu/{~}jnt/Papers/J060-96-bvr-feature.pdf},
volume = {22},
year = {1996}
}
@article{Bohm2005,
author = {B{\"{o}}hm, N and K{\'{o}}okai, G and Mandl, S},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/B{\"{o}}hm, K{\'{o}}okai, Mandl - 2005 - An Evolutionary Approach to Tetris.pdf:pdf},
isbn = {2959977688},
issn = {00222429},
journal = {Proceedings of the 6th Metaheuristics International Conference (MIC2005)},
pages = {137--148},
title = {{An Evolutionary Approach to Tetris}},
url = {https://faui20a.informatik.uni-erlangen.de/publication/download/mic.pdf},
year = {2005}
}
@article{Carr2005,
abstract = {This paper investigates the possible application of reinforcement learn-ing to Tetris. The author investigates the background of Tetris, and qual-ifies it in a mathematical context. The author discusses reinforcement learning, and considers historically successful applications of it. Finally the author discusses considerations surrounding implementation.},
annote = {NP complete - impossible to search through entire policy space
Tetris satisfies markov property

Reason for reinforcement learning: Adjust policy through changing its value function during the game
Higher resolution than genetic algorithms

One piece - Dellacherie
OR 2 piece algorithms - Fahey

Look up conditions for eternal play

Melax - reduced tetris
Driessens - full tetris, relational reinforcement learning

Reward funciton: Based on working height
Issue - steer development of agents policy

Stochastic nature of tetris limits q-learning algorithm ability to determine future rewards

THIS PAPER WAS WRITTEN NEURAL NETWORKS WERE USED FOR FUNCTION APPROXIMAITON!

Punishment/ scoring:
Punish for increase in working height: agent completes rows but does not stack effectively},
author = {Carr, Donald},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Carr - 2005 - Applying reinforcement learning to Tetris.pdf:pdf},
journal = {Business},
pages = {1--15},
title = {{Applying reinforcement learning to Tetris}},
year = {2005}
}
@article{Lundgaard2007,
abstract = {This paper presents the results of experiments carried out with the goal of applying the machine learning techniques of reinforcement learning and neural networks with reinforcement learning to the game of Tetris. Tetris is a well-known computer game that can be played either by a single player or competitively with slight variations, toward the end of accumulating a high score or defeating the opponent. The fundamental hypothesis of this paper is that if the points earned in Tetris are used as the reward function for a machine learning agent, then that agent should be able to learn to play Tetris without other supervision. Toward this end, a state-space that summarizes the essential feature of the Tetris board is designed, high-level actions are developed to interact with the game, and agents are trained using Q-Learning and neural networks. As a result of these efforts, agents learn to play Tetris and to compete with other players. While the learning agents fail to accumulate as many points as the most advanced AI agents, they do learn to play more efficiently.},
annote = {Tetris: NP complete
No formal winning strategy
Look up: Colin Fahey tetris agent and tetris implementation
Look up: Pierre Dellacherie
Look up: Siegel, chaffee
Research question: Maximise points over lines cleared

Relational reinforcement learning might be useful

Goal: Develop agent that clears lines simultaneously - competitive tetris
Normal scoring funciton - agents imploys multi-line clearing strategy

State representation:
Raw: Track configurations of occupied/ unoccupied cells
Problem - size + meaningfulness
Exact configuration of board below top layer may be irrelevent
High level State representations:
Reduce size of state space

Actions: Configuration independent - high level strategy
-minimise holes, etc - represent different evaluation functions

Agents:
Random
Heuristic
Reinforcement learning - Q learning - riskier than sarsa

Neural network using screen pixels - takes too long to converge to be useful
- doesnt perform any better than random agent},
author = {Lundgaard, Nicholas and Mckee, Brian},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lundgaard, Mckee - 2007 - Reinforcement Learning and Neural Networks for Tetris.pdf:pdf},
title = {{Reinforcement Learning and Neural Networks for Tetris}},
url = {http://www.colinfahey.com/tetris},
year = {2007}
}
@article{Thiery2009a,
annote = {Note: ALgorithms are difficult to compare - niose + different tetris implementations

Tetris domain reinforcement learning competition!

Goal: Maximise average number of lines scored per game

Tetris as a test bed: lambda policy iteration algorithm by vertsekas and tsitsiklis!

See table of features!

Implementation: Drop piece from teh top

See notes on confidence interval!

Their method:
Cross entropy, based on Szita and Lorinez using Dellacherie features
Performs after-state evaluation 

2 piece vs 1 piece controllers: Reinforcement learning approaches so far use 2 piece},
author = {Thiery, Christophe and Scherrer, Bruno and Thiery, Christophe and Scherrer, Bruno and Controllers, Building and Computer, International},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thiery et al. - 2009 - Building Controllers for Tetris.pdf:pdf},
pages = {3--11},
title = {{Building Controllers for Tetris}},
url = {https://hal.inria.fr/inria-00418954/document},
year = {2009}
}
@article{BluePlanetSoftware2009,
author = {{Blue Planet Software}},
file = {::},
keywords = {Design,Guideline,Tetris},
title = {{2009 Tetris Design Guideline}},
url = {https://tetris.fandom.com/wiki/Tetris{\_}Guideline},
year = {2009}
}
@article{Gillespie2017,
abstract = {Intelligent agents have a wide range of applications in robotics, video games, and computer simulations. However, fully general agents should function with as little human guidance as possible. Specifically, agents should learn from large collections of raw state variables instead of small collections of hand-designed features. Learning from raw state variables is difficult, but can be easier when agents are aware of the geometry of the input space. Indirect encodings allow agents to take advantage of the geometry of the task, and scale up to large input spaces. This research demonstrates the relative benefits of a direct and indirect encoding using raw or hand-designed features in Tetris, a challenging video game. Specifically, the direct encoding NEAT is compared against the indirect encoding HyperNEAT. Both algorithms create neural networks to play the game, but HyperNEAT makes better use of raw screen inputs, due to its ability to generate large networks that take advantage of the domain's geometry However, hand-designed features lead to higher scores with both algorithms. HyperNEAT makes better use of hand-designed features early in evolution, but NEAT eventually overtakes it. Since each method succeeds in different circumstances, approaches combining the strengths of both should be explored.},
annote = {Evolutionary method for adjusting neural networks
- neurons dont just update - the network can gain neurons},
author = {Gillespie, Lauren E. and Gonzalez, Gabriela R. and Schram, Jacob},
doi = {10.1145/3071178.3071195},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gillespie, Gonzalez, Schram - 2017 - Comparing direct and indirect encodings using both raw and hand-designed features in Tetris.pdf:pdf},
isbn = {9781450349208},
journal = {GECCO 2017 - Proceedings of the 2017 Genetic and Evolutionary Computation Conference},
keywords = {Games,Indirect encoding,Neural networks,Tetris},
pages = {179--186},
title = {{Comparing direct and indirect encodings using both raw and hand-designed features in Tetris}},
url = {https://dl.acm.org/doi/pdf/10.1145/3071178.3071195},
year = {2017}
}
@article{Thiery2009,
abstract = {For playing the game of Tetris well, training a controller by the cross-entropy method seems to be a viable way (Szita and Lorincz, 2006; Thiery and Scherrer, 2009). We consider this method to tune an evaluation-based one-piece controller as suggested by Szita and Lorincz and we introduce some improvements. In this context, we discuss the influence of the noise, and we perform experiments with several sets of features such as those introduced by Bertsekas and Tsitsiklis (1996), by Dellacherie (Fahey, 2003), and some original features. This approach leads to a controller that outperforms the previous known results. On the original game of Tetris, we show that with probability 0.95 it achieves at least 910,000 ± 5{\%} lines per game on average. On a simplified version of Tetris considered by most research works, it achieves 35,000,000 ± 20{\%} lines per game on average. We used this approach when we took part with the program BCTS in the 2008 Tetris domain Reinforcement Learning Competition and won the competition.},
annote = {1 piece controller
Drops piece from top

Good description of delacherie's controller!},
author = {Thiery, Christophe and Scherrer, Bruno},
doi = {10.3233/ICG-2009-32104},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thiery, Scherrer - 2009 - Improvements on learning tetris with cross entropy.pdf:pdf},
issn = {13896911},
journal = {ICGA Journal},
number = {1},
pages = {23--33},
title = {{Improvements on learning tetris with cross entropy}},
url = {https://hal.inria.fr/inria-00418930/document},
volume = {32},
year = {2009}
}
@article{algorta2009,
annote = {Reduce grid size to 10x10 to make game shorter without compromising the nature of the game

Tetris not part of atari domain or openAI universe
No deep learning algorithm has learned to play well from raw inputs},
archivePrefix = {arXiv},
arxivId = {arXiv:1905.01652v2},
author = {{Ozgur Simsek}, Simon Algorta},
eprint = {arXiv:1905.01652v2},
file = {::},
title = {{The Game of Tetris in Machine Learning ¨}},
year = {2019}
}
@article{Amundsen2014,
abstract = {In the recent past letter recognition by the human visual system was an intensely debated topic. Common theories included templates (Holbrook, 1975), spatial frequency (Gervais et. al. 1984), and geometric features (Gibson, 1969). While none of these main theories has been discounted, the spatial frequency model of letter recognition has experienced the most recent success achieving a correlation of r = .70 for predicted and actual confusions. The present work revived this topic and explored prediction potential of feature-based recognition models when features identified by different models are combined and reaction time to respond same or different to a letter pair is measured. Aggregated grand means and averaged median subject data yielded correlation values of r = .60 and r = .66 respectively, though performance was only slightly lower (r = .64) for one of the models alone. Future directions this research might take are also considered.},
author = {Amundsen, Jonas Balgaard},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Amundsen - 2014 - A comparison of feature functions for Tetris strategies.pdf:pdf},
number = {June},
pages = {78},
title = {{A comparison of feature functions for Tetris strategies}},
year = {2014}
}
@article{Gabillon2013,
abstract = {Tetris is a video game that has been widely used as a benchmark for various optimization techniques including approximate dynamic programming (ADP) algorithms. A look at the literature of this game shows that while ADP algorithms that have been (almost) entirely based on approximating the value function (value function based) have performed poorly in Tetris, the methods that search directly in the space of policies by learning the policy parameters using an optimization black box, such as the cross entropy (CE) method, have achieved the best reported results. This makes us conjecture that Tetris is a game in which good policies are easier to represent, and thus, learn than their corresponding value functions. So, in order to obtain a good performance with ADP, we should use ADP algorithms that search in a policy space, instead of the more traditional ones that search in a value function space. In this paper, we put our conjecture to test by applying such an ADP algorithm, called classification-based modified policy iteration (CBMPI), to the game of Tetris. Our experimental results show that for the first time an ADP algorithm, namely CBMPI, obtains the best results reported in the literature for Tetris in both small 10 × 10 and large 10 × 20 boards. Although the CBMPI's results are similar to those of the CE method in the large board, CBMPI uses considerably fewer (almost 1/6) samples (calls to the generative model) than CE.},
author = {Gabillon, Victor and Ghavamzadeh, Mohammad and Scherrer, Bruno},
file = {::},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {1--9},
title = {{Approximate dynamic programming finally performswell in the game of Tetris}},
year = {2013}
}
@article{Galway2008,
abstract = {Artificial intelligence for digital games constitutes the implementation of a set of algorithms and techniques from both traditional and modern artificial intelligence in order to provide solutions to a range of game dependent problems. However, the majority of current approaches lead to predefined, static and predictable game agent responses, with no ability to adjust during game-play to the behaviour or playing style of the player. Machine learning techniques provide a way to improve the behavioural dynamics of computer controlled game agents by facilitating the automated generation and selection of behaviours, thus enhancing the capabilities of digital game artificial intelligence and providing the opportunity to create more engaging and entertaining game-play experiences. This paper provides a survey of the current state of academic machine learning research for digital game environments, with respect to the use of techniques from neural networks, evolutionary computation and reinforcement learning for game agent control. {\textcopyright} 2009 Springer Science+Business Media B.V.},
author = {Galway, Leo and Charles, Darryl and Black, Michaela},
doi = {10.1007/s10462-009-9112-y},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Galway, Charles, Black - 2008 - Machine learning in digital games A survey.pdf:pdf},
issn = {02692821},
journal = {Artificial Intelligence Review},
keywords = {Computational intelligence,Digital games,Game aI,Machine learning},
number = {2},
pages = {123--161},
title = {{Machine learning in digital games: A survey}},
url = {https://link.springer.com/content/pdf/10.1007/s10462-009-9112-y.pdf},
volume = {29},
year = {2008}
}
@article{Bertsekas1997,
abstract = {We introduce a new policy iteration method for dynamic programming problems with dis-counted and undiscounted cost. The method is based on the notion of temporal differences, and is primarily geared to the case of large and complex problems where the use of approximations is essential. We develop the theory of the method without approximation, we describe how to em-bed it within a neuro-dynamic programming/reinforcement learning context where feature-based approximation architectures are used, we relate it to TD($\lambda$) methods, and we illustrate its use in the training of a tetris playing program.},
annote = {See feature vector!!!},
author = {Bertsekas, Dimitri P and Ioffe, Sergey},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bertsekas, Ioffe - 1997 - Temporal Differences-Based Policy Iteration and Applications in Neuro-Dynamic Programming.pdf:pdf},
journal = {Electrical Engineering},
number = {August 1996},
pages = {1--19},
title = {{Temporal Differences-Based Policy Iteration and Applications in Neuro-Dynamic Programming}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.7759{\&}rep=rep1{\&}type=pdf},
volume = {1996},
year = {1997}
}
@article{Stevens2016,
abstract = {We used deep reinforcement learning to train an AI to play tetris using an approach similar to [7]. We use a con-volutional neural network to estimate a Q function that describes the best action to take at each game state. This approach failed to converge when directly applied to predicting individual actions with no help from heuristics. However , we implemented many features that improved convergence. By grouping actions together, we allowed the Q network to focus on learning good game configurations instead of how to move pieces. By using transfer learning from a heuristic model we were able to greatly improve performance , having already learned relevant features about Tetris and optimal piece configurations. And by using prioritized sweeping, we were able to reduce the some of the inherent instability in learning to estimate single actions. Although we did not surpass explicitly featurized agents in terms of performance, we demonstrated the basic ability of a neural network to learn to play Tetris, and with greater training time we might have been able to beat even those benchmarks.},
annote = {Method:
Previous methods - explicit feature vectorisation
This attempt - raw pixel data
Use heuristic function to help train agent
Q learning
Agent is sensitive to the way in which state is represented
Neural network - domain specific knowledge of tetris - collapses each column into single pixel
Bellman equation uses cached version of neural network - updates every 1000 iterations to prevent instability

State representation:
Image of current game screen - pixel format
Crop screen to reduce state size in memory

Actions:
Single move - actions are reversible, difficult to determine which move contributed to reward + add noise
OR grouped - render image of final piece placement - feed that into neural network, which determined rewards - siginificantly improve results
Reward must propagate through chain of actions

Scoring function:
Standard scoring rules - MaTris library - number of lines cleared squared
Game-over penalty improved convergence
Stop oscillatory moves - penalty to move piece
OR: Heuristic function
More meaningful rewards, fitness funciton based on height + bumpiness: 
Reward = change in fitness function

Game implementation:
MaTris from python pygame library

Exploration rate/ policy:
Ebsilon greedy
Anneal from 1 to 0.1 over 1 million frames

Training strategy:
Off policy learning - implement already trained agent to increase initial rewards
Switch between neural network policy and trained agent policy using ebsilon

Experience replay:
Network trains on off policy results - different action distributions for previous states since policy changes

Prioritized sweeping:
Train network on experiences for which it had the highest error

Evaluation metrics:
Evaluate gameplay - game lenght + game score

Experiment setup:
RMSProp to optimise neural network
Gamma-discount rate small enough to reward only moves that contributed to clearing a line

Problems:
Time lag between action, reward},
author = {Stevens, Matt and Pradhan, Sabeek},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Stevens, Pradhan - 2016 - Playing Tetris with Deep Reinforcement Learning.pdf:pdf},
title = {{Playing Tetris with Deep Reinforcement Learning}},
year = {2016}
}
@article{Kakade2002,
abstract = {We provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the parameter space. Alt hough gradient methods cannot make large changes in the values of the parameters, we show that the natural gradient is moving toward choosing a greedy optimal act ion rat her than just a better action. These greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate, compatible value functions, as defined by Sutton et al. [9|. We then show drastic performance improvements in simple MDPs and in the more challenginwith g MDP of Tetris.},
author = {Kakade, Sham},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kakade - 2002 - A natural policy gradient.pdf:pdf},
isbn = {0262042088},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
title = {{A natural policy gradient}},
year = {2002}
}
@article{Thiam2014,
abstract = {In this paper we investigate reinforcement learning approaches for the popular computer game Tetris. User-defined reward functions have been applied to T D(0) learning based on $\epsilon$-greedy strategies in the standard Tetris scenario. The numerical experiments show that reinforcement learning can significantly outperform agents utilizing fixed policies.},
annote = {Method: 
TD(0) learning, with state values

Reward function:
Tetris cannot be won - dont give reward at end of game
Reward at any time step
Linear combinations of weighted features
features = highest column, average column height, holes, uneveness of profile

State encoding: Height difference between adjacent columns - limit height difference to +-3, truncate
State space still equals 40 milion states!

Training: 
Time measured in terms of number of pieces dropped
Learning steps vs played tetrimones per game
410 000 games
3.5*10{\^{}}9 moves

Results:
2nd value function performed better than 1st value function},
author = {Thiam, Patrick and Kessler, Viktor and Schwenker, Friedhelm},
doi = {10.1007/978-3-319-11656-3_15},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thiam, Kessler, Schwenker - 2014 - A reinforcement learning algorithm to train a tetris playing agent.pdf:pdf},
isbn = {9783319116556},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {165--170},
title = {{A reinforcement learning algorithm to train a tetris playing agent}},
volume = {8774},
year = {2014}
}
@article{Groß2008,
abstract = {In this paper the application of reinforcement learning to Tetris is investigated, particulary the idea of temporal difference learning is applied to estimate the state value function V. For two predefined reward functions Tetris agents have been trained by using a {\_}-greedy policy. In the numerical experiments it can be observed that the trained agents can outperform fixed policy agents significantly, e.g. by factor 5 for a complex reward function.},
annote = {Method: Temporal difference learning to approximate Value function - TABULAR!

Value function: Tabular, only look at top 2 rows
- Divided playing field into larger sections - columns and rows

Tetris cant be won - reward at end of game is weak

Performed better than fixed policy},
author = {Gro{\ss}, Alexander and Friedland, Jan and Schwenker, Friedhelm},
file = {:C$\backslash$:/Users/Andrew/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gro{\ss}, Friedland, Schwenker - 2008 - Learning to play Tetris applying reinforcement learning methods.pdf:pdf},
isbn = {2930307080},
journal = {ESANN 2008 Proceedings, 16th European Symposium on Artificial Neural Networks - Advances in Computational Intelligence and Learning},
number = {April},
pages = {131--136},
title = {{Learning to play Tetris applying reinforcement learning methods}},
url = {https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.218.4954{\&}rep=rep1{\&}type=pdf},
year = {2008}
}
@misc{klaver,
  author = {Frits Klaver},
  title = {{The economic and social impacts of fully autonomous vehicles}},
  url = {https://www.compact.nl/en/articles/the-economic-and-social-impacts-of-fully-autonomous-vehicles/},
  year = {2020}, 
  note = "[Online; accessed 30-June-2022]"
}
@article{Barab_s_2017,
	doi = {10.1088/1757-899x/252/1/012096},
	url = {https://doi.org/10.1088/1757-899x/252/1/012096},
	year = 2017,
	month = {oct},
	publisher = {{IOP} Publishing},
	volume = {252},
	pages = {012096},
	author = {I Barab{\'{a}}s and A Todoru{\c{t}} and N Cordo{\c{s}} and A Molea},
	title = {Current challenges in autonomous driving},
	journal = {{IOP} Conference Series: Materials Science and Engineering},
	abstract = {Nowadays the automotive industry makes a quantum shift to a future, where the driver will have smaller and smaller role in driving his or her vehicle ending up being totally excluded. In this paper, we have investigated the different levels of driving automatization, the prospective effects of these new technologies on the environment and traffic safety, the importance of regulations and their current state, the moral aspects of introducing these technologies and the possible scenarios of deploying the autonomous vehicles. We have found that the self-driving technologies are facing many challenges: a) They must make decisions faster in very diverse conditions which can include many moral dilemmas as well; b) They have an important potential in reducing the environmental pollution by optimizing their routes, driving styles by communicating with other vehicles, infrastructures and their environment; c) There is a considerable gap between the self-drive technology level and the current regulations; fortunately, this gap shows a continuously decreasing trend; d) In case of many types of imminent accidents management there are many concerns about the ability of making the right decision. Considering that this field has an extraordinary speed of development, our study is up to date at the submission deadline. Self-driving technologies become increasingly sophisticated and technically accessible, and in some cases, they can be deployed for commercial vehicles as well. According to the current stage of research and development, it is still unclear how the self-driving technologies will be able to handle extreme and unexpected events including their moral aspects. Since most of the traffic accidents are caused by human error or omission, it is expected that the emergence of the autonomous technologies will reduce these accidents in their number and gravity, but the very few currently available test results have not been able to scientifically underpin this issue yet. The increasing trend in automation of vehicles will radically change the composition of car industry players, as mechatronics will not only be a complementary part of the automobile industry but an indispensable part of it. There is a reasonable expectation that automated cars will perform the same or better in all respects than their conventional counterparts. However, it seems that the current regulations do not keep up with the development of technology and sometimes hinder the development and testing of autonomous technologies.}
}

@INPROCEEDINGS{Hewing2018,  
author={Hewing, Lukas and Liniger, Alexander and Zeilinger, Melanie N.},  
booktitle={2018 European Control Conference (ECC)},   
title={Cautious NMPC with Gaussian Process Dynamics for Autonomous Miniature Race Cars},   
year={2018},  
volume={},  
number={},  
pages={1341-1348},  
doi={10.23919/ECC.2018.8550162},
url={https://doi.org/10.23919/ECC.2018.8550162}
}

@article{Sharp2016,
author = {R. S. Sharp and P. Gruber and E. Fina},
title = {Circuit racing, track texture, temperature and rubber friction},
journal = {Vehicle System Dynamics},
volume = {54},
number = {4},
pages = {510-525},
year  = {2016},
publisher = {Taylor & Francis},
doi = {10.1080/00423114.2015.1131308},

URL = { 
        https://doi.org/10.1080/00423114.2015.1131308  
},
eprint = { 
        https://doi.org/10.1080/00423114.2015.1131308
}
}

@misc{Pan_Yungpen2017,
doi = {10.48550/ARXIV.1709.07174},
url = {https://arxiv.org/abs/1709.07174},
author = {Pan, Yunpeng and Cheng, Ching-An and Saigol, Kamil and Lee, Keuntaek and Yan, Xinyan and Theodorou, Evangelos and Boots, Byron},
keywords = {Robotics (cs.RO), FOS: Computer and information sciences, FOS: Computer and information sciences},
title = {Agile Autonomous Driving using End-to-End Deep Imitation Learning},
publisher = {arXiv},
year = {2017},
copyright = {arXiv.org perpetual, non-exclusive license}
}

@book{Plaat_2022,
doi = {10.1007/978-981-19-0638-1},
url = {https://doi.org/10.48550/arXiv.2201.02135},
year = 2022,
publisher = {Springer Nature Singapore},
author = {Aske Plaat},
title = {Deep Reinforcement Learning}
}

@Misc{silver2015,
author = {David Silver},
title = {Lectures on Reinforcement Learning},
howpublished = {\textsc{url:}~\url{https://www.davidsilver.uk/teaching/}},
year = {2015}
}

@InProceedings{silver2014,
  title = 	 {Deterministic Policy Gradient Algorithms},
  author = 	 {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
  pages = 	 {387--395},
  year = 	 {2014},
  editor = 	 {Xing, Eric P. and Jebara, Tony},
  volume = 	 {32},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Bejing, China},
  month = 	 {22--24 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v32/silver14.pdf},
  url = 	 {https://proceedings.mlr.press/v32/silver14.html},
  abstract = 	 {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. Deterministic policy gradient algorithms outperformed their stochastic counterparts in several benchmark problems, particularly in high-dimensional action spaces.}
}

@misc{seita2019, 
title={Model-based Reinforcement Learning:Theory and Practice}, 
url={https://bair.berkeley.edu/blog/2019/12/12/mbpo/}, 
journal={The Berkeley Artificial Intelligence Research Blog}, 
author={Seita, Daniel}, 
year={2019}, 
month={Dec}
} 

@misc{Hong2019,
  doi = {10.48550/ARXIV.1908.06012},
  url = {https://arxiv.org/abs/1908.06012},
  author = {Hong, Zhang-Wei and Pajarinen, Joni and Peters, Jan},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Model-based Lookahead Reinforcement Learning},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@book{Goodfellow2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@misc{Ng2019,
  author        = {Andrew Ng and Kian Katanforoosh },
  title         = {LCS229 Lecture Notes in Deep Learning},
  month         = {July},
  year          = {2019},
  note = {\url{https://cs229.stanford.edu/summer2020/cs229-notes-deep_learning.pdf}},
  publisher={Stanford University}
}

@misc{Ruder2016,
  doi = {10.48550/ARXIV.1609.04747},
  url = {https://arxiv.org/abs/1609.04747},
  author = {Ruder, Sebastian},
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {An overview of gradient descent optimization algorithms},
  publisher = {arXiv},
  year = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{kingma2015,
  doi = {10.48550/ARXIV.1412.6980},
  url = {https://arxiv.org/abs/1412.6980},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Adam: A Method for Stochastic Optimization},
  publisher = {arXiv},
  year = {2014},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{moerland2020,
  doi = {10.48550/ARXIV.2006.16712},
  url = {https://arxiv.org/abs/2006.16712},
  author = {Moerland, Thomas M. and Broekens, Joost and Plaat, Aske and Jonker, Catholijn M.},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Model-based Reinforcement Learning: A Survey},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{nachum2017,
  doi = {10.48550/ARXIV.1702.08892},
  url = {https://arxiv.org/abs/1702.08892},
  author = {Nachum, Ofir and Norouzi, Mohammad and Xu, Kelvin and Schuurmans, Dale},
  keywords = {Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Bridging the Gap Between Value and Policy Based Reinforcement Learning},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{mnih2013,
  doi = {10.48550/ARXIV.1312.5602},
  url = {https://arxiv.org/abs/1312.5602},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Playing Atari with Deep Reinforcement Learning},
  publisher = {arXiv},
  year = {2013},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@phdthesis{lin1993,
  author       = {Long-Ji Lin}, 
  title        = {Reinforcement Learning for Robots using Neural Networks},
  school       = {School of Computer Science, Carnegie Mellon University },
  year        = {1993},
  url = {https://isl.anthropomatik.kit.edu/pdf/Lin1993.pdf}
}

@article{hasselt2015, title={Deep Reinforcement Learning with Double Q-Learning}, volume={30}, url={https://ojs.aaai.org/index.php/AAAI/article/view/10295}, DOI={10.1609/aaai.v30i1.10295}, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={van Hasselt, Hado and Guez, Arthur and Silver, David}, year={2016}, month={Mar.} }

@misc{hsu2022,
  doi = {10.48550/ARXIV.2205.09658},
  url = {https://arxiv.org/abs/2205.09658},
  author = {Hsu, Bo-Jiun and Cao, Hoang-Giang and Lee, I and Kao, Chih-Yu and Huang, Jin-Bo and Wu, I-Chen},
  keywords = {Robotics (cs.RO), Systems and Control (eess.SY), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  title = {Image-Based Conditioning for Action Policy Smoothness in Autonomous Miniature Car Racing with Reinforcement Learning},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Zero v1.0 Universal}
}

@misc{lee2019,
  doi = {10.48550/ARXIV.1811.12555},
  url = {https://arxiv.org/abs/1811.12555},
  author = {Lee, Keuntaek and Wang, Ziyi and Vlahov, Bogdan I. and Brar, Harleen K. and Theodorou, Evangelos A.},
  keywords = {Robotics (cs.RO), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Ensemble Bayesian Decision Making with Redundant Deep Perceptual Control Policies},
  publisher = {arXiv},
  year = {2018},
  copyright = {Creative Commons Attribution 4.0 International}
}

@techreport{Coulter_1992,
author = {R. Craig Coulter},
title = {Implementation of the Pure Pursuit Path Tracking Algorithm},
year = {1992},
month = {January},
institution = {Carnegie Mellon University},
address = {Pittsburgh, PA},
number = {CMU-RI-TR-92-01},
url = {https://www.ri.cmu.edu/publications/implementation-of-the-pure-pursuit-path-tracking-algorithm/}
}

@article{talvala2011,
title = {Pushing the limits: From lanekeeping to autonomous racing},
journal = {Annual Reviews in Control},
volume = {35},
number = {1},
pages = {137-148},
year = {2011},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2011.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S1367578811000101},
author = {Kirstin L.R. Talvala and Krisada Kritayakirana and J. Christian Gerdes},
keywords = {Autonomous racing, Driver assistance system, Lanekeeping, Limit handling, Lyapunov stability, Vehicle dynamics},
abstract = {The success of Electronic Stability Control (ESC) has demonstrated the potential life-saving benefits of vehicle control systems. Lanekeeping presents an obvious next step in vehicle control, but the performance of such systems must be guaranteed before lanekeeping can be viewed as a safety feature. This paper demonstrates that simple lookahead control schemes for lanekeeping are provably robust even at the limits of tire adhesion. By responding to the heading error relative to the desired path, these schemes provide the countersteer behavior necessary to compensate for rear tire saturation and stabilize the vehicle. Using a Lyapunov-based analysis, vehicle stability can be proven even with a highly saturated tire. Taking this a step further by developing a desired path based on the racing line, this lookahead controller can be coupled with longitudinal control based on path position and wheel slip to create an autonomous racecar. The performance of this algorithm shows the potential for lanekeeping control that can truly assist even the best drivers.}
}

@misc{sherif2020,
author = {Nekkah, Sherif and Janus, Josua and Boxheimer, Mario and Ohnemus, Lars and Hirsch, Stefan and Schmidt, Benjamin and Liu, Yuchen and Borbély, David and Keck, Florian and Bachmann, Katharina and Bleszynski, Lukasz},
year = {2020},
title = {The Autonomous Racing Software Stack of the KIT19d}
}

@misc{keefer2022,
  author = {Keefer, Elizabeth and Bryan, William and Bevly, David},
  title = {International conference for robotics and automation},
  title  = {Efficient Graph-Based Motion Planning for an Autonomous Race Car},
  year = {2022}
}


@misc{alvarez2022,
  doi = {10.48550/ARXIV.2210.10933},
  url = {https://arxiv.org/abs/2210.10933},
  author = {Alvarez, Andres and Denner, Nico and Feng, Zhe and Fischer, David and Gao, Yang and Harsch, Lukas and Herz, Sebastian and Large, Nick Le and Nguyen, Bach and Rosero, Carlos and Schaefer, Simon and Terletskiy, Alexander and Wahl, Luca and Wang, Shaoxiang and Yakupova, Jonona and Yu, Haocen},
  keywords = {Robotics (cs.RO), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {The Software Stack That Won the Formula Student Driverless Competition},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{f1tenth,
  author = {{F1tenth Foundation}},
  title = {F1tenth},
  year = 2020,
  url = {https://f1tenth.org/},
  urldate = {2020-09-28},
  note = "[Online; accessed 9-September-2022]"
}

@article{Osa_2018,
	doi = {10.1561/2300000053},
	url = {https://doi.org/10.1561/2300000053},
	year = 2018,
	publisher = {Now Publishers},
	volume = {7},
	number = {1-2},
	pages = {1--179},
	author = {Takayuki Osa and Joni Pajarinen and Gerhard Neumann and J. Andrew Bagnell and Pieter Abbeel and J. Peters},
	title = {An Algorithmic Perspective on Imitation Learning},
	journal = {Foundations and Trends in Robotics}
}

@article{Schwenzer2021,
  author  = {Schwenzer, Max and Muzaffer, Ay and Bergs, Thomas and Abel, Dirk},
  title   = {Review on model predictive control: an engineering perspective},
  journal = {The International Journal of Advanced Manufacturing Technology},
  url     = {https://doi.org/10.1007/s00170-021-07682-3},
  year    = 2021,
  volume  = 117,
  number  = 1,
  pages   = {1327--1349}
}

@INPROCEEDINGS{Anderson2016,  
    author={Anderson, 
    Jeffery R. and Ayalew, Beshah and Weiskircher, T.},  
    booktitle={2016 American Control Conference (ACC)},   
    title={Modeling a professional driver in ultra-high performance maneuvers with a hybrid cost MPC},   
    year={2016},  
    volume={},  
    number={},  
    pages={1981-1986},  
    doi={10.1109/ACC.2016.7525209},
    url={https://doi.org/10.1109/ACC.2016.7525209}
}

@misc{Kalaria2021,
  doi = {10.48550/ARXIV.2109.07105},
  url = {https://arxiv.org/abs/2109.07105},
  author = {Kalaria, Dvij and Maheshwari, Parv and Jha, Animesh and Issar, Arnesh Kumar and Chakravarty, Debashish and Anwar, Sohel and Towar, Andres},
  keywords = {Robotics (cs.RO), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Local NMPC on Global Optimised Path for Autonomous Racing},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@ARTICLE{Funke2017,
  author={Funke, Joseph and Brown, Matthew and Erlien, Stephen M. and Gerdes, J. Christian},
  journal={IEEE Transactions on Control Systems Technology}, 
  title={Collision Avoidance and Stabilization for Autonomous Vehicles in Emergency Scenarios}, 
  year={2017},
  volume={25},
  number={4},
  pages={1204-1216},
  doi={10.1109/TCST.2016.2599783},
  url={https://doi.org/10.1109/TCST.2016.2599783}
}

@INPROCEEDINGS{Buyval2017,
  author={Buyval, Alexander and Gabdulin, Aidar and Mustafin, Ruslan and Shimchik, Ilya},
  booktitle={2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={Deriving overtaking strategy from nonlinear model predictive control for a race car}, 
  year={2017},
  volume={},
  number={},
  pages={2623-2628},
  doi={10.1109/IROS.2017.8206086}
}

@INPROCEEDINGS{Ni2017,
  author={Ni, Jun and Hu, Jibin},
  booktitle={2017 IEEE Intelligent Vehicles Symposium (IV)}, 
  title={Path following control for autonomous formula racecar: Autonomous formula student competition}, 
  year={2017},
  volume={},
  number={},
  pages={1835-1840},
  doi={10.1109/IVS.2017.7995972}, 
  url={https://doi.org/10.1109/IVS.2017.7995972}
  }

@ARTICLE{Fu2016,
    author={Fu, Miaomiao and Ni, Jun and Li, Xueyuan and Hu, Jibin},
    journal={International Journal of Automotive Technology}, 
    title={Path tracking for autonomous race car based on G-G diagram}, 
    year={2016},
    volume={4},
    number={4},
    pages={659-658},
    doi={10.1007/s12239-018-0063-7},
    url={https://doi.org/10.1007/s12239-018-0063-7}
    }

@ARTICLE{Kritayakirana2012,
    author={Kritayakirana, Krisada and Gerdes, Christian},
    journal={International Journal for Vehicle Autonomous Systems}, 
    title={Autonomous vehicle control at the limits of handling}, 
    year={2012},
    volume={10},
    number={4},
    pages={271-296},
    doi={10.1504/IJVAS.2012.051270},
    url={https://doi.org/10.1504/IJVAS.2012.051270}
    }

@misc{Nekkah2020,
  doi = {10.48550/ARXIV.2010.02828},
  url = {https://arxiv.org/abs/2010.02828},
  author = {Nekkah, Sherif and Janus, Josua and Boxheimer, Mario and Ohnemus, Lars and Hirsch, Stefan and Schmidt, Benjamin and Liu, Yuchen and Borbely, David and Keck, Florian and Bachmann, Katharina and Bleszynski, Lukasz},
  keywords = {Robotics (cs.RO), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {The Autonomous Racing Software Stack of the KIT19d},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@INPROCEEDINGS{Li2019,
  author={Li, Mengyang and Wang, Yafei and Zhou, Zhisong and Yin, Chengliang},
  booktitle={2019 IEEE Vehicle Power and Propulsion Conference (VPPC)}, 
  title={Sampling Rate Selection for Trajectory Tracking Control of Autonomous Vehicles}, 
  year={2019},
  volume={},
  number={},
  pages={1-5},
  doi={10.1109/VPPC46532.2019.8952506},
  url = {https://doi.org/10.1109/VPPC46532.2019.8952506}
  }

@misc{Sakai2018,
  doi = {10.48550/ARXIV.1808.10703},
  url = {https://arxiv.org/abs/1808.10703},
  author = {Sakai, Atsushi and Ingram, Daniel and Dinius, Joseph and Chawla, Karan and Raffin, Antonin and Paques, Alexis},
  keywords = {Robotics (cs.RO), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {PythonRobotics: a Python code collection of robotics algorithms},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{Patnaik2020,
  doi = {10.48550/ARXIV.2012.02978},
  url = {https://arxiv.org/abs/2012.02978},
  author = {Patnaik, Adarsh and Patel, Manthan and Mohta, Vibhakar and Shah, Het and Agrawal, Shubh and Rathore, Aditya and Malik, Ritwik and Chakravarty, Debashish and Bhattacharya, Ranjan},
  keywords = {Robotics (cs.RO), Systems and Control (eess.SY), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  title = {Design and Implementation of Path Trackers for Ackermann Drive based Vehicles},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@INPROCEEDINGS{Werling2010,
  author={Werling, Moritz and Ziegler, Julius and Kammel, Soren and Thrun, Sebastian},
  booktitle={2010 IEEE International Conference on Robotics and Automation}, 
  title={Optimal trajectory generation for dynamic street scenarios in a Frenet Frame}, 
  year={2010},
  volume={},
  number={},
  pages={987-993},
  doi={10.1109/ROBOT.2010.5509799},
  url={https://doi.org/10.1109/ROBOT.2010.5509799}
  }

@misc{Moghadam2020,
  doi = {10.48550/ARXIV.2011.13098},
  url = {https://arxiv.org/abs/2011.13098},
  author = {Moghadam, Majid and Alizadeh, Ali and Tekin, Engin and Elkaim, Gabriel Hugh},
  keywords = {Robotics (cs.RO), Artificial Intelligence (cs.AI), Systems and Control (eess.SY), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  title = {An End-to-end Deep Reinforcement Learning Approach for the Long-term Short-term Planning on the Frenet Space},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{Novikov2018,
author = {Novikov, Aleksandr and Novikov, Ivan and Shevtsova, Anastasiya},
year = {2018},
month = {January},
pages = {548-555},
title = {Study of the impact of type and condition of the road surface on parameters of signalized intersection},
volume = {36},
journal = {Transportation Research Procedia},
doi = {10.1016/j.trpro.2018.12.154},
url = {https://doi.org/10.1016/j.trpro.2018.12.154}
}


@article{Zhao2017,
author = {Zhoa, You-Qun and Li, Hai-Qing and Lin, Fen and Wang, Fen and Ji, Xue-Wu},
year = {2017},
month = {January},
pages = {982-990},
title = {Estimation of Road Friction Coefficient in Different Road Conditions Based on Vehicle Braking Dynamics},
volume = {36},
journal = {Chinese Journal of Mechanical Engineering},
doi = {10.1007/s10033-017-0143-z},
url = {https://doi.org/10.1007/s10033-017-0143-z}
}

@article{Vorotovic2013,
  url = {https://www.mas.bg.ac.rs/_media/istrazivanje/fme/vol41/1/08_gvorotovic.pdf},
  author = {Vorotic, Goran and Rakicevic, Branislav and Mitic, Sasa Stamenkovic, Dragan},
  title = {Determination of cornering stiffness through integration of a mathematical model and real vehicle exploitation parameters},
  publisher = {Faculty of Mechanical Engineering, Belgrade},
  year = {2013},
  pages = {66-71},
  volume = {41},
  journal = {FME Transactions},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Dai2022,
title = {Analysing deep reinforcement learning agents trained with domain randomisation},
journal = {Neurocomputing},
volume = {493},
pages = {143-165},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.04.005},
url = {https://doi.org/10.1016/j.neucom.2022.04.005},
author = {Tianhong Dai and Kai Arulkumaran and Tamara Gerbert and Samyakh Tukra and Feryal Behbahani and Anil Anthony Bharath},
keywords = {Deep reinforcement learning, Generalisation, Interpretability, Saliency}
}

@article{White1985,
title = {Real applications of Markov Decision Processes},
journal = {INFORMS},
volume = {15},
pages = {73-83},
year = {1985},
url = {http://www.jstor.org/stable/25060766},
author = {White, Douglas}
}

@article{Hornik1989,
title = {Multilayer feedforward networks are universal approximators},
journal = {Neural Networks},
volume = {2},
pages = {359-366},
year = {1989},
url = {https://cognitivemedium.com/magic_paper/assets/Hornik.pdf},
author = {Hornik, Kurt}
}

@article{Zhou2020,
title = {Sim-to-real trainsfer in deep reinforcement learning for robotics: a survey},
journal = {IEEE Symposium Series on Computational Intelligence},
pages = {737-744},
year = {2020},
url = {https://doi.org/10.48550/arXiv.2009.13303},
author = {Zhao, Wenshuai and Queralta, Pena and Westerlund, Tomi}
}

@inproceedings{
Truong2022,
title={Rethinking Sim2Real: Lower Fidelity Simulation Leads to Higher Sim2Real Transfer in Navigation},
author={Joanne Truong and Max Rudolph and Naoki Harrison Yokoyama and Sonia Chernova and Dhruv Batra and Akshara Rai},
booktitle={6th Annual Conference on Robot Learning},
year={2022},
url={https://openreview.net/forum?id=BxHcg_Zlpxj}
}

@ARTICLE{Rieber2004,
  author={Rieber, J.M. and Wehlan, H. and Allgower, F.},
  journal={IEEE Control Systems Magazine}, 
  title={The ROBORACE contest}, 
  year={2004},
  volume={24},
  number={5},
  pages={57-60},
  doi={10.1109/MCS.2004.1337859},
  url={https://doi.org/10.1109/MCS.2004.1337859}
  }

@INPROCEEDINGS{Hanqing2018,
  author={Tian, hanqing and NI, Jun and HU, Jibin},
  booktitle={IEEE Intelligent Vehicles Symposium (IV)}, 
  title={Autonomous Driving System Design for Formula Student Driverless Racecar}, 
  year={2018},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/IVS.2018.8500471},
  url={https://doi.org/10.1109/IVS.2018.8500471}
  }

@InProceedings{Wischnewski2022,
author="Wischnewski, Alexander
and Geisslinger, Maximilian
and Betz, Johannes
and Betz, Tobias
and Fent, Felix
and Heilmeier, Alexander
and Hermansdorfer, Leonhard
and Herrmann, Thomas
and Huch, Sebastian
and Karle, Phillip
and Nobis, Felix
and {\"O}gretmen, Levent
and Rowold, Matthias
and Sauerbeck, Florian
and Stahl, Tim
and Trauth, Rainer
and Lienkamp, Markus
and Lohmann, Boris",
editor="Pfeffer, Peter",
title="Indy Autonomous Challenge - Autonomous Race Cars at the Handling Limits",
booktitle="12th International Munich Chassis Symposium 2021",
year="2022",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="163--182",
url={https://doi.org/10.1007/978-3-662-64550-5_10}
}

@INPROCEEDINGS{Babu2020,
  author={Babu, Varundev Suresh and Behl, Madhur},
  booktitle={2020 IEEE 16th International Conference on Automation Science and Engineering (CASE)}, 
  title={f1tenth.dev - An Open-source ROS based F1/10 Autonomous Racing Simulator}, 
  year={2020},
  volume={},
  number={},
  pages={1614-1620},
  doi={10.1109/CASE48305.2020.9216949},
  url = {https://doi.org/10.1109/CASE48305.2020.9216949}
  }


@article{Heilmeier2020,
author = {Alexander Heilmeier  and  Alexander   Wischnewski  and  Leonhard   Hermansdorfer  and  Johannes   Betz  and  Markus   Lienkamp  and  Boris   Lohmann },
title = {Minimum curvature trajectory planning and control for an autonomous race car},
journal = {Vehicle System Dynamics},
volume = {58},
number = {10},
pages = {1497-1527},
year  = {2020},
doi = {10.1080/00423114.2019.1631455},
url = {https://doi.org/10.1080/00423114.2019.1631455}
}


@article{Betz_2023,
	doi = {10.1002/rob.22153},
	url = {https://doi.org/10.1002%2Frob.22153},
	year = {2023},
	publisher = {Wiley},
	author = {Johannes Betz and Tobias Betz and Felix Fent and Maximilian Geisslinger and Alexander Heilmeier and Leonhard Hermansdorfer and Thomas Herrmann and Sebastian Huch and Phillip Karle and Markus Lienkamp and Boris Lohmann and Felix Nobis and Levent Ogretmen and Matthias Rowold and Florian Sauerbeck and Tim Stahl and Rainer Trauth and Frederik Werner and Alexander Wischnewski},
	title = {TUM autonomous motorsport: An autonomous racing software for the Indy Autonomous Challenge},
	journal = {Journal of Field Robotics}
}

@article{Kelly2010,
author = { D.P. Kelly  and  R.S.Sharp },
title = {Time-optimal control of the race car: a numerical method to emulate the ideal driver},
journal = {Vehicle System Dynamics},
volume = {48},
number = {12},
pages = {1461-1474},
year  = {2010},
publisher = {Taylor & Francis},
doi = {10.1080/00423110903514236},
url = {https://doi.org/10.1080/00423110903514236},
eprint = {https://doi.org/10.1080/00423110903514236}
}

@INPROCEEDINGS{Stahl2019,
  author={Stahl, Tim and Wischnewski, Alexander and Betz, Johannes and Lienkamp, Markus},
  booktitle={IEEE Intelligent Transportation Systems Conference (ITSC)}, 
  title={Multilayer Graph-Based Trajectory Planning for Race Vehicles in Dynamic Scenarios}, 
  year={2019},
  volume={},
  number={},
  pages={3149-3154},
  doi={10.1109/ITSC.2019.8917032},
  url={https://doi.org/10.1109/ITSC.2019.8917032}
  }

@article{Li2017,
title = {Development of a new integrated local trajectory planning and tracking control framework for autonomous ground vehicles},
journal = {Mechanical Systems and Signal Processing},
volume = {87},
pages = {118-137},
year = {2017},
note = {Signal Processing and Control challenges for Smart Vehicles},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2015.10.021},
url = {https://www.sciencedirect.com/science/article/pii/S0888327015004811},
author = {Xiaohui Li and Zhenping Sun and Dongpu Cao and Daxue Liu and Hangen He},
keywords = {Autonomous ground vehicles, Local trajectory planning, Tracking control, Obstacle avoidance, Model-based predictive trajectory generation}
}

@misc{Becker2022,
      title={Model- and Acceleration-based Pursuit Controller for High-Performance Autonomous Racing}, 
      author={Jonathan Becker and Nadine Imholz and Luca Schwarzenbach and Edoardo Ghignone and Nicolas Baumann and Michele Magno},
      year={2022},
      eprint={2209.04346},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://doi.org/10.48550/arXiv.2209.04346}
}


@InProceedings{Achin2021,  title =  {BayesRace: Learning to race autonomously using prior experience},  author =       {Jain, Achin and O'Kelly, Matthew and Chaudhari, Pratik and Morari, Manfred},  booktitle =  {Proceedings of the 2020 Conference on Robot Learning},  pages =  {1918--1929},  year =  {2021},  editor =  {Kober, Jens and Ramos, Fabio and Tomlin, Claire},  volume =  {155},  series =  {Proceedings of Machine Learning Research},  month =  {16--18 Nov},  publisher =    {PMLR},  pdf =  {https://proceedings.mlr.press/v155/jain21b/jain21b.pdf},  url =  {https://proceedings.mlr.press/v155/jain21b.html},  abstract =  {Autonomous race cars require perception, estimation, planning, and control modules which work together asynchronously while driving at the limit of a vehicle’s handling capability. A fundamental challenge encountered in designing these software components lies in predicting the vehicle’s future state (e.g. position, orientation, and speed) with high accuracy. The root cause is the difficulty in identifying vehicle model parameters that capture the effects of lateral tire slip. We present a model-based planning and control framework for autonomous racing that significantly reduces the effort required in system identification and control design. Our approach alleviates the gap induced by simulation-based controller design by learning from on-board sensor measurements. A major focus of this work is empirical, thus, we demonstrate our contributions by experiments on validated 1:43 and 1:10 scale autonomous racing simulations.}}

@misc{Cai2020,
    title={High-Speed Autonomous Drifting With Deep Reinforcement Learning}, 
    author={Peide Cai and Xiaodong Mei and Lei Tai and Yuxiang Sung and Ming Liu},
    year={2020},
    month={April},
    journal = {IEEE Robotics and automation letters},
    volume = {5},
    url={https://doi.org/10.1109/LRA.2020.2967299}
}

@misc{Wurman2022,
    title={Outracing champion Gran Turismo drivers with deep reinforcement learning}, 
    author={Wurman, Peter R. and Barrett, Samuel and Kawamoto, Kenta and MacGlashan, James and Subramanian, Kaushik and Walsh, Thomas J. and Capobianco, Roberto and Devlic, Alisa and Eckert, Franziska and Fuchs, Florian and Gilpin, Leilani and Khandelwal, Piyush and Kompella, Varun and Lin, HaoChih and MacAlpine, Patrick and Oller, Declan and Seno, Takuma and Sherstan, Craig and Thomure, Michael D. and Aghabozorgi, Houmehr and Barrett, Leon and Douglas, Rory and Whitehead, Dion and Durr, Peter and Stone, Peter and Spranger, Michael and Kitano, Hiroaki},
    year={2022},
    month={February},
    journal = {Nature},
    volume = {602},
    url={https://doi.org/10.1038/s41586-021-04357-7}
}

@inproceedings{Caps2021, 
    title={Regularizing Action Policies for Smooth Control with Reinforcement Learning},
    author={Siddharth Mysore and Bassel Mabsout and Renato Mancuso and Kate Saenko},
    journal={IEEE International Confernece on Robotics and Automation}, 
    year={2021},
    url={https://doi.org/10.48550/arXiv.2012.06644}
    }   

@INPROCEEDINGS{Betz2019,
  author={Betz, Johannes and Wischnewski, Alexander and Heilmeier, Alexander and Nobis, Felix and Stahl, Tim and Hermansdorfer, Leonhard and Lienkamp, Markus},
  booktitle={2019 IEEE 89th Vehicular Technology Conference (VTC2019-Spring)}, 
  title={A Software Architecture for an Autonomous Racecar}, 
  year={2019},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/VTCSpring.2019.8746367},
  url={https://doi.org/10.1109/VTCSpring.2019.8746367}
  }

@ARTICLE{Weiss2022,
  author={Weiss, Trent and Behl, Madhur},
  journal={IEEE Robotics and Automation Letters}, 
  title={This is the Way: Differential Bayesian Filtering for Agile Trajectory Synthesis}, 
  year={2022},
  volume={7},
  number={4},
  pages={10414-10421},
  doi={10.1109/LRA.2022.3193496},
  url={https://doi.org/10.1109/LRA.2022.3193496}
  }

@misc{wang2019benchmarking,
      title={Benchmarking Model-Based Reinforcement Learning}, 
      author={Tingwu Wang and Xuchan Bao and Ignasi Clavera and Jerrick Hoang and Yeming Wen and Eric Langlois and Shunshi Zhang and Guodong Zhang and Pieter Abbeel and Jimmy Ba},
      year={2019},
      eprint={1907.02057},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url = {https://doi.org/10.48550/arXiv.1907.02057}
}

@book{Rajamani2012,
    title     = {Vehicle Dynamics and Control},
    author    = {Rajesh Rajamani},
    year      = 1988,
    publisher = {Springer},
    address   = {Minneapolis},
    url       = {https://doi.org/10.1007/978-1-4614-1433-9}
}