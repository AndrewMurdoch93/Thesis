\chapter{Reinforcement learning}\label{chp:rl}


Reinforcement learning (RL) is a paradigm of machine learning concerned with training an agent to take sequential decisions in an environment in order to maximise a reward signal.
In RL, an agent interacts with an environment and learns by receiving feedback in the form of rewards. 
The agent's goal is to learn a policy by which it can take actions that maximise the cumulative reward \cite{sutton2020}.


This chapter begins with an introduction to Markov Decision Processes (MDPs), which provide the formal framework for modeling the sequential decision-making problem that reinforcement learning aims to solve.
Subsequently, we discuss how RL solves the MDP.
Specifically, we present a method from the policy gradient class of reinforcement learning algorithms known as twin delay deep deterministic policy gradient (TD3).


\section{Markov decision processes}\label{sec:mdps}

The Markov Decision process is a popular mathematical framework for modelling discrete-time decision-making processes where the outcomes are partly random and partly under the control of the \emph{agent} (i.e., the decision maker).
MDPs are useful for studying optimisation problems whereby the goal is for the agent to learn a sequence of actions that maximise a \emph{reward} signal.
Such optimisation problems are commonly found in the fields of robotics, automated control, and manufacturing \cite{White1985}.
The ability of MDPs to model sequential problems is useful for the autonomous racing problem, where the outcome of the race is determined by a sequence of control actions, and the exact vehicle dynamics model is often uncertain.



Within an MDP, the learner and decision maker is called the agent.
The environment comprises everything outside of the agent.
The agent and environment interact in a sequence of discrete time steps, $t=0,1,2,3, \cdots$.
A representation of this interaction is shown in Figure \ref{fig:agent_env_boundary}.
At every time step $t$, the agent receives a state $S_t$, which is a representation of the environment, on which basis it selects an action $A_t$.
The environment represents a set of state transition probabilities that are dependent only on the previous state and action.
At the next time step $t+1$, the agent receives a scalar reward $R_{t+1}$, along with a new state $S_{t+1}$, which is sampled according to the state transition probabilities \cite{sutton2020}.
This agent-environment interaction is repeated, giving rise to a trajectory
\begin{equation}\label{eq:mdp_trajectory}
S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \cdots.
\end{equation}


\begin{figure}[!htb]
\centering
\input{contents/chapt3/figs/agent_environment_boundary.pdf_tex}
\caption[The agent environment interaction within an MDP]{The agent environment interaction within an MDP. Adapted from Sutton and Barto \cite{sutton2020}. }
\label{fig:agent_env_boundary}
\end{figure}


As stated previously, the objective of the agent is described in terms of the reward signal: it must formulate a policy $\pi(a|s)$ that maximises the sum of discounted rewards over the course of its entire trajectory.
This sum of discounted rewards is called the \emph{return}, and is denoted $G_t$,
\begin{equation}
G_t \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... = \sum_{k=t+1}^{T} \gamma^{k-t-1} R_{k},
\label{eq:G_t_discount}
\end{equation}
where $T$ denotes the final time step in the trajectory, and $\gamma$ is a discount factor that determines the weighting of future rewards \cite{sutton2020}.





\section{Taxonomy of reinforcement learning}

Reinforcement learning provides an iterative approach for solving MDPs.
Broadly speaking, there are two classes of RL techniques: these are model-based and model free approaches.
Model-free RL methods aim to learn a policy solely through direct trial-and-error interactions with the environment, without utilising a representation of the environment dynamics.
On the other hand, model-based reinforcement learning involves learning a model of the environment dynamics, which the agent then uses to plan ahead before taking an action \cite{wang2019benchmarking}.
Due to the complexity involved in learning the dynamics of a race-car, we opted to limit the scope of the project to encompass only model-free methods.

Another relevant distinction that can be made is that of value-based and policy gradient methods.
Value-based RL methods  focus on estimating the action-value function, which represents the expected return of being in a particular state taking a specific action.
In these methods, the policy is derived from the action-value function.
For instance, a greedy policy always selects the action associated with the highest action-value.
However, a limitation of value-based methods is that they require the action-space to be discretised \cite{sutton2020}.
In the context of autonomous racing, where precise control actions are necessary for maintaining vehicle safety, discretized action-spaces fall short in providing the required level of control.

Meanwhile, Policy gradient techniques form a class of RL methods that explicitly represent the agent's policy as a function.
These methods directly optimise the policy, which serves as a mapping between states and actions, in order to maximise performance.
Representing the policy as a differentiable function such as a DNN allows for the representation of continuous state and action spaces \cite{silver2014}.
We have therefore chosen to employ policy gradient methods in this study.
Specifically, we use a state of the art policy gradient method known as Twin Delay Deep Deterministic Policy Gradient (TD3) \cite{Fujimoto2018}.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Twin delay deep deterministic policy gradient}\label{sec:td3}

The twin delay deep deterministic policy gradient (TD3) algorithm by Fujimoto et al. \cite{Fujimoto2018}, is a popular RL technique applied to solve robotics problems due to its capability to handle continuous action-spaces.
The basic idea of TD3 is to represent the policy, denoted as $\pi_{\bm{\phi}}(s)$, as a DNN with parameters $\bm{\phi}$.
This DNN is also known as the actor.
To update the actor parameters such that the policy improves, gradient ascent is performed with respect to a performance measure $J(\bm{\phi})$,
\begin{equation}\label{eq:policy_gradient_ascent}
    \bm{\phi}_{t+1} = \bm{\phi}_{t} + \alpha \nabla_{\bm{\phi}} J(\bm{\phi}_t).
\end{equation}
This performance measure is defined in terms of an \emph{action-value function}, denoted $Q_{\bm{\theta}}(s,a)$, which is the expected return $\mathbb{E}[G_t]$ from the current state to the end of the episode, given that a specific action is selected in the current state, after which the policy $\pi_{\bm{\theta}}$ is followed.
This action-value function is known as a critic, and is represented by a DNN with parameters $\bm{\theta}$.
In TD3, the idea is to set the performance measure equal to the action-value function
\begin{equation}
    J(\bm{\phi}) = Q_{\bm{\theta}}(s,a)| _{a=\pi_{\bm{\phi}}(s)}.
\end{equation}
Then, updating the actor parameters as in Equation \ref{eq:policy_gradient_ascent} will result in a policy that maximises the expected return, thus solving the MDP. 
TD3 is presented in Algorithm \ref{alg:td3}.


\begin{algorithm}[hbt!]
\caption[Twin delay deep deterministic policy gradient (TD3)]{The twin delay deep deterministic policy gradient (TD3) algorithm, adapted from Fujimoto et al. \cite{Fujimoto2018}.}\label{alg:td3}
Initialise critic networks $Q_{\bm{\theta}_1}, Q_{\bm{\theta}_2}$ and actor network $\pi_{\bm{\phi}}$ 
with \\
\nonl parameters $\bm{\theta}_1, \bm{\theta}_2$ and $\bm{\phi}$ \\
Initialise target networks $Q_{\bm{\theta}_1'}$, $Q_{\bm{\theta}_2'}$ and $\pi_{\bm{\phi}'}$ with weights \\
\nonl $\bm{\theta}_1' \leftarrow \bm{\theta}_1$, $\bm{\theta}_2' \leftarrow \bm{\theta}_2$, and $\bm{\phi}' \leftarrow \bm{\phi}$ \\
Initialise experience replay buffer $\mathcal{B}$ \\
\vspace{0.5cm}
\For{\emph{episode} = 0, M}
{
    \For{t=0, T}
    {
        Select an action with random exploration noise \\
        \nonl $a_t \sim \pi_{\bm{\phi}}(s) + \epsilon$, $\epsilon \sim \mathcal{N}(0, \sigma)$ \\
        Execute action $a_t$, observe reward $r$ and new state $s_{t+1}$ \\
        Store transition tuple $(s_t,a_t,r_t,s_{t+1})$ in $\mathcal{B}$ \\
        \vspace{0.5cm}
        Sample mini-batch of $N$ transitions $(s_i,a_i,r_i,s_{i+1})$ from $\mathcal{B}$ \\
        Select target actions for every sample:\\
        \nonl $\tilde{a} \leftarrow \pi_{\bm{\phi}'}(s_{i+1}) + \epsilon$, $\epsilon \sim \text{clip}(\mathcal{N}(0,\tilde{\sigma}), -c,c)$ \\
        Set TD target:
        $y \leftarrow r_i + \gamma \min_{q=1,2} Q_{\bm{\theta}_q'}(s_{i+1}, \tilde{a})$ \\
        Update critics by minimising the loss: %\hspace{1.8cm}
            % \rlap{\smash{$\left.\begin{array}{@{}c@{}}\\{}\\{}\end{array}\right\}
            % \begin{tabular}{l}Policy\\evaluation\end{tabular}$}} \\
        $\bm{\theta}_q \leftarrow \text{argmin}_{\bm{\theta}_q} ( \frac{1}{N} \sum_{i=0}^{N} (y_i - Q_{\bm{\theta}_q}(s_i,a_i))^{2} ) $ \\
        \vspace{0.5cm}
        \If{$t$ \emph{mod} $d$}
        {
            Update $\bm{\phi}$ by the deterministic policy gradient: \\ %\hspace{5.2cm} 
                % \rlap{\smash{$\left.\begin{array}{@{}c@{}}\\{}\\{}\end{array}\right\}
                % \begin{tabular}{l}Policy\\improvement\end{tabular}$}} \\
            \nonl $J(\bm{\phi}) = \frac{1}{N} \sum Q_{\bm{\theta}_1}(s_i,a) | _{a=\pi_{\bm{\phi}}(s_i)}$ \\
            Update target networks: \\
            $\bm{\theta}_{q}' \leftarrow \tau \bm{\theta}_q + (1 - \tau) \bm{\theta}_{q}'$ \\
            $\bm{\phi}' \leftarrow \tau \bm{\phi} + (1 - \tau) \bm{\theta}'$ \\
        }
    }
}
\end{algorithm}



In the first two lines of this algorithm, the actor and two critic DNNs are initialed as $\pi_{\bm{\phi}}$ and $Q_{\bm{\theta}_q}$, respectively.
The actor is parameterised by $\bm{\phi}$, and the critics by $\bm{\theta}_q$, where the subscript $q$ indicates the critic number.
Two critics are utilised to combat a phenomenon known as \emph{overestimation bias}, whereby states with a low real return are evaluated as having a high expected return. 


Subsequently, \emph{target networks} are initialised for each critic and the actor.
These target networks are identical in structure to their corresponding counterparts, and are indicated with a prime symbol.
Therefore, the target actor and critics are denoted as $\pi_{\bm{\phi}}'$ and $Q_{\bm{\theta}_q}'$, respectively.
These target networks are updated at a slower rate than their counterparts.
According to Mnih et al. \cite{mnih2013}, utilising these target networks to update the action-value function increases training stability.


Next, a replay buffer, denoted by $\mathcal{B}$, is initialised as an empty array to store the agents experiences.
These experiences are stored as a tuple 
\begin{equation}
    \bm{e}_t = (s_t,a_t,r_t,s_{t+1}),
\end{equation}
where $t$ is the time step of the experience.
By maintaining a history of experiences, each experience can be sampled multiple times to update the parameters of the actor and critics.
This approach also helps to break correlations between samples, preventing the DNN parameters from converging to a sub-optimal local minima \cite{mnih2013}.


After initialising the actor, critics and replay buffer, the TD3 algorithm executes for $M$ number of of episodes, where each episode represents a trajectory of state, action and reward sequences, as shown in Equation \ref{eq:mdp_trajectory}.
Every episode consists $T$ time steps.
At each time step, the algorithm follows this structure: the agent is sampled for an action, after which the environment provides the next state and reward.
Subsequently, the critics are updated to improve the accuracy of action-value estimation.
Additionally, the policy and target networks are updated every $d$ time steps.



In line $6$, an action is sampled from the actor using
\begin{equation}\label{eq:dpg_policy}
    a_t = \pi_{\bm{\phi}}(s_t) + \epsilon, \hspace{0.3cm} \epsilon \sim \mathcal{N}(0,\sigma),
\end{equation}
where $ \pi_{\bm{\phi}}(s_t)$ is obtained by forward passing the state through the actor DNN.
To encourage exploration, Gaussian noise with a mean of $0$ and a standard deviation of $\sigma$ is added to each action.
After sampling the new state , denoted as $s_{t+1}$ and reward, denoted as $r_{t}$ from the environment in line $7$, the agent's experience tuple is stored in the replay buffer in line $8$.



The process of updating the critics occurs in lines $9$ to $12$. 
Firstly, a batch of $N$ experiences is sampled from the replay buffer.
Each experience is represented as a tuple comprised of a state $s_i$, action $a_i$, reward $r_i$ and subsequent state $s_{i+1}$.
For each of these experiences, the target actor is used to select \emph{target actions} associated with the subsequent state,
\begin{equation}
    \tilde{a} \leftarrow \pi_{\bm{\phi}'}(s_{i+1}) + \epsilon, \epsilon \sim \text{clip}(\mathcal{N}(0,\tilde{\sigma}), -c,c).
\end{equation}
Guassian noise is added to the target action with upper and lower magnitude bounds of $c$.



The critic DNNs are then updated towards a \emph{TD target} vector, denoted as $y$, in line $12$.
Each element in the TD target vector corresponds to a sampled experience $i$, and is computed by
\begin{equation}
    y_i \leftarrow r_i + \gamma \min_{q=1,2} Q_{\bm{\theta}_q'}(s_{i+1}, \hat{a}),
\end{equation}
where $\gamma$ is the discount factor. The cost to be utilised in the gradient descent step for the critics, which we denote as $J(\bm{\theta})$, is computed as the MSE between the TD target and the action-values of the sampled experiences, as evaluated by the critics themselves:
\begin{equation}
     J(\bm{\phi}) = \frac{1}{N} \sum_{i=0}^{N} (y_i - Q_{\bm{\theta}_q}(s_i,a_i))^{2}.
\end{equation}
Note that the cost is computed twice, by taking the MSE between the target and both critics.
The parameters of both critics are then updated using gradient descent with the objective of minimizing the cost function represented in line $12$.
Utilising two critics in this manner aims to overcome the issue of overestimation bias of action-values.


The updates to the actor and target DNNs occur every $d$ time steps to reduce the variance of their updates. 
The update to the actor parameters is performed by gradient ascent with respect to the performance measure $J(\bm{\phi})$.
This performance measure is defined as the the average of the first critics evaluation of the sampled states, where the action is given by the current policy $\pi_{\bm{\theta}}$, over the $N$ experiences sampled from the replay buffer:
\begin{equation}
    J(\bm{\theta}) = \frac{1}{N} \sum Q_{\bm{\theta}_1}(s_i,a) | _{a=\pi_{\bm{\phi}}(s_i)}.
\end{equation}
Therefore, updating the policy in a way that maximizes the performance measure results in a policy that maximizes the estimated return for each action.


Lastly, the target network parameters $\bm{\phi}'$ and $\bm{\theta}'$ are updated towards $\bm{\phi}$ and $\bm{\theta}$ using a \emph{soft update} rule, 
\begin{equation}
\begin{split}
    \bm{\theta}' &\leftarrow \tau \bm{\theta}' + (1 - \tau) \bm{\theta}, \\
    \bm{\phi}' &\leftarrow \tau \bm{\phi} + (1 - \tau) \bm{\phi}',
\end{split}
\end{equation}
where $\tau \ll 1$ is the soft update rate, which prevents the target DNN parameters from changing too quickly, which in turn stabilises learning. 





\section{Summary}

This chapter presented the theoretical foundation of the Markov Decision Process (MDP), which serves as the formal mathematical framework for the problem that Reinforcement Learning (RL) aims to solve.
Within an MDP, the agent interactions with an environment that is defined by a set of state transition probabilities. 
The objective of the agent in an MDP is then to formulate a policy by which it chooses actions, such that a reward signal is maximised.


Next, we explored the field of reinforcement learning (RL), with particular emphasis on a technique called Twin Delay Deep Deterministic Policy Gradient (TD3). 
To the best of our knowledge, TD3 is the state of the art in reinforcement learning algorithm for continuous state and action spaces.
This is particularly significant in the context of autonomous racing, as the driving task necessitates very fine control actions.


In the next chapter, we discuss how F1tenth autonomous racing is modelled as an environment within the MDP framework, such that TD3 can be applied to train RL agents to learn how to race.

