\chapter{Literature Study}
\label{chp:litreview}

Keeping our project objectives in mind, we now give a brief overview of the F1Tenth autonomous racing platform, before reviewing existing approaches to designing autonomous driving algorithms.


\section{F1Tenth racing}
\label{sec:f1tenth}

\begin{figure}[b]
    \centering
    \includegraphics[width=\textwidth*7/10]{contents/chapt2/figs/f1tenth_prague_1.png}
    \caption[Two F1Tenth cars racing head-to-head]{Two F1Tenth cars race head-to-head at the International Conference on Intelligent Robots and Systems (IROS) 2021 event.}
    \label{fig:f1tenth_prague}
\end{figure}

F1Tenth is a semi-regular autonomous racing competition that allows teams to compete with a standardised set of hardware and rules.
The organisers of the competition provide specifications for the vehicle, such as the chassis, onboard computer and sensors.
Furthermore, to aid the development process, a vehicle simulator is provided. 
The track is known prior to the races, and its boundaries are built from LiDAR perceivable material.
The standardisation of the vehicles and LiDAR perceivable track boundaries shifts the research focus towards trajectory planning and controller design.

The competition comprises two stages, a time trial event and a head-to-head race.
In time trial, the goal is to drive around the track as fast as possible without any other vehicles on the track, whereas in head-to-head racing there are two vehicles on the track at the same time.
Head-to-head racing is shown in Figure \ref{fig:f1tenth_prague}.
For both of the stages, the cars must navigate using only onboard sensors and computers. 
This introduces a limitation on the computational expense of the algorithms that teams can deploy.
The support that the F1Tenth platform provides makes it an ideal test-bed for autonomous vehicle development \cite{f1tenth}.


\section{Approaches to driving algorithm design}
\label{sec:driving_algorithms}
In the previous section, we described our racing environment.
We now discuss existing approaches to solving the autonomous racing task that are not specific to F1Tenth.
We have chosen to group the literature according to their architecture (the structure of the algorithm, its components and relationship between components).
These architectures are the classical, end-to-end and partial end-to-end pipelines.
This grouping is shown in the taxonomy of Figure \ref{fig:lit_tax_sec}. 

As described in the introduction, the classical pipeline uses a series of separate modules with unique functions to control the vehicle.
Two of these modules, trajectory planning and control, are relevant to this project and are discussed in more detail.
The end-to-end pipeline aims to use a neural network to perform the task of all the modules in the classical pipeline.
We categorise end-to-end approaches by the method used to train the neural network, namely reinforcement or imitation learning.
Finally, the partial end-to-end pipeline uses a neural network within the modularised structure of the classical pipeline. 
These approaches are categorised according to which function of the pipeline is learned by the neural network. 

\begin{figure}[htb!]
    \centering
    \input contents/chapt2/figs/literature_taxonomy_sections.tex
    \caption[A taxonomy of the autonomous racing literature with sections]{A taxonomy of the autonomous racing literature. The second layer reflects approaches to designing architectures, while the third row indicate techniques used within those architectures.}
    \label{fig:lit_tax_sec}
\end{figure}


\section{Classic Pipeline}
\label{sec:classic}

The classic pipeline is the traditional method of designing autonomous driving architectures.
It is split into an \emph{offline} and \emph{online} phase, as shown in Figure \ref{fig:full_stack}.
The offline phase occurs before the vehicle is deployed on the track.
If the track is known, then a global trajectory (also known as a \emph{raceline}) can be computed during this offline phase.
This raceline typically comprises a path ($x(t), y(t)$) and velocity profile $v(t)$ that extends along the entire track.

In the online phase, the vehicle is deployed onto and allowed to interact with a simulated or real track.
The concern during this phase is processing sensor input to generate control outputs.
The classical approach to controlling the car during the online phase is to process the sensor data using a series of modules with well defined functions.
This has the advantages that each function can be developed and improved independently of the others, and vehicle behaviour can easily be debugged.

The modules most used by the classic pipeline are perception, local planning and control.
Perception algorithms derive knowledge about the vehicles surroundings. 
This includes constructing an updated map, localising the vehicle within that map, and detecting obstacles such as other vehicles. 
Information from the perception module is passed onto the local planner and controller to generate and follow a trajectory  \cite{Betz2021}.
Due to their relevance to  our project, methods for local planning and control are now discussed in more detail. 

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth*8/10]{contents/chapt2/figs/classic_pipeline.png}
    \caption{The classic autonomous driving pipeline.}
    \label{fig:full_stack}
\end{figure}


\subsection{Local planning}
\label{sec:trajectory_planning}

The purpose of the local planner is to compute a trajectory based on the current information provided by the perception module.
Local trajectory planning is a requirement for the safe deployment of racing vehicles because racing is a highly dynamic environment, especially when multiple vehicles are involved.
This means that global path that was computed offline may be infeasible or unsafe due to changing conditions on the track.
The challenge for local planners is then to compute a trajectory faster than real-time.

Similar to the raceline, the local trajectory is comprised of a path and velocity profile, but with a finite horizon. 
This is because optimising a trajectory over the entire track is computationally expensive, and also only the part of the trajectory immediately in front of the vehicle is relevant to the current control action.

Model Predictive Control (MPC) methods are the most commonly used and state of the art methods for local planning \cite{Betz2021}. 
This method relies on a vehicle dynamics model to optimise a finite horizon trajectory by minimising a specified cost function.
Only the first section of the trajectory is executed before optimising again.
The MPC method is powerful because it directly considers the vehicle and track constraints.
Therefore, the trajectory generated is safe so long as these constraints specified correctly and the horizon of the trajectory long enough \cite{Schwenzer2021}.

Despite achieving impressive results there are many challenges to deploying MPC for trajectory planning.
One drawback is that performing an optimisation at every time step is computationally expensive.
As such, many applications of MPC in autonomous racing, such as that by  Anderson et al. \cite{Anderson2016} and Funke et al. \cite{Funke2017} utilise simplified linear dynamics models that do not entirely capture the full non-linear system dynamics.
Despite the linearisation of dynamic models, computational expense may still result in the requirement for expensive hardware for online deployment \cite{Pan2017a}.
Furthermore, MPCs rely on convexification (such as with quadratic approximation) of the cost function to be optimised \cite{Williams2017}.


\subsection{Control}
\label{sec:control}

In the previous section, we described the common method of generating a local trajectory.
This local trajectory provides the reference information to compute control commands used to actuate the motors such as to minimise the error between the actual and planned trajectory.
At this level of abstraction, the control commands are the steering angle $\delta$ for \emph{lateral} control, and longitudinal acceleration $a_{\text{long}}$ for \emph{longitudinal} control.
Lateral and longitudinal control are commonly decoupled and treated as separate problems:
while lateral controllers are used for path tracking, longitudinal controller are used for velocity tracking.

\subsubsection*{Lateral Control}

Many of the approaches surveyed employed classic controllers that utilise well known principles in the fields of path and velocity tracking, namely feedforward and feedback control.
Whereas feedback controllers compute control commands using the error between the actual and desired vehicle trajectory, feedforward controllers are open-loop, calculating the control signal based only on the properties of the vehicle and desired path, with no feedback loop compensating for the error between the actual and desired trajectory.

A basic feedback lateral controller for path tracking, known as pure pursuit, is presented by Coultier \cite{Coulter_1992}. 
Pure pursuit calculates the steering angle the car should maintain to reach a target point on the path which is a fixed distance away from the vehicle, under the assumption that the vehicle wheels do not slip. 
Despite this no-slipping assumption, the pure pursuit controller proved robust.
Other classic approaches such as that by Hoffman et al. \cite{Hoffmann2007}, utilise feedback controllers which compute a control signal based on the cross track and heading error.
These errors are the distance between the closest point on the path to the front axle of the vehicle, and the the angle between the heading of the vehicle and the tangent on the closest point on the path, respectively.

Ni and Hu \cite{Ni2017}, Fu et al. \cite{Fu2016} and Kritayakirana and Gerdes \cite{Kritayakirana2012} further the work by Hoffman et al. \cite{Hoffmann2007} by using both feedback and feedforward control. 
Their feedforward controllers use linearised vehicle dynamics models and information about the path to compute a desired steering angle. Feedback control is added to the value computed by the feedforward controller.
The desired behaviour of this controller setup is to imitates a human driver that computes a steering angle based on knowledge about the path and vehicle, then trims the steering angle based on the deviation from the intended path.

Besides classical control, MPCs are one of the main methods behind many autonomous racing controllers that have been implemented on physical vehicles.
Alvarez et al. \cite{alvarez2022} and Nekkah et al. \cite{Nekkah2020} develop MPCs for the lateral control of Formula Student racecars.
Their MPCs  are similar to those described in the trajectory planning section, as they rely on a linearised vehicle dynamics model to forecast a series of optimal series of states $\{ x_t, x_{t+1}, ..., x_{t+N}\}$ and corresponding actions $\{ u_t, u_{t+1}, ..., u_{t+N}\}$. 
The first control action $u_{t}$ is executed and the optimisation process is run again.
However, if an inaccurate vehicle model is used for MPC in the control setting, the vehicle may deviate from its intended path

\subsubsection*{Longitudinal Control}
Longitudinal control for velocity tracking is simpler than path tracking, and is usually achieved using proportional integral derivative (PID) control. 
For instance, Nekkah et al. \cite{Nekkah2020} use a feedforward proportional integral (PI) controller for tracking a velocity target. 
In this case, feedforward control works because the longitudinal dynamics of the race car are known.
Other approaches, such as those by Hoffman et al. \cite{Hoffmann2007} and Alvarez et al. \cite{alvarez2022} utilise feedback proportional integral (PI) control to track a velocity target.

Ni and Hu \cite{Ni2017}, Fu et al. \cite{Fu2016} and Kritayakirana and Gerdes \cite{Kritayakirana2012} take control a step further by calculating the maximum allowable longitudinal acceleration before the tires slip.
This is done using a $g-g$ diagram that describes the friction limit of the tires in terms of lateral and longitudinal acceleration.
The curvature of the planned path is used to calculate the lateral acceleration, which in turn is used to estimate the maximum allowable longitudinal acceleration and velocity.
This allows the vehicle to operate close to the tire friction limits.
The calculated velocity is then tracked with both feedforward and feedback control.

In summary, the modularised pipeline adopted by classical approaches has eased the driving algorithm development process.
By decoupling the tasks of perception, planning and control, research efforts into each task have achieved impressive results.
Within the literature reviewed, many local planners and controllers are based on MPC algorithms.
The limitations to MPC include the requirement for expensive computation (especially for non-linear dynamic models), expensive sensor suites due to the need for frequent and accurate state information, and lack of flexibility in the cost function. 
In particular, reliance on an accurate vehicle dynamics model is a serious issue due to the difficulty of measuring the vehicle model parameters \cite{Kabzan2019, Pan2017}.
%However, there are several robust classical controllers.


\section{End-to-end pipeline}
\label{sec:end_to_end}

The limitations of optimisation techniques from classical methods has led to research in learning-based systems that improve the vehicle dynamics model or action policy with real-world data and allow more complex cost formulations and non-linear dynamics \cite{Fuchs2021}.
Many learning approaches use an end-to-end pipeline whereby a single neural network predicts control outputs from sensor data.
This pipeline is illustrated in Figure \ref{fig:end_to_end}.
The neural networks used in end-to-end approaches are typically trained using imitation or reinforcement learning paradigms \cite{Betz2021}. We present a summary of research efforts into both approaches.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth*8/10]{contents/chapt2/figs/end_to_end_pipeline.png}
    \caption{The end-to-end autonomous driving pipeline.}
    \label{fig:end_to_end}
\end{figure}

\subsection{Imitation learning}
\label{sec:imitation_learning}

Imitation learning techniques train a neural network to mimic an expert such as a human or optimisation method.
The process is analogous to supervised learning, where labelled data is used to train an algorithm to predict outcomes on new data.
In the context of autonomous racing, the labelled training data is produced by allowing an optimisation method or human to drive the vehicle.
This produces a set of control commands which act as the labels for the inputs to the neural network.
Supervised learning is then used to train an algorithm to predict control commands for new data.
Imitation learning is a good choice for training a neural network if it is feasible for an expert to demonstrate optimal behaviour \cite{Osa_2018}.

Many approaches, such as those by Tatulea-Codrean et al. \cite{Tatulea-Codrean2020}, Pan et al. \cite{Pan2017a} and Lee et al. \cite{lee2019} rely on classical methods for expert training data. 
As such, the link between classical and imitation learning approaches is stronger than for classical and reinforcement learning approaches.
This makes the benefits of using neural networks in an end-to-end approach over classical methods clearer in the imitation learning literature than reinforcement learning literature.

A major benefit of imitation learning with neural networks is the cost of online computation: neural networks are far less computationally expensive to run than optimisation methods. This is demonstrated by Tatulea-Codrean et al. \cite{Tatulea-Codrean2020}, who train a neural network to mimic the policy of a non-linear MPC (NMPC). The NMPC is too computationally expensive for real-time control of the vehicle. 
However, the neural network computes the control command in a much faster time, enabling the deployment of the vehicle.

Imitation learning in conjunction with neural networks also have the benefit of being flexible towards their inputs. 
Whereas optimisation methods have strict requirements for their input such as frequent state and boundary condition updates, a neural network is able to learn from high dimensional inputs such as images from a video feed. 
The usefulness of this property is demonstrated by Pan et al. \cite{Pan2017a}, who trained a neural network on a much cheaper sensor suit than what was required for their MPC algorithm.

The flexibility of neural networks towards their inputs is related to another useful property: robustness to sensor noise and failure. 
This property is showcased well by Lee et al. \cite{lee2019}, who use imitation learning to create an ensemble of bayesian neural networks (BNN) on different sensor inputs to create a redundant control policy. 
Their algorithm is robust to sensor noise and even multiple sensor failures, which was not possible using more traditional optimisation methods.

From these examples we see that imitation learning learning with neural networks is a powerful tool.
However, there are some weaknesses to the imitation learning technique:
The first is that the vehicle may encounter a scenario for which no expert training data was generated during training, in which case it is likely to take an incorrect action \cite{Osa_2018}.
This can happen because the learned policy affects the distribution of states that the vehicle encounters.
Since the learned policy differs slightly from the expert policy, the distribution of states that the imitation learned algorithm encounters is different to the training set.

%Furthermore, Wadekar et al. \cite{Wadekar2021} describe that the data collected by the expert must be curated so that the agent does not learn any undesirable behaviour. 

Perhaps the most undesirable characteristic of imitation learning is that it requires expert training data.
This makes it unsuitable for scenarios where expert training data is difficult to generate, as in cases where optimisation methods fail \cite{Fuchs2021a}.
As such, we turn towards a method for training neural networks for control tasks that does not require expert training data.

\subsection{Reinforcement learning}
\label{sec:reinforcement_learning}

Reinforcement learning optimises a neural network policy to maximise a scalar reward signal through direct interaction with the environment \cite{Plaat_2022}. 
The benefits of using reinforcement learning are the same as the imitation learning approach, i.e., less stringent sensor requirements, robustness to sensor failure, and fast online execution time. However, the technique allows for more complex cost formulation than what is possible with even state of the art expert optimisation methods and negates the need for expert training data \cite{Fuchs2021}.
As such, it solves several of the drawbacks of the imitation learning algorithm.
%Reinforcement learning approaches may be categorized according to their action selection method. Whereas model-based methods use a model of the vehicle and environment to plan ahead, model-free methods select an action based only on past experience. 


Several research efforts into reinforcement learning applied to autonomous racing have been aimed at creating pixel to control algorithms. 
Perot et al. \cite{Perot2017} and Jaritz et al. \cite{Jaritz2018} are early works in model-free reinforcement learning applied to autonomous racing that present such pixel to control algorithms for racing in the video game World Rally Cross 6.
Both approaches utilise a convolutional neural network (CNN) with long short-term memeory (LSTM). 
Jaritz et al. \cite{Jaritz2018} state that a score given at the end of the race is too sparse for the agent to learn effectively.
They therefore introduce a continuous reward schema.
Continuous reward schemas have been used by every approach reviewed since Perot et al. \cite{Perot2017} and Jaritz et al. \cite{Jaritz2018}.
However, the approaches show that pixel to control is both extremely inefficient, as both Jaritz et al. \cite{Jaritz2018} and Perot et al. \cite{Perot2017} train their agent for 80 million steps.



Another pixel to control strategy is developed by Schwarting et al. \cite{Schwarting2021}, who apply model-based reinforcement learning to openAI gym's two player racecar environment.
When multiple vehicles are involved, the opponents behaviour must be taken into account.
However, these behavioral and environmental features may be too complex to be captured by MPC or purely game-theoretic approaches that assume perfect knowledge of the state of all vehicles.
Schwarting et al. \cite{Schwarting2021} showcase the strength of reinforcement learning to generate policies without perfect knowledge of the environment.
Their solution learns a model of the world (including the opponent behaviour) using a neural network.
This model is then used to generate imagined gameplay experiences to train the agent on.


A more recent and state of the art approach is presented by Fuchs et al. \cite{Fuchs2021}, who creates a model-free soft actor-critic (SAC) agent that achieves lap times on par with competitive e-sports drivers in the racing game Gran Turismo Sport.
Key to their approach is careful selection of hand-crafted input features that include: 1) the vehicles velocity $\pmb{v}_t \in \mathbb{R}^3$, 2) acceleration $\pmb{\dot{v}}_t \in \mathbb{R}^3$, 3) distance measurements of $M$ rangefinders $\pmb{d}_t \in \mathbb{R}^M$, 4) the previous steering command $\delta_{t-1}$, 5) a binary flag indicating wall contact, and 6) $N$ sampled curvature measurements of the track centerline $\pmb{c}_t \in \mathbb{R}^N$.
These are important input features, as most other reinforcement learning approaches that do not utilise a pixel to control strategy use a selection of these features as input to their neural network.
Fuchs et al. \cite{Fuchs2021} also introduce a continuous progress term into the reward signal, while penalising the agent for making contact with the track boundary:
\begin{equation}
    r_t = r_{t}^{\text{progress}} - 
    \begin{cases}
    c_w \| \pmb{v}_t \| & \text{if in contact with wall} \\
    0 & \text{otherwise}
    \end{cases}
\end{equation}
where $c_w$ is a tuned constant.
Although their implementation remains in simulation, Fuchs et al. \cite{Fuchs2021} do measure the effect of vehicle model inaccuracies by changing the parameters of the vehicle model between training and testing. They find that increased tire friction causes the agent to steer more aggressively leading to brief contact with the inside wall. A decrease in tire friction causes the agent to take corners slightly wide resulting in contact with the outside.
They also measure the effect of delays to the agents inference, finding that delays of greater than 100ms leads to track boundary contact. 
However, it is not clear from Fuchs et al. \cite{Fuchs2021} what degree of model inaccuracy results in collisions.


Song et al. \cite{Song2021} extends the work from Fuchs et al. \cite{Fuchs2021} to a multi-agent setting, training an agent to overtake other vehicles in Gran Turismo Sport.
key to their success is the use of curriculum learning, whereby the agent is trained to solve increasingly difficult tasks by adding components to both the environment and reward signal. 
The agent is trained to race on its own before another vehicle is added to the simulation, and the reward signal is modified to encourage the agent to make progress relative to the other vehicle.
This showcases the strength of reinforcement learning over optimisation methods: while optimisation algorithms rely on the convexification of cost functions, reinforcement learning agents can learn to solve difficult tasks from complex cost functions.


We now move away from now games and discuss research efforts that more directly address the sim2real gap and deployment onto physical cars.
Niu et al. \cite{Niu2020} anticipate that the reinforcement learning algorithm may select an unsafe action.
To prevent this, they propose a model-based safety controller that acts as a safeguard mechanism to prevent the agent from selecting unsafe actions.
Furthermore, they foresee that the safety module's reliance on an accurate vehicle model presents an issue. 
As such, a neural network is used to learn a vehicle model online that eventually replaces the initial assumed vehicle model.
This idea of improving a vehicle model after the vehicle is deployed is a commonplace sim2real practice.
Another benefit of the safety controller is that it constricts the agent's exploration, allowing the policy to converge faster and more robustly.


Another issue that presents itself during physical deployment is that of control smoothness. 
Sudden control actions will induce excessive mechanical wear and cause the vehicle to exhibit dangerous behaviour.
Hsu et al. \cite{hsu2022} identify this issue andapply Conditioning for Action Policy Smoothness (CAPS) to smooth the control action of the vehicle.
Although Hsu et al. \cite{hsu2022} do not directly address model inaccuracies, their policy smoothing approach produces a conservative policy which is expected to increase performance and safety in settings where the model inaccuracies occur.
This is because a smoother policy results in less lateral and longitudinal acceleration and does not bring the vehicle as close to its handling limits as a jerkier policy.


More sim2real practices are explored by Chisari et al. \cite{Chisari2021}, who deploy a soft actor-critic model-free reinforcement learning algorithm onto a physical vehicle.
They make the agent policy robust to real-world transfer training in simulation with a slightly randomised vehicle model.
The agent policy is then refined by retraining the neural network after the physical vehicle is deployed.
This allows the reinforcement learning algorithms to achieve lap times but have the number of track boundary collisions as an MPC controller.
Ivanov et al. \cite{Ivanov2020} take a similar approach to Chisari et al. \cite{Chisari2021} by training an agent with model randomisation, but are not able to achieve robustness to the sim2real transfer.


The final end-to-end approach we discuss is by Brunbauer et al. \cite{brunnbauer}, who demonstrate demonstrate the effectiveness of advanced model- based deep RL compared to model-free agents in the real-world application of autonomous racing.
Brunnbauer et al. \cite{brunnbauer} deploy the model-based Dreamer reinforcement learning algorithm by Hafner et al. \cite{Hafner2019a} on a physical F1tenth vehicle.
Their algorithm learns an observation model which is used to predict agent-environemnt interactions.
It then learns a policy purely on imagined sequences using the observation model, without interaction with the environment.
This results in safe and data-efficient learning.

From the literature reviewed, we see that there are many benefits to integrating neural networks into the driving pipeline.
These include learning from real-world data to improve the driving policy and vehicle dynamics model, shorter online execution time than optimisation methods, and a higher degree of flexibility in the algorithm input.
We also see that reinforcement learning is a viable approach to training a neural network within an end-to-end pipeline, as it circumvents the issue of relying on expert training data that imitation learning approaches suffer from.

However, wrapping the entirety of the driving task into one neural network has repercussions.
The driving behaviour of an end-to-end pipeline is difficult to design and interpret, leading to safety concerns.
Even slight errors in the control input can lead to very large errors in the trajectory, resulting in catastrophe.
Furthermore, the standard end-to-end pipeline is not particularly robust to model inaccuracies that occur when deploying driving algorithms onto physical vehicles, unless the neural network is trained using a set of sim2real best practices.

\begin{landscape}
    \input{contents/chapt2/figs/reinforcement_learning_table.tex}
    \footnote{The absence of a tick in the model-based, multi-agent , and physical vehicle columns indicate that the approach was model-free, single-agent and simulation only, respectively. \label{footnote_1}}
\end{landscape}

%Conclusion:
%RL techniques over MPC and IL
%Challenges to vehicle behaviour
%Challenges with sim2real transfer due to model mismatch
%Classical approaches absorb uncertainty using a controller.
%Presents a question to system architecture design: integrating a neural network into the classical pipeline.


\section{Partial end-to-end pipeline}
\label{sec:partial_end_to_end}


%Wrapping the entirety of the driving task into one neural network has repercussions.
%As the neural network needs to learn all three high level tasks from the classical pipeline (perception, planning and control), it can %Betz et al. and \cite{Weiss2020} et al. \cite{Betz2021} argue that it would be simpler to learn only one task from the classic pipeline.
%Furthermore, Weiss et al. \cite{Weiss2020} argue that even though end-to-end methods perform well on validation data, that does not mean that the method contains a useful driving strategy that can perform well in an actual driving test.
%This is because end-to-end methods fail to capture how expert drivers behave.
%Expert drivers do not select only the control action at the current time step based on their current vehicle state, but instead plan out a trajectory. 
%They then execute control commands to achieve that trajectory.
%Furthermore, slight errors in the control input can lead to very large errors in the trajectory, resulting in catastrophe.

We now consider the approach to designing autonomous driving algorithms that synthesises the classic and end-to-end pipelines. 
In the partial end-to-end approach, the modular structure of the classic pipeline is utilised.
However, rather than using techniques associated with the classic pipeline for implementing individual modules, the function of a module is learned using a neural network.
Approaches following this driving algorithm architecture may benefit from the structure the classic pipeline, and flexibility of the end-to-end pipeline.
There are two possible architectures within the partial end-to-end pipeline: either the controller or the trajectory planner is learned using a neural network. 
These two architectures are shown in Figure \ref{fig:pete_learned_control} and \ref{fig:pete_learned_control}, respectively.
We now give attention to research efforts into both architectures within the partial end-to-end pipeline.

\begin{figure}[h]
    \centering
    \begin{subfigure}[htb!]{\textwidth}
        \centering
        \includegraphics[width=\textwidth*8/10]{contents/chapt2/figs/partial_end_to_end_pipeline_learned_control.png}
         \caption[The partial end-to-end pipeline with a learned controller]{The partial end-to-end pipeline with a learned controller.}
        \label{fig:pete_learned_control}
    \end{subfigure}
    \hfill
    \begin{subfigure}[htb!]{\textwidth}
        \centering
        \includegraphics[width=\textwidth*8/10]{contents/chapt2/figs/partial_end_to_end_pipeline_learned_trajecory_planning.png}
        \caption[The partial end-to-end pipeline with a learned trajectory planner]{The partial end-to-end pipeline with a learned trajectory planner.}
        \label{fig:pete_learned_trajectory_planning}
    \end{subfigure}
\caption[Configurations of the partial end-to-end pipeline]{The two configurations of the partial end-to-end pipeline.}
\label{fig:pete}
\end{figure}

\subsection{Learned controller}
\label{sec:learned_controller}

In the literature reviewed, there were two attempts at creating a partial end-to-end pipeline whereby the controller function is learned by a neural network.
The first approach we look at is by Evans et al. \cite{Evans2021b}, who combined a reinforcement learning agent with a pure pursuit path follower.
By modifying control commands from a pure pursuit path follower that tracks a global path, the agent is able to avoid randomly placed obstacles.
However, a kinetic bicycle model that does not account for tire slipping was used.
As such, the performance of their system under more realistic racing conditions is unknown.

The other was by Ghignone et al. \cite{Ghignone2022}, who replaced the controller with a reinforcement learning agent.
Their agent was trained with varying tire friction coefficients to generalise to different model parameters.
It outperformed both an the MPC controller and end-to-end agent in terms of track boundary collisions by a large margin under model mismatch conditions.
However, Ghignone et al. \cite{Ghignone2022} do not describe the exact parameter values that result in this performance difference.

Furthermore, no local trajectory is computed in either of these approaches. The reinforcement learning agent is therefore used to track a global path.
If the classic structure were adhered to, the neural network controller will need to be combined with a classic local planner. 
As such, approaches following this architecture will suffer from the drawbacks of classic local planners, as described in Section \ref{sec:trajectory_planning}.


\subsection{Learned Trajectory planner}
\label{sec:learned_planner}
We now discuss approaches where the local trajectory planning task is learned by a neural network.
The learned trajectory planner is used in conjunction with a classic controller, as shown in Figure \ref{fig:pete_learned_trajectory_planning}. 
This architecture may benefit from the use of classic controllers which absorb uncertainty in the system and decrease the error between the actual and planned trajectory in a predictable manner.
As such, it may be a viable technique to combat the degradation in performance due to model uncertainty that end-to-end approaches suffer from. 

Weiss et al. \cite{Weiss2020a} trained an imitation learning agent to generate a local path as a bezier curve.
A pure pursuit controller is used to ensure that the vehicle tracks this path.
A similar approach is taken by Capo et al. \cite{Capo2020}, except that a reinforcement learning agent outputs only a single point in front of the vehicle, which is tracked using low-level logic.
Both of these approaches outperformed end-to-end approaches in terms of distance raced before colliding with track boundaries, and generalisation across different tracks. However, they do not take model uncertainty into account.
A summary of the partial end-to-end techniques in literature is presented in Table \ref{table:autonomous_racing_partial_end_to_end_summary}.
%All partial end-to-end pipeline results indicate that the partial end-to-end pipeline outperforms the end-to-end pipeline.

\input{contents/chapt2/figs/partial_end_to_end_table.tex}

\section{Research Gaps and Expected Contributions}
\label{research_gap}
Having explored current solutions to developing autonomous driving architectures, we identify a few trends in the literature.
Research efforts into utilising machine learning for decision making in autonomous racing have been largely limited to end-to-end systems. 
Within these learning (i.e., end-to-end) approaches, reinforcement learning is an especially attractive method for training the neural network because it negates the need for expert training data.

Although the end-to-end approach has shown promising results in simulation, deployment onto hardware remains a challenge.
Aside from the safety concern of wrapping the entire driving task into one neural network, the degradation in performance due to model inaccuracy while deploying agents onto physical vehicles is an issue.
Although some approaches do combat sensitivity towards model inaccuracy in end-to-end systems by randomising model parameters during training or refining the vehicle model after deployment, these techniques do not guarantee the safety of the vehicle, as end-to-end systems still crash after physical deployment.
Furthermore, the effect of this model inaccuracy is not well quantified.

There have been only a few attempts at integrating machine learning into the modular structure of the classical pipeline, as in the partial end-to-end pipeline.
These research efforts have shown promise, with several results indicating that partial end-to-end outperforms end-to-end.
This is because partial end-to-end systems may benefit from the advantages that both the classical and end-to-end offer: the structure of the end-to-end pipeline eases development, while the learning methods from end-to-end approaches provide flexibility.

In particular, we have identified the partial end-to-end approach whereby a neural network trajectory planner is used in conjunction with a classic controller as a suitable method to combat the sensitivity towards model inaccuracy that current learning systems experience.
This is because controllers are used to absorb uncertainty within the classic pipeline.
However, whether this approach will actually result in a performance benefit over purely end-to-end systems under model-mismatch conditions is not explored in literature and remains an open question - one which we aim to answer through this project.
We conclude this section by repeating the literature taxonomy in Figure \ref{fig:literature_taxonomy_1}, but with the leaf nodes showing research papers we reviewed. We also indicate where our approach fits into the taxonomy.

\begin{figure}[h]
    \centering
    \input contents/chapt2/figs/literature_taxonomy_articles.tex
    \caption[A taxonomy of the autonomous racing literature with articles]{A taxonomy of the autonomous racing. The blue leaf nodes reference the articles relevant to their description.}
    \label{fig:literature_taxonomy_1}
\end{figure}

